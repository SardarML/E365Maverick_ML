 While you likely know that data science is the practice  of making data useful, you may not  have a clear landscape around the tools that  can help you at each stage of the data science workflow.  Well, stay with me to discover the six broad areas that  are critical to the process of making data useful  and some corresponding Google Cloud products and services  for those areas.  Let's go.  Well, first is data engineering.  Perhaps the greatest missed opportunity in data science  stems from data that exists somewhere  but hasn't been made accessible for use in further analysis.  Laying the critical foundation for downstream systems,  data engineering involves the transporting, shaping,  and enriching of data for purposes  of making it available and accessible.  We consider data ingestion as moving data  from one place to another.  And data preparation, the process  of transformation, augmentation, or enrichment of data  prior to consumption.  Now, global scalability, high throughput, real-time access,  and robustness are really some common challenges  at this stage.  For scalable real-time and batch data processing,  look into building data ingestion and preprocessing  pipelines with Dataflow, a managed Apache Beam service.  There's a reason why Dataflow is called the backbone  of analytics on Google Cloud.  And we've done a video on this before.  I will link that in the description.  If you're looking for a scalable messaging system  to help you ingest data, consider Cloud PubSub,  a global horizontally scalable messaging infrastructure.  Cloud PubSub was built using the same infrastructure components  that enable Google products, including the ads, search,  and Gmail, to handle hundreds of millions of events per second.  If you want an easy way to automate data movement  to BigQuery, a serverless data warehouse on Google Cloud,  then look into the BigQuery Data Transfer Service.  For transferring data to cloud storage,  take a look at Storage Transfer Service.  And for no-code data ingestion and transformation tools,  check out Data Fusion, which has over 150  pre-configured connectors and transformations.  In addition to Dataflow and Data Fusion for data preparation,  Spark users may want to look at related products and features  for Spark on Google Cloud.  Now, for data storage and data cataloging on Google Cloud,  for structured data, consider a data warehouse like BigQuery  or any of the cloud databases.  The relational ones, like the Cloud SQL and Spanner,  and NoSQL ones, like Cloud Bigtable and Firestore.  For unstructured data, you can always  use Cloud Storage, which is an object store.  You can also use Cloud Storage as a data lake.  I have done another video where I've compared  different database options.  I will link that in the description below.  For data discovery, cataloging, and metadata management,  consider Data Catalog.  For a unified solution, take a look  into Dataplex, which integrates a unified data management  solution with an integrated analytics experience as well.  Now comes data analysis.  From descriptive statistics to visualizations,  data analysis is where the value of data starts to appear.  Now, data exploration, data pre-processing, and data  insights are parts of this.  Data exploration is a highly iterative process.  It involves slicing and dicing the data  via data pre-processing before data insights  can start to manifest through visualizations  or simply via simple group by order by operations.  One hallmark of this phase is that the data scientists may  not yet know which questions to ask about the data.  In this somewhat ephemeral phase,  a data analyst or scientist has likely  uncovered some aha moments but hasn't shared them yet.  Once insights are shared, the flow  enters the insights activation stage  where those insights become used to guide business decisions,  influence consumer choices, or become  embedded in other applications or services.  On Google Cloud, there are many ways  to explore, pre-process, and uncover insights in your data.  If you're looking for a notebook-based end-to-end data  science environment, check out Vertex AI Workbench,  which enables you to access, analyze, and visualize  your entire data estate from structured data  at the petabyte scale in SQL with BigQuery  to pre-processing data with Spark on Google Cloud  and its serverless auto-scaling and GPU acceleration  capabilities are amazing.  You don't have to worry about managing any of that.  As a unified data science environment,  Vertex AI Workbench also makes it  easy to do machine learning with TensorFlow, PyTorch, and Spark  but with built-in MLOps capabilities.  Finally, if your focus is on analyzing structured data  from data warehouse and insights activation for business  intelligence, you may want to also consider  using Looker with its rich interactive analytics,  visualizations, dashboarding tools, and Looker blocks  to help you accelerate your time to insights.  Now, third is model development.  From linear regression to XGBoost,  from TensorFlow to PyTorch, the model development stage  is where machine learning starts to provide new ways  of unlocking value from your data.  Experimentation is a strong theme here,  with data scientists looking to accelerate iteration speed  between models without worrying about infrastructure overhead  or context switching between tools for analysis  and tools for productizing models with MLOps.  Now, to solve these challenges, once again,  as a Jupyter-based, fully managed, scalable,  and enterprise-ready environment,  you've got Vertex AI Workbench, which  makes it easy as the one-stop shop for data scientists  combining analytics and machine learning,  including Vertex AI Services, Apache Spark, XGBoost,  TensorFlow, and PyTorch, are just some of the frameworks  that it supports in Vertex AI Workbench.  Now, Vertex AI Workbench makes managing  the underlying compute infrastructure  needed for model training easy, with the ability  to scale vertically and horizontally,  and with idle timeouts and auto-shutdown capabilities  to reduce unnecessary costs while training the models.  Now, notebooks themselves can be used  for distributed training and hyperparameter optimizations,  and they include Git integrations  for version control.  Due to the significant reduction in context switching required,  data scientists can build and train models five times faster  using Vertex AI Workbench than when  using traditional notebooks.  With Vertex AI, custom models can be trained and deployed  using containers.  For low-cost model development, data analysts and data  scientists can also just use SQL with BigQuery ML  to train and deploy models, including XGBoost,  deep neural networks, and PCA.  Now, directly using BigQuery's built-in serverless  auto-scaling capabilities, all of this is possible.  Now, behind the scenes, BigQuery ML  actually leverages Vertex AI to enable  automated hyperparameter tuning and explainable AI.  For no-code model development, Vertex AI training  provides a point-and-click interface  to train powerful models using AutoML,  which comes with multiple flavors.  You've got tables, images, text, video, and translation.  Fourth is ML engineering.  Now, once a satisfactory model is deployed,  the next step is to incorporate all the activities  of a well-engineered application lifecycle,  including your testing, development, monitoring.  And all of those activities should  be as automated and robust as possible.  Managed datasets and feature store on Vertex AI  provides shared repositories for datasets and engineered  features, respectively, which provides  a single source of truth for data  and promote reuse and collaboration  within and across teams.  Now, Vertex AI's model-serving capability  enables deployment of models with multiple versions,  automatic capacity scaling, and user-specific load balancing.  Now, finally, Vertex AI model monitoring  monitors and provides the ability  to monitor prediction requests flowing  into the deployed model and automatically alert model  owners whenever the production traffic deviates  beyond user-defined thresholds and previous historic  prediction requests.  Now, MLOps is the industry term for modern, well-engineered ML  services with scalability, monitoring, reliability,  automated CI, CD, and many other characteristics and functions  that are not taken for granted in application domain.  The ML engineering features provided by Vertex AI  are informed by Google's extensive experience  deploying and operating internal ML services.  And our goal with Vertex AI is to really provide everyone  with easy access to essential MLOps services and best  practices.  Fifth is Insights Activation.  Now, Insights Activation stage is  where your data has now become useful to other teams  and processes.  You can use Looker and Data Studio  to enable use cases in which data  is used to influence business decisions with charts, reports,  and alerts.  Data can also influence customer decisions  and, as a result, increase your usage or decrease churn.  Finally, the data can also be used by other services  to drive insights.  These services can run inside Google Cloud, on Cloud Run,  or Cloud Functions, or outside Google Cloud  where you're connecting using Apigee API  management as an interface.  Sixth is Orchestration.  All of the capabilities that we've discussed so far  provide the key building blocks to a modern data science  solution.  But a practical application of those capabilities  require orchestration to automatically manage  the flow of data from one service to another.  This is where a combination of data pipelines,  ML pipelines, and MLOps comes into play.  Effective orchestration reduces the amount of time  that it takes to reliably go from data ingestion  to deploying of your model in production  in a way that lets you monitor and understand your ML system.  For data pipeline orchestration, Cloud Composer and Cloud  Scheduler are both used to kick off and maintain the pipeline.  For ML pipeline orchestration, Vertex AI Pipelines  is a managed machine learning service  that enables you to increase the pace at which you experiment  with and develop machine learning models  and the pace at which you transition  those models into production.  Now, Vertex Pipelines is serverless,  which means that you don't really  need to deal with managing an underlying GKE  cluster or infrastructure.  It scales up when you need it to,  and you pay only for what you use.  In short, it lets you really focus  on building your data science pipeline.  Those were the six broad areas that  are critical to a data science workflow  and for making data useful.  Which of these areas is of most interest to you?  Share with me in the comments below.  If you like this video, don't forget  to like and subscribe for more such content.  Thank you.  Thank you. 