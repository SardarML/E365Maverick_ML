 Welcome. Today, I'll show you how to create  a continuous integration Azure Pipeline to  automatically build and test  all changes to your GitHub repository.  You will learn how to enable  continuous integration also known as CI with Azure Pipelines,  what is a YAML-based Pipeline and why use it,  how to create a Pipeline that runs on  any change to your GitHub repository,  how to diagnose and fix issues detected by the Pipeline,  and how to report the status of the Pipeline on GitHub.  Before diving into how to create a Pipeline,  it is good to understand what's  a typical sequence of steps in  Azure Pipelines and how it  enables continuous integration scenarios.  It all starts with a software developer who  has some code ready to go in his box,  and it's ready to just push that code via  Git into his remote GitHub repository.  The push happens either directly or via a pull request.  At that point, GitHub has already been configured to talk to  Azure Pipelines to notify it that such an event has happened.  GitHub just goes ahead and notify  your project in Azure Pipelines.  A push happens, let's say master branch.  At that point, Azure Pipelines will read or  will evaluate what we call the Pipeline definition,  which is a YAML file that's  stored actually in your GitHub repository too.  Azure Pipelines will read it and that will tell it what are  all the steps to execute for this Pipeline,  and also where to execute the Pipeline,  and any constraints, and any other configurations  that are related to the execution of the Pipeline.  Once it reads that file,  it will go ahead and queue what we call a run,  which is a series of tasks to execute.  It will queue this run in what we call an agent pool.  The agent pool is a series of a bunch of machines that are  ready to receive the requests  that are coming from Azure Pipelines.  This agent pool can be a series of,  let's say, VMs, virtual machines.  It could also be physical machines,  just normal physical boxes connected to Azure Pipelines,  or they could be also Docker containers.  The most common way to do things these days is via virtual machines.  But a more interesting way to do this is via containers,  and we will see that in the video in a few moments.  Now, these machines could be either hosted by  Microsoft in the Azure DevOps product,  or they could be self-hosted.  So if you don't want to really worry too much about how  to prepare these machines and how to connect them to  Azure Pipelines so that they can execute your Pipeline,  so you would just go with Microsoft hosted.  In this case, they are VMs.  I think for public projects,  you will get up to 10 concurrent pipelines  that can run simultaneously.  But of course, it's easy to just use those,  but it has its own restrictions.  You don't have any control on the software that goes into  those machines or the spec of the machines themselves.  So depending on what you want to do,  there may or may not be as convenient for you.  The other option, self-hosted,  has the benefit of you can prepare  the entire machine by yourself with exact specs that you need.  But of course, it's an ongoing maintenance task for these machines.  So it's up to you what you want to use.  We will be using Microsoft hosted in this video.  Now, these machines can also be configured to use  either Linux, Windows, or Mac OS as the operating system.  So it really depends on what you want to do in the Pipeline.  If you need to, let's say you want to build a microservice,  it usually can run just fine on Linux.  So you will pick the Linux OS or a Linux VM.  But if you want to do things like using,  let's say, .NET Framework,  or you want to build a UWP,  Universal Windows Platform projects,  you may need to go with Windows-based VMs.  If you want to build something for iOS,  so most likely you have to go for a Mac OS agent.  Then after that happened,  an agent will be selected from this pool,  and the agent will just go ahead and pull the source,  the source code that you have on GitHub.  It will be pulled into that machine,  and the series of tasks that are configured in your Pipeline will execute.  So one of the typical and most basic task is just the build step,  where we build this code just as you would have built it in your box.  Let's say in the .NET case, .NET build.  In this case, the agent will do that for you.  We'll just build the code automatically.  Once it's built, it can do other things like,  let's say, run the tests like .NET test  or any other kind of test runner that you have configured.  It can go ahead and run all these tests for you.  Finally, it will publish results into the Azure Pipelines UI,  and it can also send all sorts of notifications like e-mails,  if you want to know what happened with the Pipeline.  So this is the overall flow on Azure Pipelines.  It will vary a lot,  especially in terms of the tasks that execute,  depending on what you have configured in your YAML Pipeline.  Of course, there's another side of this,  which is the continuous deployment,  which we will not cover yet in this video.  Now, here are a few things that we will be using in this tutorial.  First, a couple of .NET Core projects  already published in a GitHub repository.  Second, Git, which we will use to manage changes to the repository.  Third, the .NET Core 3.0 SDK,  which we will need to build and test the code locally.  Finally, Visual Studio Code,  which we will use as our code editor.  You could, of course, use any other code editor  that works best for you.  To illustrate how to enable  continuous integration with Azure Pipelines,  we're going to use the Hello Pipelines repository  that I have already published into GitHub.  This repository has just a couple of  very simple .NET Core 3.0 projects.  The first one is a Web API.  This one is very similar to the one that you will get if you  used to .NET new Web API via the .NET Core CLI.  The main thing about this project is going to be  the controller that we have here,  the weather forecast controller,  which only has just one API here, Git.  What it does is just returns  a list or a collection of weather forecasts.  In each of these forecasts,  it's going to have a date,  a temperature, and a summary.  That summary is just  a random string out of this string that you can see at the top.  The other project that we have here is a little test project.  This is an ex-unit project that just has one test class,  and that test class just has one very simple test that is  going to invoke that API and it's going to  confirm that the expected number of days are being returned.  So how do we enable an Azure Pipeline for this GitHub project?  What you want to do is go to  azure.microsoft.com slash services slash DevOps slash pipelines.  Here, depending on if you  have already an Azure DevOps account or not,  you may want to click on start free with  pipelines or sign in to Azure DevOps.  In this case, let's assume that we don't have an account yet.  So we're starting brand new.  So start free with pipelines.  Now, we're going to authenticate.  In this case, I'm going to use my Microsoft account.  Here, we're asked for a project name.  So your project is the place that's going to host  both your pipelines and any other Azure DevOps related  artifact that you want to use  across your software development lifecycle.  So this project, we're going to just call Hello Pipelines.  You can choose if you want to make it private,  meaning that only you and the people that you  invite can see what's going on in this project,  or public, meaning anybody  can go ahead and see what's going on here.  So since our repository is public,  let's go ahead and just make it public too here.  So I'll click continue.  This is also going to create  what they call an Azure DevOps organization,  which is an Uber container of  a bunch of potential projects that you can have in Azure DevOps.  Now, as you can see,  an organization has been created.  It is called Huluc0382,  and a project has been created,  Hello Pipelines over there.  Now, we're presented with an interesting choice,  which is to choose where to get the code from.  At the same time, we're presented with  the option of using either in  Java-based pipelines or using  the classic editor to create the pipeline.  So Java, by the way,  stands for yet another markup language,  that's an acronym, and it's nothing more than  a human-friendly data serialization standard  for all programming languages.  These days, the recommended approach is to  just go for the Java-based pipeline,  but why would you want to use  this as opposed to the classic editor?  Now, the classic editor,  which is legacy at this point,  will allow you to use more of a UI-friendly approach,  just drag and drop tasks and do a bunch of things  visually in this designer to create your pipeline.  But the main pitfall of that classic designer is that  the pipeline definition itself is  not checked in alongside your code.  The main problem with this, which is not  evident as you're starting with this,  but after a while,  months from now, when you want to go  back and build again some old code that you  need to build again with  the same pipeline that you're using today,  in many cases, you just can't.  Why? Because the pipeline has evolved in  a separate way from your code.  In the past, you may have had some other projects or  some other binaries or test code or artifacts,  some other things that today are not  there and that the pipeline is not honoring anymore.  That disconnect makes the classic editor and  the pipelines created by the classic editor  not ideal for a long-term project.  Overall, I'll strongly recommend using the YAML-based pipeline.  The other thing, of course, is that there are new features,  new Azure Pipelines features that are already  being introduced into the YAML-based pipelines,  like deployment jobs,  cron-based jobs, skilled jobs,  and probably some other things.  Those things are just not available in the classic editor.  Even if it takes a little bit more to  learn the YAML-based pipelines,  I would strongly recommend that you go for this one.  Now, at the time where we are recording this,  there's a feature that we want to use and it's not  yet available broadly, so we have to enable it explicitly.  So to do that, I'm going to go here to my profile  and click dot dot dot,  Preview Features, and it's called Multistage Pipelines.  All right. Now, where's my code?  Well, my code is in GitHub.  So I'll click GitHub.  Now, at this point, you may be prompted to authenticate to GitHub.  In my case, it's not prompting me  because it already did it and just remember it.  So I'll click on Hello Pipelines.  Now, we're taken into GitHub.  Why? This is because GitHub is  asking us for permission to let Azure DevOps get access to the code.  So pretty much Azure DevOps wants to get notice  of any time that some code is pushed into GitHub.  So for that, we need to install this,  what they call the Azure Pipelines application into GitHub,  and it will grant access to these permissions that we see here.  So we have to say, yes,  and I will authenticate here.  Okay. Authenticate again with the Microsoft account.  So this sets up the connection between GitHub and Azure Pipelines.  So Azure Pipelines from now on  has access to what's in your GitHub repository.  Now, at this point, we're presented with a bunch of options in  terms of a template to initialize your YAML file.  You could choose among a series of templates that are available  depending on what kind of framework,  task, or build tool,  or test tool, whatever you want to do.  There's a bunch of templates for you.  But in our case, we'll just keep it simple,  go step-by-step, so we'll go for a starter pipeline.  Here we are. So an initial pipeline,  very simple pipeline has been generated for us.  So let's start exploring what's going on here.  I'm going to collapse this section here to have more space,  and let's start looking at this.  The first thing that I'll recommend you is to actually go to  this link over here, aka.ms.yaml,  which I think I have already opened somewhere here, right here.  So this page is super useful because this describes  the entire YAML schema reference.  So here you can tell exactly how to structure your YAML file,  how the pipelines are defined by this YAML file,  conventions, the basics, and a bunch of samples so that  you can get to know how to actually build these pipelines.  There's also a description of all the tasks that are  available and a bunch of concepts and things.  So super useful page.  You should keep this handy whenever  you're dealing with a YAML-based pipeline.  So now back to here.  One more thing about YAML pipelines, by the way,  is that this is enforcing what we call a configuration as code,  which is this very nice practice of storing  your pipeline alongside the code in the repository.  So this is great because from here on,  you will know exactly what's going on with the changes to  the pipeline as people is making changes to them,  while pushing them to the GitHub repository.  Again, that would not be available with the classic pipeline editor.  So keeping my configuration as code, great stuff.  First thing here, the trigger.  The trigger is what defines when  this pipeline is going to get kicked off.  So what it is saying right now is that  anytime something is pushed or merged into the master branch,  this pipeline has to get kicked off.  This you can change.  It could be any of the branches that you  have available in your repository.  There's also some other options if you want to limit  exactly which paths within your branch you  want to use to trigger a pipeline run.  Now, there are other options available also,  like this is called a CI-based trigger,  but you could create a pull request-based trigger  where the pipeline will kick off whenever  a new pull request, let's say in GitHub, is created.  So that's another way to run your pipeline.  The other way is a schedule type pipeline.  So you can say, well,  every hour, go ahead and kick off the pipeline,  or every night, or every morning,  or once a week, stuff like that. That's also available.  Next is the pull.  So we talked about virtual machine pulls or agent pulls before.  So here's where you define what kind of machine you want to use.  So by choosing a VM image,  you're telling Azure Pipelines that the first thing is that you  actually want to use the Microsoft-hosted virtual machine.  Second, in this case,  by saying Ubuntu, you're saying, well,  I want to use a Linux-based machine.  So it will really depend on what you want to do.  You could do Ubuntu latest,  you could also do Windows latest,  if you want to use a Windows virtual machine,  or you could do, I think it's a Mac OS latest,  if you want to build in a Mac OS device.  So again, it depends on what you want to do.  There's also some other,  you can also pick specific versions of this image.  You don't have to use latest.  Again, if you want to know exactly what's available,  go back to that YAML schema reference page,  and somewhere in here, you're going to find  all the options of virtual machines available for you.  Like I said, we're going to use a hosted image.  Here, we're not going to be managing our own virtual machine.  Now, one thing that I like to recommend here,  is to not run the pipeline directly into the virtual machine.  Why? Because usually, you don't know  exactly what's going on in that virtual machine.  You don't know. Usually, these machines have tons,  and tons, and tons of tools,  and frameworks, and compilers,  and test runners, and artifacts,  and all sorts of things installed on them.  So for the very specific project that you  want to go continuous integration across,  you may not need all those dozens,  and dozens, and dozens of things that  could have unintended consequences within your pipeline.  So one thing that you can do to  prevent having to use all that,  is just use a container.  So by using a container,  you can say, okay, so you're going to build,  sorry, you're going to run my pipeline  specifically within this container that I am specifying.  So for instance, in this case,  we know that we're building and we are  testing .NET Core 3.0 set of projects.  So for instance, in that case,  what we can do, and I re-pull this up,  is go and find the .NET Core SDK Docker image,  which is right here, and I'm going to copy it.  I'm going to say, hey,  when you run the pipeline,  don't just run the pipeline directly in the virtual machine.  First, go ahead and pull the .NET Core 3.0 SDK container image,  run it, and within that container,  go ahead and run my pipeline.  So that makes sure that only the things that you  need for your pipeline will be used across the pipeline.  In this case, the .NET Core SDK is all we need.  For us, we don't need all the other tooling  that's available there.  So in fact, I didn't use a container,  I just go with the Ubuntu latest virtual machine.  That image does not have the .NET Core 3.0 SDK.  It has a previous version as of the time of this recording.  So I would have to add an additional task in  this pipeline to make sure that I  actually get the .NET Core 3.0 SDK.  So containers, great stuff,  may add some seconds to your pipeline,  but it's totally worth it.  Now, going to steps,  here's where you actually declare  what are the actions on the steps that you want to execute.  So for an example, this is giving us a couple of scripts,  but we're definitely just removing them.  Then what we want to do is to add our steps.  So there's two ways to add steps.  The first way is by just typing them,  and we can use some intelligence here.  So for instance, the first task that we want to use here  is a task.  So on this task is going to be the .NET Core CLI 2,  and we're going to need some inputs.  Sorry. Inputs, and what we want there is just one command,  and that command is called build.  So the way that the projects are set up,  we just have to do .NET build,  and it will go ahead and build all the projects.  And that's all we need to do,  and that is the setup for this task.  But now you may say, well,  I don't want to be typing all this stuff all the time.  I have no idea what to put here.  So again, keep in mind that you can always go back  to the YAML schema, and this will have the definition  of all the tasks and samples and all these things, right?  So you're not alone there.  But if you really don't want to just type this stuff,  there's this thing called the assistant on the right side.  You just have to click in there,  and this is going to open up a list  of all the tasks available in Azure Pipelines.  And you just have to select the one that you care about,  and this is going to bring a bunch of options  so that you don't have to type them,  but you actually want to select them over here.  So in this case, what we want to do is to use the,  let's say, the test command,  because we want to build the code and then test the code, right?  And what's the path to the project?  In this case, we're just going to use a mini match expression.  Say we want to scan all the directories in the source  and find anything that contains tests  in the name of the project, csproc.  So anything that has tests in the project name,  we will be picking across all the repository.  And also, let's publish those test results  and code coverage if available into the Azure Pipelines store.  So I click Add, and as you can see,  that adds immediately the task right here.  So you can see, so either you type it  or you can pick it from here.  It's not as fantastic as the old designer,  but it's a very handy tool,  and it lets us build beautiful YAML pipelines.  So now the pipeline is pretty much ready to go,  and what I'm going to do is just hit Save and Run.  And at this point, you're prompted with the option  of either committing this directly to the master branch,  or you can create a new branch for this commit,  and you can potentially even create a pull request  if you want to get others' reviews and approvals on that.  To keep things simple in this video,  we'll just go ahead and commit directly to the master branch.  So Save and Run.  So this is now created a pipeline.  So again, remember, so the Azure Pipelines YAML  is checked in into your repository.  So it will leave and move forward  as your repository moves forward.  So here we are in the Pipelines Monitoring page.  So now you're looking at one specific run  and it's telling you the duration,  and it's right now in the queued state,  and it just changed it to running.  So your pipeline is now running,  and if you want to know what exactly is going on  with that pipeline, you can always just click on the job.  This will open up this UI here,  and we can walk through what's going on there.  First, what it's doing is, of course,  pulling that Docker container image,  the .NET Core 3.0 SDK, like we said,  because it is inside this container  that all the pipeline is going to execute.  So now we're checking out the code.  So it's pulling the code from GitHub into this container,  and next, it will go ahead and it will build the code, right?  So just .NET build with that task that we added,  building the code,  and I think something happened while building the code.  So let's see.  Let's wait for the file.  Okay, so pipeline has finished.  So let's scroll up a little bit and see what we can find.  So we have an error, actually, in the build step.  In the Web of Forces controller, there's an error.  Cannot convert from method group to int.  All right.  So there's something going on here.  So the best thing that we can do, I think,  is to actually try to reproduce this thing locally  and see what happens.  So go back to the GitHub repository.  I'll get the clone URL.  Now I'll go to my box here,  and I'll just do git clone.  All right.  So let's go to Hello Pipelines,  and let's open VS Code  to see what's going on here.  All right.  Here we are.  Let's close this welcome screen.  And let's look again.  We were looking at the controllers,  weather forecast controller file, line 34.  Let's go there.  Web API controllers, weather forecast controller.  Sure, let's restore Nuget packages.  And let's see line 34.  Indeed, there's something going on here.  And yeah, so the problem here  is that we're trying to use account property,  which does not really exist,  because summaries, it's an array.  Arrays don't have account property.  We could use account method if we're using link here,  but probably it's more efficient to just use linked,  which is an actual property that's already computed.  So it's more efficient than using the commenter.  So let's do that.  So this should fix it,  but let's make sure it's actually fixed.  Let's do run build task.  This is going to do .NET build  for both projects.  And if this is fixed, yes, indeed, this succeeded.  So let's commit this.  Use length instead of count in get.  All right, so let's do that.  All right, it's right there.  Now let's open another terminal  and let's do git push origin master.  All right, so this should fix the issue.  Let's go back to Azure Pipelines.  And as you can see, just by doing that git push,  another build has kicked in.  So this is what we call continuous integration.  So any change that's made to our master branch  is being immediately exercised by Azure Pipelines,  by the continuous integration pipeline.  And so as you can see, we have the pipeline now running.  And so let's see if we can get a successful run this time.  All right, so the pipeline has completed  and indeed the job has failed.  And the one thing that I noticed,  besides the fact that it has failed,  is that zero test has passed.  So first, well, let's first review what failed here.  So it's saying that,  yes, so the .NET test step failed.  It's saying that we have an assert equals failure.  So that something failed in the test,  expecting seven, actual five.  And if you go back to the run  and we click on this section where it says test zero pass,  we can click there.  And this actually gives us an overall view  of all the tests that failed in this run.  So let's go back to the run.  All the tests that failed in this run.  And if you click in the failed test,  it will give you a very nice view of what happened.  So like we saw in the error before,  there's a place where we're asserting  that we will expect seven and we're getting five.  And that's in the only test that we have.  So let's go back to the test and see what's going on.  So let's see.  Our test over here.  Okay.  So this test will go ahead,  create a controller,  passing a stop of the logger that it needs.  It is expecting to receive seven days,  calls get,  and we are not getting seven days.  Let's see where it forces controller.  Aha.  So it is getting a range of five days actually,  not seven days.  So that's the issue.  So at this point to fix this,  so either the test is wrong  or our implementation of the method is wrong.  So let's assume that the test is actually right.  And let's say, well,  actually let's return the seven days  that the test is expecting.  So let's see if the test is now happy with this.  So let's run all the tests.  Expand this a little bit.  See what we get.  Aha.  One out of one test pass.  So this fixes it.  So let's go back here and say,  fix slash get to return expected number of days.  So yes.  Back to the terminal  and let's just do git push,  origin master.  And back to pipelines.  Let's go here.  And again,  just by magic,  the pipeline just runs immediately.  So let's click here.  This will go ahead and run the pipeline again.  And if we're lucky,  this time we'll get a successful run.  And so indeed,  this time the job succeeded.  We have a 100% pass rate.  So this means that we're good  and we can actually click there  and we'll see all tests are good.  There's no failures here.  And so everything's great.  So the pipeline is ready.  Now, one more thing that we might want to do  just to reflect the fact that we have a pipeline  in our GitHub repository  is to add a status batch to the GitHub page.  So that batch we can show right here in this page.  And to enable that,  let's go back to the Hello Pipelines page.  And if you just click this dot dot dot,  click status batch,  you can click on the sample markdown here.  I'll just copy it.  And then go back to here.  We use code.  Let's open our readme file.  By the way,  you should always have a readme file.  That's super useful for future readers of your repo.  And just paste that markdown.  I'll hit save.  And I'll say add status batch.  All right.  And let's push this.  Okay.  And by doing that,  we now go to GitHub and we refresh this page.  We'll see a status batch right here.  So anybody that just comes to this repository  wants to know what's the status of this code,  it will know that the status is,  well, this case succeeded.  It will say failed if the last build of this failed.  And if you click there,  click there,  you will see the status of the latest build  associated to this repository.  So there you go.  Continuous integration for your GitHub repository  enabled by Azure Pipelines.  If this video was useful,  please consider hitting the like button.  Don't forget to hit subscribe and the notification bell  to know right away when I publish new videos.  Also, please leave me a comment below  with any thoughts about this video.  Thanks for watching.  See you next time.  Bye. 