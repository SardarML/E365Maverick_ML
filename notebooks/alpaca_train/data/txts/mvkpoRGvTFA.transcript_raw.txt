 Let's take a look at the data pipeline and how it works and what you can expect to happen on  every stage of the data pipeline. The first step is the ingestion. Here you get log files,  media files, generally unstructured files, and you get a lot of information from business and  custom applications. All of this goes into the Azure Data Factory. Now this is all being ingested  and brought into Azure. After you have your data, you've ingested it, you need to store that data  so you can store that data in the Azure Data Lake storage. From there, you can also use it  immediately to model and serve. If you want to do that on raw data, then you can do that  immediately from Azure Synapse Analytics and Azure Analysis Services, but right now this data isn't  processed. It's completely raw. So to process this data, we can prepare it and train it in  Azure Databricks. From Azure Databricks, we can send it to Power BI to send business intelligence  reports, and we can send it to Azure Cosmos DB for storage or to be displayed in web application. 