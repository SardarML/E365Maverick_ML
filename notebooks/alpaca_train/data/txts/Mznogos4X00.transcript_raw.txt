 All right, we're going to talk about the Hadoop ecosystem at a very high level now, and it's going to be a little bit overwhelming.  I got to warn you, there's a lot of technologies here, but bear with me.  You know, all I really want you to do is to get some exposure to these terms and these technologies so that they don't seem as foreign when we dive into them in more depth.  And, you know, this can be a very valuable lecture in and of itself.  So if you watch this a couple of times, it might be a good idea because this is all you really need to really understand what all these cryptic names in the Hadoop ecosystem really mean and what everything is for at a very high level.  So let's dive in and actually uncover the secrets of Hadoop.  All right, so let's go into some detail about the major components in Hadoop.  I just want to kind of briefly touch on all these different technologies, and we're going to go into a lot more depth.  That's what the rest of this course is all about, just deep diving into each one of these and giving you some examples of using them.  So I've split things up here into three general areas here.  We have what I call the core Hadoop ecosystem, which is just things built on the Hadoop platform directly.  And then we have some ancillary systems that we'll talk about as well.  There's a lot of different ways of organizing these systems.  And this is just what makes sense to me.  There are a lot of complex interdependencies between these systems.  So it's there's really no right way to represent these relationships.  But what I'm trying to do here is show you graphically the things that build on top of each other within Hadoop.  Now, the pink things here are the things that are part of Hadoop itself.  Everything else is sort of add-on projects that have come out over time that integrate with Hadoop and solve specific problems.  So let's start at the base of it all, which is HDFS.  That stands for the Hadoop Distributed File System.  So remember, we talked about GFS.  HDFS is the Hadoop version of that, and that is the system that allows us to distribute the storage of big data across our cluster of computers.  So it makes all of the hard drives on our cluster look like one giant file system.  And not only that, it actually maintains redundant copies of that data.  So if one of your computers happens to randomly burst into flames and melts into a puddle of silicon, hey, it happens.  It can actually recover from that and will back itself up to a backup copy that it had of that data automatically.  It's like you'll never even know anything happened.  So that's the power of HDFS.  That is the data storage, the distributed data storage piece of Hadoop.  Now, sitting on top of HDFS, we have YARN, and that stands for Yet Another Resource Negotiator.  So we talked about the data storage part of Hadoop, and there's also the data processing part of Hadoop.  YARN is where the data processing starts to come into play.  So YARN is basically the system that manages the resources on your computing cluster.  It's what decides what gets to run tasks when, what nodes are available for extra work, which nodes are not, which ones are available, which ones are not available.  So it's kind of the heartbeat that keeps your cluster going.  Now, given that we have this resource negotiator, we can build interesting applications on top of that.  And one of them is MapReduce, which, again, is a piece of Hadoop proper.  And MapReduce, at a very high level, is just a programming metaphor or programming model that allows you to process your data across an entire cluster.  And let's break that down.  It consists of mappers and reducers.  These are both different scripts that you might write, or different functions, if you will, when you're writing a MapReduce program.  Mappers have the ability to transform your data in parallel across your entire computing cluster in a very efficient manner.  And reducers are what aggregate that data together.  And it may sound like a very simple model, and it is, but it's actually very versatile.  And we'll see later on that there are some very creative ways you can put mappers and reducers together to solve very complex problems.  Now, originally, MapReduce and Yarn were kind of the same thing in Hadoop.  They got split out recently, and that's enabled other applications to be built on top of Yarn that solve the same problem as MapReduce, but in a more efficient manner.  We'll talk about that in a bit.  And then sitting on top of MapReduce, we have technology such as Pig.  So if you don't want to write Java or Python MapReduce code, and you're more familiar with a scripting language that has sort of a SQL-style syntax, Pig is for you.  Pig is a very high-level programming API that allows you to write simple scripts that look a lot like SQL in some cases, that allow you to chain together queries and get complex answers, but without actually writing Python or Java code in the process.  So Pig will actually transform that script into something that will run on MapReduce, which in turn goes through Yarn and HDFS to actually process and get the data that it needs to get the answer you want.  That's Pig, just a high-level scripting language that sits on top of MapReduce.  Let's zoom out a little bit here and remind ourselves where everything fits together.  We'll talk about Hive next, which also sits on top of MapReduce, and it solves a similar problem to Pig, but it's really more directly looks like a SQL database.  So Hive is a way of actually taking SQL queries and making this distributed data that's just really sitting on your file system somewhere look like a SQL database.  So for all intents and purposes, it's just like a database.  You can even connect to it through a shell client and ODBC or what have you, and actually execute SQL queries on the data that's stored on your Hadoop cluster, even though it's not really a relational database under the hood.  So if you're familiar with SQL, Hive might be a very useful interface for you to use.  Zoom out a little bit here and again, get our bearings here.  We'll talk next about Ambari.  And Apache Ambari is basically this thing that sits on top of everything, and it just gives you a view of your cluster and lets you visualize what's running on your cluster, what systems are using how much resources.  And it also has some views in it that allow you to actually do things like execute Hive queries or import databases into Hive or execute Pig queries and things like that.  So Ambari is what sits on top of all of this and lets you have a view into the actual state of your cluster and the applications that are running on it.  Now, there are other technologies that do this for you. Ambari is what Hortonworks uses.  There are competing distributions of Hadoop stacks out there, Hortonworks being one of them.  Other ones include Cloudera and MapR.  But for Hortonworks, they use Ambari.  All right, so let's go over here.  Mesos. So Mesos isn't really part of Hadoop proper, but I'm including it here because it's basically an alternative to Yarn.  So it too is a resource negotiator.  Remember, Yarn is yet another resource negotiator.  Mesos is another one. They basically solve the same problems in different ways.  There are, of course, pros and cons to using each one that we'll talk about later on.  But Mesos is another potential way of managing the resources on your cluster.  And there are ways of getting Mesos and Yarn to work together if you need to as well.  And we bring up Mesos because we're going to talk about Spark, which I think is one of the most exciting technologies in the Hadoop ecosystem.  This is sitting at the same level of MapReduce in that it sits on top of Yarn or Mesos.  It can go either way to actually run queries on your data.  And like MapReduce, it requires some programming.  You need to actually write your Spark scripts using either Python or Java or the Scala programming language, Scala being preferred.  But Spark is kind of where it's at right now. It is extremely fast.  It's under a lot of active development right now.  So Spark's a very exciting technology right now and a very powerful technology.  So if you need to very quickly and efficiently and reliably process data on your Hadoop cluster, Spark is a really good choice for that.  And it's also very versatile. It can do things like handle SQL queries.  It can do machine learning across an entire cluster of information.  It can actually handle streaming data in real time and all sorts of other cool stuff.  So I'm very excited to teach you more about Spark later in this course.  Moving on, Tez, similar to Spark in that it also uses some of the same techniques as Spark.  Notably, with something that's called a directed acyclic graph.  And this gives us a legs up on what MapReduce does because it can produce more optimal plans for actually executing your queries.  Tez is usually used in conjunction with Hive to accelerate it.  So we remember we looked at Hive earlier.  That kind of sat on top of MapReduce, but it can also sit on top of Tez.  So you have an option there. Hive through Tez can often be faster than Hive through MapReduce.  They're both different means of optimizing queries to get an efficient answer from your cluster.  Weehee! Let's talk about HBase.  So HBase kind of sits off to the side and it's a way of exposing the data on your cluster to transactional platforms.  So HBase is what we call a NoSQL database.  It is a columnar data store.  And you might have heard that term before.  It's basically a really, really fast database meant for very large transaction rates.  So it's appropriate, for example, for hitting from a web application, hitting from a website, doing OLTP types of transactions.  So HBase can actually expose the data that's stored on your cluster.  And maybe that data was transformed in some way by Spark or MapReduce or something else.  And it provides a very fast way of exposing those results to other systems.  And what else can we talk about?  Let's go over here and talk about Apache Storm.  Storm is basically a way of processing streaming data.  So if you have streaming data from, say, sensors or web logs, you can actually process that in real time using Storm.  And Spark Streaming solves the same problem.  Storm just does it in a slightly different way.  So Apache Storm made for processing streaming data quickly in real time.  So it doesn't have to be a batch thing anymore.  You can actually update your machine learning models or transform data into a database all in real time as it comes in.  Pretty cool stuff.  Let's go over here and talk about Uzi.  Uzi is just a way of scheduling jobs on your cluster.  So if you have a task that needs to happen on your Hadoop cluster that involves many different steps and maybe many different systems,  Uzi is a way of scheduling all of these things together into jobs that can be run on some sort of schedule.  So when you have more complicated operations that require loading data into Hive and then integrating that with Pig and maybe querying it with Spark and then transforming the results into HBase,  Uzi can manage that all for you and make sure that it runs reliably on a consistent basis.  Moving over here a bit, Zookeeper also sits alongside all these technologies.  It's basically a technology for coordinating everything on your cluster.  So it's the technology that can be used for keeping track of which nodes are up, which nodes are down.  It's a very reliable way of just kind of keeping track of shared states across your cluster that different applications can use.  And many, many of these applications we've talked about rely on Zookeeper to actually maintain reliable and consistent performance across a cluster, even when a node randomly goes down.  So Zookeeper can be used, for example, for keeping track of who the current master node is or keeping track of who's up, who's down, what have you.  And it's really more extensible than that even, but we'll talk about that later.  Over here, there's also systems that are just focused on the problem of data ingestion.  So how do you actually get data into your cluster and onto HDFS from external sources?  Scoop, for example, is a way of actually tying your Hadoop database into a relational database.  Anything that can talk to OLTP or, I'm sorry, ODBC or JDBC can be transformed by Scoop into your HDFS file system.  So Scoop is basically a connector between Hadoop and your legacy databases.  Flume is a way of actually transporting web logs at a very large scale and very reliably to your cluster.  So let's say you have a fleet of web servers.  Flume can actually listen to the web logs coming in from those web servers in real time and publish them into your cluster in real time for processing by something like Storm or Spark Streaming.  Kafka solves a similar problem, although it's a little bit more general purpose.  It can basically collect data of any sort from a cluster of PCs, from a cluster of web servers or whatever it is, and broadcast that into your Hadoop cluster as well.  So those are all three technologies that solve the problem of data ingestion.  All right, moving on.  Now, your data might be exposed or stored in other places, too.  So let's talk about those as well.  HBase would also fit into this category.  But since HBase is really part of the Hadoop stack itself, I left it off of this little collection here.  MySQL, of course, or any SQL database is something you might be integrating with your Hadoop cluster.  You can not only import data from Scoop into your cluster.  You can also export it to MySQL as well.  So a lot of these technologies like Spark have the ability to write to any JDBC or ODBC database.  And you can store and retrieve your results from a SQL database if you're so inclined.  Cassandra, like HBase, and also MongoDB are both also columnar data stores.  And they're also good choices for exposing your data for real time usage to, say, a web application.  So you definitely want some sort of layer like this, like Cassandra or MongoDB, sitting between real time applications and your cluster.  We'll talk about those in a lot more depth.  Both are very popular choices for vending simple key value data stores at very large transaction rates.  MySQL, Cassandra, MongoDB, all external databases that might integrate with your cluster.  And there are also several query engines that sit on top of your Hadoop cluster.  So if you want to actually interactively enter SQL queries or whatever, you can do that using these technologies.  Again, things don't always fit neatly into different circles here.  If you remember Hive, that actually is a similar thing as well.  But again, since Hive is more tightly integrated into Hadoop, I chose to leave it out of this particular circle.  But it too is a way of querying your data.  Apache Drill, pretty cool stuff.  It actually allows you to write SQL queries that will work across a wide range of NoSQL databases, potentially.  So it can actually talk to your HBase database and maybe your Cassandra and your MongoDB database as well,  and tie those results all together and allow you to write queries across all those disparate data stores and bring them all back together when you're done.  Hue, also a way of interactively creating queries.  It works well with Hive and HBase.  And actually for Cloudera, it kind of takes the role of Ambari for sort of the thing that sits on top of everything  and lets you visualize and execute queries on the Hadoop cluster as a whole.  Apache Phoenix, kind of similar to Drill.  It lets you do SQL-style queries across the entire range of data storage technologies you might have.  But it takes it one step further.  It actually gives you ACID guarantees and OLTP.  So it can actually make your NotSQL Hadoop data store look a lot like a relational data store and a relational database with all the guarantees that come with that.  And finally, Presto, yet another way to execute queries across your entire cluster.  These all solve, you know, kind of the same problem.  Zeppelin is just another angle on it that takes more of a notebook type approach to the UI and how you actually interact with the cluster.  But at the end of the day, those are all ways of actually executing queries and extracting meaning from your cluster without necessarily writing programs to do it.  So that's the world of Hadoop in a nutshell.  A lot of stuff there, but hey, that's all there is to it.  We're going to spend the rest of this course diving into each one of these one at a time and doing some actual exercises and activities with them.  So you get to understand them more in more depth.  But for now, those are the buzzwords.  And hopefully when you hear these terms and these technologies, they won't seem quite so foreign to you anymore.  So welcome to the world of Hadoop.  Let's move on and dive into some more detail.  All right, that was a lot of stuff right there.  And, you know, I understand if you need to go back and watch it again, there's a lot of information there.  But don't worry, we're going to dive into each and every one of these technologies in a lot more depth later on in the course.  I just wanted to kind of hit you with all this right up front so you could know just enough to be dangerous and know enough about these technologies to actually know what people are talking about when they're talking about Hadoop.  I mean, that really that's half of the battle here.  None of these things are really that complicated, to be honest.  You know, all of these things are actually fairly easy to use and understand.  Hadoop is only really hard to grasp because there are so many different technologies and a lot of them do the same thing.  But if you can understand what they're all for and how to choose between them and how to put them together to actually solve real business problems, well, that's what they pay people the big bucks for.  So let's move on and actually start to dive into some more depth.  Start with HDFS and see how that works. 