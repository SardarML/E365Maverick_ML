 What is going on guys, Vanity here, back with my complete guide to mastering audio.  The contents of this tutorial will be as follows.  We'll start with a quick disclaimer and then move into what mastering actually is.  Chapter one will be the art of dynamic range.  Chapter two, loudness normalization.  Three, compression.  Four, equalization.  Five, depth and dimension.  Six, loudness metering.  And finally, seven will be clipping, soft clipping, and oversample clipping.  Each of the chapters contains several subsections within.  Times for both are listed and linked in the pinned comments below.  This is obviously a fairly long tutorial, and it's definitely going to get technical  in a few places, predominantly the first couple of chapters.  Think of this video as a buffet.  Browse the list of chapters below, pick the ones that are most relevant,  and discard the rest for now.  If you're looking for the minimum effective dose,  I'd recommend the compression, equalization, depth and dimension,  and loudness metering chapters.  A quick side note too, for recently surpassing 10,000 subscribers,  I wanted to give something away just to show my appreciation  and say thank you to you all.  I'm running a giveaway with a first, second, and third prize,  ranging from an audio interface, a pair of headphones, and a MIDI controller.  The link for which is in the description below.  And yes, the competition is free to enter.  Just a disclaimer, right at the start of this video,  this tutorial is heavily researched,  which in itself does seem to be rare on YouTube these days.  I do not claim to own anything in this video as my own,  as it's simply a collection of research and information  taken mainly from books, fellow engineers,  and a little bit of my own experiences.  Much of the foundational information in this tutorial,  such as the definitions and principles,  have been taken from Bob Katz's brilliant book,  The Art and Science of Mastering,  which I'll definitely recommend you all read.  I wanted to base this tutorial around the book,  as Bob Katz has already written the complete guide to mastering,  but it's largely unheard of among producers.  Perhaps this is because it contains a lot of information  that only applies to mastering engineers,  as opposed to music producers.  The book is, of course, written for the former.  With this tutorial, I've tried to take what I deemed as relevant  and condensed it into a shortened version of the book  that goes into more detail in certain sections  to cater for all levels of experience.  Anyone looking to become a mastering engineer  should therefore read the book in its entirety,  after, of course, watching this tutorial.  This whole video is also in written format over on my blog,  which is, again, listed and linked in the description.  I've included a couple of extra chapters in the blog post  for some of the more technical details,  as I couldn't find a chapter to place them in  within this video.  I will be following this tutorial up  with a practical mastering guide inside of Ableton Live,  using only free and stock plugins.  But this tutorial covers everything I think you need to know  about the subject in order to effectively master  your own compositions.  So, without further ado,  please enjoy this complete guide to mastering audio.  What is mastering?  In a sentence, mastering is the art of compromise.  What do I mean by this?  Well, changing anything in mastering affects everything,  as any processing is applied across the entire mix.  This makes mastering the art of compromise,  of knowing what is sonically possible  and then making informed decisions  about what is the most important for the music.  A mastering engineer requires the same ear training  as a mixing engineer.  Though, the mastering engineer becomes expert  in the techniques for improving complete mixes,  while the mixing engineer specializes in improving the mix  at the level of the individual events that make up the whole.  Mastering requires us to develop new skills  since it is concerned with overall mixes  rather than individual instruments.  Chapter one, the art of dynamic range.  Before we can dive into the specifics of mastering,  it's important we understand the most critical factor  in the subject, dynamic range.  Even if you know and understand what dynamic range is,  I guarantee that you'll learn something new  in the next few minutes.  Any experienced mastering engineer will tell you  that they begin mastering  with the loudest part of the song.  Why?  Because loud passages accentuate those peaks  that the ear is most sensitive to.  Equalization choices that are pleasing  during a soft passage may sound harsh during a loud one.  The term dynamic range refers to the difference  between the loudest and the softest passages of a recording.  It should not be confused with loudness  or a program's average level.  Ironically, it wasn't until the year 2012  that we officially agreed on how to measure dynamic range.  Before that, we had no measurement method  to deal with the following issues.  If a song fades out to silence,  can we claim it has 90 decibels of dynamic range?  Does adding a spoken word introduction to a hard rock song  give it 40 decibels of dynamic range instead of only five?  Does a 10 second soft passage in a pop song  negate the effect of its otherwise constant loudness?  The answer to all three of these questions is of course not.  So how can we judge the effectiveness  of a brief soft passage  in the middle of a highly compressed song?  The answers lie in two groundbreaking developments.  The first is the International Audio Standard  for Measuring Loudness, ITU's BS 1770-3,  which tells us how to measure  both our program's integrated loudness,  also known as program loudness, and its true peak level.  It's set by the International Telecommunication Union,  or ITU for short.  The second breakthrough  is the European Broadcasting Union's recommendation R-128,  which defines our program's loudness range.  This is the first formal definition  for how to measure dynamic range.  In this tutorial,  I'm going to refer to dynamic range as the latter,  EBU's R-128 definition of loudness range.  A musical work that includes soft and loud passages  usually sound more natural,  and it can sound more exciting than one that does not.  The typical measured LRA of a popular music recording  is only six to eight decibels.  In fact, in 2012, Rudolf Altner,  a master's student in audio at the time,  produced a comprehensive thesis  that measured and statistically analyzed  over 10,000 charting popular music recordings  made between 1951 and 2011.  Altner determined that the medium LRA of popular music  has constantly been around six decibels,  plus one decibel, minus two decibels,  for each of the past 60 years.  This means that popular music producers  have been very consistent in the dynamic range  that they judge to be suitable.  But it does not mean that there was no loudness race.  On the contrary, other statistics uncovered by Altner  reveal the shocking extent of this race.  Furthermore, Altner determined that producers  have held soft passages of popular music  around five to seven decibels below the program level  with some variances.  Macrodynamics and Microdynamics.  Dynamics can be divided further into two categories.  Macrodynamics, which is the loudness differences  between sections of a song or song cycle  measured by LRA, and Microdynamics,  which is the music's rhythmical expression,  transient quality, or bounce,  which involves the music's short-term peaks.  Dynamic processes, such as compressors,  expanders, et cetera,  can affect the music's Microdynamics  as well as its Macrodynamics.  Manual gain riding can only affect  the music's Macrodynamics  since we can't move a fader fast enough  to affect the short-term peaks.  But we can affect Microdynamics  by editing very short movements.  The micro and macro manipulations work hand-in-hand  and many good compositions incorporate  both Microdynamic changes,  such as percussive hits or instantaneous changes,  as well as Macrodynamic, like crescendos and decrescendos.  Think of a music album as a four-course meal.  The progression from soup to appetizer  to main course and dessert is the Macrodynamics.  We'll now further explore on the Macrodynamics.  The art of reducing, or compressing, dynamic range.  The dynamics of a song or song cycle  are critical to creative musicians and composers.  Our quality reference  should be the sound of a live performance.  We should be able to tell by listening  if a recording will be helped or hurt  by modifying its dynamics.  Even when mastering largely constricted dramas like hip hop,  using the dynamics of live performance  should be your standard.  In a natural performance,  the choruses should sound louder than the verses,  ensembles louder than soloists,  and the climax meaningfully louder than the rest.  Many recordings have gone through  several stages of compression of dynamic range,  and indiscriminate or further dynamic reduction  can easily push the clarity and the impact downhill.  However, usually the recording medium  and intended listening environment  simply cannot keep up with the full dynamic range  of real life.  So the mastering engineer is often called upon  to raise the level of soft passages  and or reduce loud passages,  which can be done by manual compression,  moving a fader up or down,  or manipulating gain in a door.  We may reduce dynamic range  when the original range is too large  for the typical home environment,  or to help make the mix sound more exciting,  fatter, more coherent,  to bring out inner details  or even out dynamic changes within a song  if they sound excessive,  which is definitely a subjective judgment.  Experience tells us when a passage is too soft.  The ears sensitivity changes with the context.  So a soft introduction located  immediately after a loud song may have to be raised,  but a similar soft passage in the middle of a song  may be just fine.  Meter readings are fairly useless in this regard.  How soft is too soft?  As case in point,  the engineers at Lucasfilm discovered  that having a calibrated monitor gain  and a dubbing stage with a very low noise floor  does not guarantee that a film's mix  will translate to the theater.  During theater test screenings,  some very delicate dialogue scenes were being eaten up  by the air conditioning rumble  and audience noise in a real theater.  So they created a calibrated noise generator  labeled popcorn noise,  which could be turned on and added to the monitor mix  whenever they wanted to check a particularly soft passage.  The art of increasing dynamic range.  Increasing dynamic range can also make a song  sound more exciting by using contrast  or by increasing the intensity of a peak.  The key to success here  is to recognize when an enhancement has become a defect.  Musical interest can be enhanced by variety,  but too much variety is just as bad as too much similarity.  Passages that are too loud compared to the average  can disturb listeners,  especially those playing music quietly as background,  which sadly seems to be becoming more the norm.  Another reason to increase dynamic range  is to restore or attempt to restore  the excitement of dynamics that were lost  due to multiple generations of compression  or tape saturation.  The four varieties of dynamic range modification.  We always use the term compression  for the reduction of dynamic range  and expansion for its increase.  There are two varieties of each,  downward compression and upward compression  and downward expansion and upward expansion.  Downward compression is the most popular form  of dynamic modification.  It brings high level passages down.  Limiting is a special case.  It is downward compression, but with a very high ratio.  Examples include just about every compressor  or limiter you have ever used.  Downward compression can easily be done manually  by simply lowering the level of loud passages  without introducing the artifacts of compression processors.  Note that compression processors can decrease micro dynamics  while manual compression does not.  Upward compression raises the level of loud passages.  This too can be done manually or through a processor.  For example, the automatic gain control,  which some broadcasters use to make things louder.  It's a type of compressor frequently used  in consumer camcorders,  while pumping and breathing artifacts  have given automatic gain control a bad name.  Downward expansion is the most commonly used  type of expansion.  It brings low level passages down further.  Most downward expanders are processors  employed to reduce noise, hiss or leakage.  A delicate noise gate is a special case.  That is downward expansion with a very high ratio.  Upward expansion takes high level passages  and brings them up even further.  This can be done manually,  thereby increasing macro dynamics  or with a processor cord and upward expander,  which can increase both macro and micro dynamics.  Upward expanders are relatively rare.  In skilled hands, they can be used to enhance dynamics,  increase musical excitement or restore lost dynamics.  The art of manual gain riding,  macro dynamic manipulation.  During mixing, it is difficult to simultaneously  pay attention to the internal balances  and the dynamic movement of the music  from section to section.  For example, verses and choruses.  Sometimes engineers inadvertently lower the master fader  during the mix to keep it from overloading.  If performed during a build,  this will strip the climax of his impact.  In mastering, we can enhance a well-balanced rock or pop mix  by taking the dynamic movement of the music  where it would like to go.  Delicate level changes can make a big difference.  It's amazing what a single decibel can accomplish.  It's also important to make sure  that the client's own level change  was not intentional before attempting a correction.  The art of changing internal levels of a song.  How and when to move the fader.  Artistic level changes can really improve a production,  but they need to be made in the most musical way.  To this end, internal level changes are at least intrusive  when performed manually by raising or lowering the fader  as little as a quarter of a decibel at a time,  as opposed to using processors,  such as compressors or expanders,  which tend to expose that action.  When riding the gain,  aim to just augment the natural dynamic flow.  If the musicians are trying for upward impact,  pulling the fader back during a crescendo  can be detrimental since it will  diminish the intended impact.  Extra soft passages require special attention.  If the highest point in the song  sounds just right after processing,  but the intro sounds too soft,  it's best to simply raise the intro,  finding just the right way to restore the gain  using one or more of these approaches.  A quick edit and level change at a transition  between the raised level intro and the normal level body.  This can have a nice effect and be the least intrusive.  If that doesn't sound good,  try a long gradual lowering of the gain,  which might occur at the end of the intro  or slowly during the first verse of the body.  If that doesn't work, then after the raised intro,  try a series of quarter or one decibels downward edits,  taking the sound down step-by-step at critical moments.  This is useful when we don't want the listener  to notice that we're cheating the gain back down  and we may be forced to work against the natural dynamics.  Retaining dynamic impact while reducing dynamic range.  Some soft passages must be raised,  but if the musicians are trying  to play something delicately,  pushing the fader too far can ruin the effect.  The art is to know how far to raise it  without losing the feeling of being soft  and to find the ideal speed to move the fader  without it being noticed.  In a door, physical fader moves are replaced by crossfades  or by drawing gain changes on an automation curve.  The mastering engineer's aim is to be invisible.  If the sound is being orderly manipulated,  the job has not been done properly.  A technique for decreasing dynamic range  in the least damaging way is to lower the gain  at the end of the preceding soft passage  before the loud part begins.  This is, of course, if we need to take a loud passage down.  Look for a natural dip or decrease in energy  and apply the gain drop during the end of the soft passage  before the crescendo into the loud part.  In other words, take down the level during a decrescendo,  not during the loud passage that follows.  That way, the loud passage  will not lose its comparative impact  for the ear-judged loud passages  in the context of the soft ones.  Before I dig into the compression, equalization,  and depth and dimension tips and tricks,  there's one more important concept  that we need to first understand  as it will influence our choices in these categories,  and it's loudness normalization.  What is loudness normalization?  Loudness normalization is the process  of correcting the level of a program  to a standardized target level.  It is a simple gain or attenuation value,  a level control which can be applied on broadcast  or in the case of a media player like iTunes  just before hitting play on the selected song or movie.  For example, the EBU has defined the standard target  for program loudness for both European radio and television  as minus 23 LUFS plus one dB.  The ATSC has defined a minus 23 LUFS plus two decibels.  So the two standards are reasonably compatible.  The target is drama neutral.  It applies to all dramas from spoken word to heavy metal.  A standardized measure of loudness.  The loudness revolution has arrived,  and it's widely implemented  throughout European broadcast networks  and after December 13th, 2012 on US television.  This will eventually affect all recorded sound  in specific ways.  In Europe, both TV and radio sound levels  are measured by the international ITU standard,  BS.1770-3 and are regulated  by the European Broadcast Union's recommendation, R128.  In the US, levels are regulated  by the Advanced Television Systems Committee,  ATSC for short, A85 specification.  The European standard was adopted  to obtain more consistent sound levels  and encourage better sound quality  by requiring much less dynamic processing.  In the US, however, the driving factor  has been to eliminate loud television commercials  via the CARM Act, which was passed by the US Congress,  Commercial Advertisement Loudness Migration Act.  But the result is similar,  provided that stations do not cut corners  and implement shocking processing  instead of regulating levels.  Some of the measures defined by these standards  appear on this diagram of a recorded audio file.  The purple area represents the loudness range  of this recording from the softest to the loudest passage  with its average loudness, also known as program loudness,  in LUFS at the marked line.  Its crest factor, which is the peak to average ratio,  is measured from its average loudness  to the highest peak of the material,  which is at the top of this yellow area.  Its headroom, as it could be defined,  is the maximum potential crest factor,  the distance between its average loudness and full scale.  For example, if a recording's average loudness  is minus 23 LUFS and its maximum peak level  is minus three dBFS,  then it has a 20 decibel crest factor  and 23 decibels of headroom.  Until now, headroom has been defined  as the distance between the highest peak level  and the full scale, or peak headroom.  But now that the ITU  has defined the standardized average loudness,  it makes sense to define headroom in relation to loudness.  The ratio between the average  and the peak level of a recording  directly affects its sonic character,  but the effect depends on the style of music.  In percussive music, for example,  having a relatively large distance  between the average and the peak level is important  because percussive music includes not only drums,  but also the tap of a guitarist's hand  on his guitar body for musical emphasis.  But peak-to-average ratio is not that important  in non-percussive music,  which includes everything from string quartets  to solo vocal recordings.  Engineers manipulate peak-to-average ratio,  or crest factor, for special effect by using compressors.  Sample drum sets using hip-hop are often highly compressed,  with little peak energy above the average energy,  so they can be made to sound very loud,  and the subsequent distortion  is part of the language associated with that musical drama.  Peak normalization exaggerates loudness differences.  It is natural to have programs and musical styles  with different peak-to-average ratios and different sound.  The prelim is that ever since the CD was invented  in Ciara 1980, engineers have been trying  to peak all recordings to digital full scale,  which causes extreme loudness differences  between percussive and non-percussive recordings,  and between processed and unprocessed recordings.  This diagram compares the level of a string quartet  and a symphony orchestra,  both adjusted until their peaks hit full scale,  which is called peak normalization.  The symphony orchestra needs much more peak headroom  to accommodate all of the momentary transients  in the material,  so peak normalization exaggerates the loudness difference.  It makes the string quartet sound 10 to 14 decibels  louder than the symphony.  This is especially problematic  when constructing playlists in iTunes.  Peak normalization encourages over-processing.  Peak normalization also encourages program producers  to compare or limit material  in order to gain a loudness advantage.  Even classical music producers  have asked mastering engineers to peak limit  and raise the level of their program  so it won't sound quieter than a competing product.  Peak limiting can easily cause  the sound quality to deteriorate.  Small amounts of peak limiting may be inaudible to some,  but critical listeners will notice  the deterioration of the clarity,  the increase in the distortion,  and the softening of the transients.  Hard rock, metal, pop music,  and other heavy processed genres  have suffered during the loudness race.  Drums, particularly snare and bass drums in hip hop,  have lost their punch.  There's just not enough room in this tiny yellow area  to properly express the transients,  as you can see on the diagram here.  Loudness normalization.  Radio and TV have always regulated sound levels  to produce a more consistent consumer experience.  They have also engaged in loudness wars  to gain higher ratings.  In the past, broadcast loudness regulation  was done by using SOFIA processing  and very strong compressors and limiters  that squashed the sound, causing SOFIA distortion.  The distortion multiplies when already distorted,  hyper compressed CDs are played on the radio.  During the days of analog broadcasting,  there was probably no way to stop the practice  of SOFIA processing,  but digital techniques permit all broadcast audio,  except obviously for live broadcasts,  to be stored in computer files,  which can be analyzed ahead of time  and adjusted to the consistent loudness  by simply adjusting their gain without further processing.  Sonically, this offers a big advantage over the old way.  Once the standard organizations  defined a loudness measurement standard  and regulations came into place,  true loudness normalization was underway in broadcast.  For example, the classical and pop recordings  the diagram is showing here can be loudness normalized  by placing each of their average loudness points  at the same target level as this diagram shows.  The influence of radio broadcasting  on music production techniques.  Radio broadcasting has always had a profound influence  on the sound of music and how we produce it.  It is believed that the advent of loudness normalization  radio production in Europe  is starting to have an effect  on how we all produce and engineer music.  Consider this scenario.  Whether you are producing popular or classical music,  EBU digital broadcasts normalize all recordings  to minus 23 LUFS, if they are not already at this level.  A well-managed US TV  or European digital broadcast network  does not need to change the sound of your recordings.  They maintain its crest factor  since they use very little processing.  For example, let's imagine that an acoustic music song  was originally mastered with a crest factor of 11 decibels,  which you can see here is figure A.  And everyone is happy with the sound.  One contributing factor  is that the relatively high crest factor  influences its clarity and impact.  However, artist management decides it sounds too low  compared to the competition.  Keep in mind that the management need not be concerned  about how loud the song will sound on the radio,  which broadcasts everything at the same average loudness.  But they do want to impress the radio program's directors  their loudness when the program directors audition the CD  for the first time.  So they ask the mastering engineer to make it louder.  He adds two decibels of peak limiting,  which raises the average level two decibels  and reduces its crest factor, figure B.  For better or worse, this becomes the release CD,  not just for the program directors to hear one time,  but for posterity.  When this CD is broadcast on US radio,  its hot level interacts with the extreme processing,  creating strong distortion  and the song loses impact and clarity,  especially on typical car speakers.  But it sounds no worse than any other hot CD  on popular US stations.  However, in European digital broadcasting,  the song is not processed.  The station simply lowers its average level  to match that loudness standard of minus 23 LUFS, figure C.  And the sound is as lacking like that of the release CD.  Soon the producers discover that on European radio,  their song doesn't sound as good  as some of the competing songs.  It has less impact and is less clear on the radio  than some of its competitors with a lower CD level.  In response, the producers released the original master  as a new radio version, figure D,  which sounds much better on the digital radio  without the extra peak limiting.  There's more transient clarity,  it's louder, bigger and better than its competition.  All this because European radio has not processed the sound  other than to normalize its loudness.  Note that example to A and D should sound the same  if the playback level is adjusted to be the same.  But example D shows that a loudness normalized medium  gives the producer the freedom  to decide the sound quality he wants  without causing an increased crest factor  to run into the top of the medium.  I'm not claiming that crest factor is the only  or the most important influence  on the sound character of any recording.  It's just that in the ultimate days of the loudness race,  crest factor has been reduced so much  that it cannot help being a strong limiter.  Incidentally, this new radio version  also sounds better on US radio  because it does not push the processor as far.  Essentially, the producers have done an end run  around the radio program director who picked their product  when they gave him that hot CD.  Now they can make it sound better on the radio.  This experience reveals one thorn in the side of the effort  to end the loudness race,  the manner in which program directors evaluate artist CDs.  They often put in one CD, listen for a very short time  and then put in another with little attention  to its musicality or dynamics.  Under those conditions, there's no question  they are influenced by the loudest disc,  forgetting that the loudest disc sounds worse on radio.  It loses the race.  If the disc is not loud enough  to meet their preconceptions, they reject it.  So the program director becomes a major influence  on the sound quality of our recorded product.  If program directors switch to evaluating music  using iTunes Soundcheck,  that would make all the difference in the world.  iTunes Soundcheck technology.  The cross-genre iTunes playlist exhibits big loudness jumps,  especially between older and newer material.  The solution for iTunes as for broadcast  is loudness normalization.  Soundcheck is iTunes' loudness normalization technology.  It can be enabled in the iTunes preference settings.  From there on, it will work transparently and seamlessly.  Upon playback, the gain of each file is adjusted  to eliminate sudden increases or decreases  in track-to-track sound level as you listen.  The music files themselves are not altered in any way  by this process, which is reversible.  With Soundcheck activated,  modern hip-hop can play next to classic rock,  and dynamic material can sit effectively  next to compressed material in any tailor-made playlist.  Soundcheck, on by default.  I believe Soundcheck will be turned on by default  in future versions of iTunes and Apple iOS devices,  though Apple has made no announcements about this  as of the upload date of this video.  Soundcheck, on by default, will be a game-changer.  Although producers have no control  over the playback loudness of their product,  they can create a new product  with much more control over its clarity,  macrodynamics, microdynamics, and spatial characteristics.  The word competition takes on a whole new meaning  with loudness normalization.  Life in the music world just gets better.  When producers listen to their song on iTunes  with Soundcheck turned on,  they discover that over-processed material sounds worse.  Neither your music playback platforms  are capable of loudness normalization.  New cars come equipped with aux connections,  and the aux and iTunes are rapidly replacing  the compact disc player in the car and at home.  Other playback systems and some digital music services,  such as Spotify, are also using  loudness normalization technology.  The benefits of Soundcheck.  During the mastering session,  producers can rest assured that Soundcheck  will maintain their albums as loud  as any of its competitors.  Soundcheck offers many advantages  for both producers and consumers,  such as producers can make choices  about how much distortion or dynamic manipulation  they want to apply without worrying  that their recordings will sound softer  than the competition.  The mastering engineer does not have to combat  severe compression by over-equalizing.  Bass drums in hip-hop recover their boom,  punch, and crack.  Peak limiting and clipping  return to optional creative tools,  allowing drums to sound louder and more effective,  since excessive processing can be avoided.  Snare drums and snappy instruments sound lively again.  The word headroom finally means something.  Choruses sound louder than verses once again.  Space and depth are restored,  and music lovers can rediscover  the sound of classic recordings,  which no longer sound much too soft  compared to contemporary pop recordings.  Listeners play hip-hop and metal loudly,  because that's how these genres are meant to be played.  But over-compressing a modern hip-hop or metal album  to make it sound louder is just band-aid.  As soon as Soundchecker is engaged,  severely compressed pop material  is revealed as sounding flat and lifeless,  and all the weaknesses of over-processing become exposed.  Once the efforts of hyper-compression are uncovered,  a lot of producers and engineers  will be scrambling to make music sound good again,  not just loud.  The only way to restore dynamic excitement  is by remixing and or remastering.  So to protect your precious catalog  every time you have an album mastered,  remember to produce an archive  of high-resolution dynamic masters  that can benefit from iTunes' Soundcheck.  Everything loud, then everything else.  The intent of loudness normalization systems  is to regulate loudness for the consumer,  as well as eliminate a loudness war.  But don't bother to ask a producer  to voluntarily make his production sound equal  to its competition if he can make it sound louder.  Loudness normalization systems  store the loudness information of a song in metadata,  which, in Dolby's Dynorm system,  is in the hands of its producers.  So any program producer can literally hack the metadata  to make a song apparently louder.  This includes discs encoded in Dolby Digital  and Dolby True HD,  resulting in a perversion of Dynorm's entire intent.  Nearly all music DVDs and Blu-ray discs  have been given a Dynorm value of 31,  the highest gain, by their program producers.  For all of these discs,  the playback system applies the maximum possible gain,  which makes a mockery of loudness normalization.  There's no turning back the clocks  once a single competing disc has been pressed and released  with incorrect Dynorm.  It sets off the wall  and lets the loudness genie out of the bottle.  For these reasons, many popular music video discs  have been produced with as much distortion  as the hottest pop CDs.  Soundcheck means no cheating.  There's only one way to put the genie back in the bottle,  use a secure normalization system.  This marks an important distinction  between Apple's Soundcheck and other systems.  Soundcheck means no cheating.  Soundcheck is a closed system,  putting loudness normalization securely  under the control of an independent third party.  Apple is securely in control of the integrity of audio  from delivery of the masters.  A unique vendor encodes all the loudness metadata,  protects that data, securely distributes, sells,  and in the case of streaming, broadcasts the material.  Big Brother may be watching,  but his intentions are beneficial,  protecting the interests of all music producers  who might otherwise forge loudness data to their advantage.  Apple does provide tools for producers  to evaluate Soundcheck metadata in advance.  Engineers can make test encoders,  see how the material sounds in comparison to the competition  and inspect the Soundcheck values  that will be given to their material.  But this metadata cannot be submitted to Apple.  They control the final encode  and securely compute the loudness metadata.  The Soundcheck value Apple gets  will be the same as the one you get  by importing your music into iTunes.  Noise is not a problem.  With loudness normalization,  system noise does not come up perceptibly,  even though the audio levels of some products  have been lowered.  That's because listeners keep their volume controls  at a reasonable constant gain.  It's a myth to say that we have to up all the bits,  since peak bits have little or nothing to do  with perceived loudness or signal-to-noise ratio.  Album normalization now.  Producers and mastering engineers  adjust the levels of songs in an album  according to how they feel.  They do not regulate them to the same measured loudness.  Adjusting all songs to the same loudness will be silly.  Ballads would be the same level as rockers,  and the slow, relaxed movements of symphonies  would be just as loud as the final bombastic movement.  Even when a soft movement from an album  is placed into a playlist with loud material,  it's usually better to respect the original relationship  of the soft piece to the loud numbers.  When Frank Sinatra sings a ballad,  it's supposed to sound a little softer.  He can mix quite well in the same playlist with rockers  in an album-normalized system,  as can be seen by this diagram.  The dashed line here  represents the average loudness of a song.  In figure A, we combined Frank with the Beatles  without any loudness normalization.  Without normalization,  the loudest song on this Sinatra album  is as soft as the softest song on the Beatles album,  because the albums have previously been peak-normalized.  Each album plays fine within itself,  but in a mixed playlist,  either Frank has to be turned up  or the Beatles have to be turned down.  So this mixed drama playlist doesn't work at all.  In figure B, we perform a per-track normalization,  which adjusts each song level to the same loudness,  the target level line.  But the soft songs now sound way too loud.  In fact, the album sounds compressed  because songs that are intended to be soft are made loud.  Consequently, the loud numbers aren't swinging as they should  compared to the soft numbers.  Frank and the Beatles perform together  without any musicality.  In figure C, we perform album normalization,  which adjusts the loudest song of each album  to the target level.  Now the loud songs of both performances are set the same,  and the soft songs fall into blissful proportion.  Notice that the Sinatra loud song  has a larger peak-to-loudness ratio  than the Beatles loud song,  so it proudly sounds more lively and open.  The Beatles producers compressed the song  to that degree for artistic purpose,  but assuredly, once loudness normalization  becomes a default,  many producers will regret  some of the compression choices they had made  as a lack of impact becomes painfully obvious.  Now that we understand the art of dynamic range  and loudness normalization,  as well as how it will affect our masters,  I'd like to move into what I'd consider  as the main body of this tutorial, compression.  For the first two or three years of my journey  as a music producer, I, as many do,  fail to really grasp the topic of compression.  More specifically, when to deploy the tool  and what the controls contained within one actually did.  Compression is a tool  that can change the inner dynamics of music  by enlivening low and mid-level passages,  enhancing rhythmic movement,  or producing a stronger musical message.  There's a great quote from Einstein that reads,  if you can't explain it simply,  you don't understand it well enough.  I've deployed this quote  into the philosophy of these definitions.  The harder ones will have the most simplistic definition  I could come up with,  followed by the complex, more technical explanation.  Controls of the compressor.  We'll start with threshold.  Simply, the threshold is one word, level.  The threshold of a compressor is the level  at which gain reduction begins.  Ratio.  The ratio is the amount.  A compressor's ratio describes the relationship  between input and output above the threshold.  For example, a simple compressor  with a fairly gentle 2.5 to 1 compression ratio  and a threshold at around minus 40 dBFS.  2.5 to 1 means that an increase in the input signal  of 2.5 decibels will yield an increase  in the output of only 1 decibel.  Or for an input rise of 5 decibels,  the output will rise only 2 decibels.  Try to remember threshold as level and ratio as amount.  Gain makeup.  The gain makeup is a simple gain amplifier  after the compression section  that allows the average level of the material to be raised  while the loudest passages are still brought down.  Gain reduction.  The gain reduction meter in a compressor  tells us when the signal has exceeded the threshold  and how much the compressor  is reducing the gain of the signal.  Knee.  The portion of the curve near the threshold  is called the knee,  which marks a transition between unity gain  and compressed output.  In limiters, the knee should be very sharp.  Compressors can have hard or soft knees.  Soft knee refers to the rounded knee shape  or a gentle transition,  and hard knee refers to a sharper shape  where the compressor reaches full ratio  immediately above the threshold.  A soft knee can also sweeten the sound of a compressor  near the threshold.  For those models of compressors that have only hard knees,  some of the effects of a soft knee can be simulated  by reducing the ratio or raising the threshold,  which will result in less action by the compressor.  Attack time.  Attack time is the time it takes for a compressor  to implement a full gain reduction  after the signal has crossed the threshold.  Because digital compressors can react  with essentially infinite speed,  a digital compressor set to 100 milliseconds  may sound similar to an analog compressor set,  say to 40 milliseconds.  The method the designer uses to define attack  is not standardized,  so it is not possible to compare specific attack times  between brands.  It's better to remove all labels  besides slow and fast and just listen.  With digital compressors,  typical attack times used in music for mastering  can range from 30 milliseconds to 300 milliseconds  or even longer on some occasions,  with the average time used  being probably around 100 milliseconds.  But go by your ears, not the numbers.  To help set your attack time,  listen to the percussive and transient quality of the music.  Shorter attack times soften transients  and produce the more closed sound.  Longer attack times let the music breathe  and reveal more of the percussive transients.  Release time.  Release time or recovery time is how long it takes  for the signal to return to unity gain  after it has dropped below the threshold.  Typical release times used in music  range from 50 to 500 milliseconds  or as much as a second or two,  with the average being somewhere around  150 to 250 milliseconds.  The terms short or fast with attack or release times  are used interchangeably,  as are slow and long attack and release times.  Manufacturers may measure times to 90% of gain reduction  or use another empirical approach to define them.  Release time is probably  the single most influential setting  affecting the sound of a compressor.  Super fast release times can help to make the sound appear  unrelentingly loud and aggressive,  and slow release times are more gentle on the sound.  Analog optical compressors have a fast initial release  and then a slow final release,  which yields a more gentle aspect and a bloom to the sound.  VCA compressors produce the reverse effect,  which can aid in producing a more aggressive sound quality.  Digital compressors attempt to emulate  one or the other of these analog characteristics,  or to be switchable to do either.  A good starting point for a digital compressor  on mixed music is to set the attack time  to about 100 milliseconds  and the release to about 250 milliseconds,  then listen and adjust.  If you want a more punchy, aggressive sound,  shorten the release slightly,  and if useful, shorten the attack from there.  Higher ratios, harder knees, and greater gain reduction  also contribute to a more aggressive sound,  but be careful, when a parameter is turned too far,  the sound loses its definition and punch.  As with any process, less is more.  With digital limiters, release time is very important.  The faster the release time,  the more invisible the limiter can be.  It jumps in, quickly controls the transients,  then gets out of the way.  The fastest digital limiters have a release time  of only one millisecond.  However, super fast release times  can cause significant distortion.  This is why most successful digital limiters  have an auto release control,  which slows down the release time  if the duration of the limiting  is greater than a few milliseconds.  The effective release time of an auto release circuit  can be as short as a couple of milliseconds  or as long as 50 to 500 milliseconds.  All of this is intended to make the digital limiter  as invisible as possible.  Part of the sonic aggressiveness of a fast release time  comes from the distortion at low frequencies  that can occur if the release time is too fast.  Look ahead.  The preview or look ahead function  allows very fast or even instantaneous attack time,  which is especially useful in a peak limiter  to prevent overloads.  This unit can effectively react to the transients  before it has even occurred.  This requires a delay line,  so analog processors do not have a look ahead.  When there is a look ahead,  the attack time can be as short as we desire,  controlling any peak that concerns us.  Look ahead is only relevant  when we want a short attack time,  since if we want a long attack time,  then we probably also want to let  the initial transients pass through.  So look ahead is probably unnecessary  for attack times longer than about 10 milliseconds.  Crest factor control.  Some compressors provide a crest factor control  expressing decibels or a range from RMS or full average  to quantity peak through full peak.  This means that the compressor can be set to act  on the average parts of the music,  the peak parts, or somewhere in between.  Compressors with RMS characteristics sound more natural  because they correspond with the ear's sense of loudness.  But one of the best sounding compressors out there  is peak sensing.  When the crest factor control is set to peak,  short transients tend to control the action,  and at RMS, more continuous sounds control it.  Compressors with unique characteristics.  A subject I see discussed online  mainly by producers who are new to music production  is the difference between different compressors.  Perhaps they've seen a masterclass  in where an engineer may use different digital  or analog compressors throughout a mix,  and they're unsure as to why this is the case.  Part of the fun in using compressors  is discovering the specialties  of different brands and models.  Even with the same settings,  some are smooth, some are punchy,  some nicely fatten the sound,  and others make it brighter, harder, or more percussive.  This is often due to the differences  in the curve or acceleration of the time constants,  the attack and release times,  how the device recovers from gain reduction,  and whether the gain returns to unity  on a linear, logarithmic, or even a irregular curve.  Analog compressor designers  choose from several styles of gain manipulation.  The most common are FET, Field Effect Transistor,  Optical, abbreviated Opto,  VCA, Voltage Controlled Amplifier,  Vari-MU, PWM, Pulse Width Modulation,  and their various subcategories.  Digital designers may emulate their characteristics  as in the Wave Renaissance series of digital compressors  that have both Opto and Electro models.  As mentioned before, in Opto,  the release time slows down  for the last portion of the release,  while in Electro, it accelerates.  Electro can yield a more aggressive sound,  while Opto is good for gentle, easygoing purposes.  Analog optical compressors are great on vocals  in tracking or mixing,  but not as good for aggressive mastering  of overall program material  because they are generally too slow.  However, digital Opto models can be faster  than their analog counterparts.  Generally, analog optical models  are more suitable for gentle mastering.  Multiband processing.  For most downward compression purposes,  multibands are rarely needed.  One or two bands are usually enough.  However, splitting our compressor's signal  into multiple bands avoids the problem of modulation  with a single sidechain,  since compression in one band  will not affect another band.  For example, the vocal will not pull down the bass drum  or vice versa.  This is perhaps the biggest selling point of multiband  because with the same amount of gain reduction,  it can sound superior to wideband  or sidechain equalization.  The action can be made very invisible.  Also, a higher amount of compression  and average level can be achieved in a multiband  with fewer interaction or clamping artifacts.  Another advantage is that high frequency transients  can be left unaffected  while compressing the midrange more strongly,  producing a brighter, snappier sound  than a single band unit.  But when the thresholds are set aggressively,  loud action in one frequency band  can dynamically change the overall tonality,  producing a non-cohesive sound,  especially if all bands are moving different amounts  throughout the song.  The multiband device's virtues  permit louder average levels  than were previously achievable,  making it the most powerful,  but also potentially the most deadly  audio process ever invented.  Deadly because multiband compression fuels the loudness race.  The technique has been hyped as a cure for all ills,  which it is not,  and can easily produce a very unmusical sound  or take a mix where it doesn't want to go.  When multiband processing is used,  the line between equalization  and dynamic processing becomes nebulous  because the output levels of each band  form a basic equalizer,  and if the bands have different thresholds,  then they affect the tonality dynamically.  Use standard equalization  when instruments at all levels need alteration  or use multiband compression  to provide spectral balancing at different levels.  This is a form of dynamic equalization,  so depending on one's point of view,  a multiband compressor can be looked upon  as a dynamic equalizer.  Use multibands as a last resort.  Fix the disease at its source,  not with a multi-band aid in mastering.  Limiters.  Just a quick side note in case anyone is wondering  why limiters do not have their own chapter.  Limiting is a type of compression.  A limiter is a compressor.  However, not all compressors are limiters.  Limiters have a very high ratio,  typically 20 to one all the way to infinity to one.  Moving on to equalization.  Introduction to EQ in mastering.  As far as music production is concerned,  equalization is really just a fancy word for tone control,  a device that can cut or boost  particular parts of the audio spectrum.  EQ can be used to create new tonalities  and to help correct or equalize problems  that occurred in the recording chain,  though you should always strive  to fix such problems first.  There are three main controls you'll be using  on an equalizer.  Gain.  Once you've pushed in a band,  the first control you should tweak is the gain control.  The EQ has no effect without some gain reduction or addition  no matter what you do with the other controls.  Gain determines how much of a certain frequency  is added or removed.  It is the vertical axis on the EQ graph  and the taller it is,  the more that frequency is being added.  Q.  Q determines how wide or narrow the EQ band is.  A setting of zero will pretty much encompass  the entire spectrum depending on your gain amount,  while a setting of 10 will only affect  a very small range of frequencies.  The third control is frequency.  This determines which frequency the band affects.  How we use an equalizer in the mastering stage  can differ greatly from how one is used during mixing.  I'd now like to look at some equalization principles  and techniques that are common in mastering.  An EQ technique used in mastering  can be crucially different  from an apparently similar technique used in mixing.  For example,  when mastering,  adjusting the low bass of a mix  will affect the perception of extreme highs.  Similarly,  if a snare drum sounds dull,  but the vocal sounds good,  then the voice may suffer  when you try to equalize for the snare.  These problems occur even between different elements  in the same frequency range.  During mixing,  bass range instruments that exhibit problems  in their harmonic range can be treated individually.  But in mastering,  their harmonic range overlaps  with the range of other instruments.  Another key to effective mastering  is that everything starts with the mid-range.  The fundamentals of the vocal,  guitar, piano,  and other instruments must be correct  or nothing else can be made right.  Parametric and shelving.  There are two basic types of equalizers,  parametric and shelving,  named after their characteristic curves.  Parametric EQ,  invented by George Mason Berg in 1967,  is the most flexible curve,  providing three controls,  center frequency,  bandwidth,  level boost,  or cut.  The parametric curve,  also known as the peaking or bow curve,  is also the most popular EQ shape  used in mastering  because it can be used surgically  to remove certain defects,  such as an overly resonant bass instrument  or enhance narrow ranges of frequencies.  By comparison,  shelving equalizers are more popular  in mastering than in mixing,  since they provide boosts or cuts  to the entire spectrum  below or above a selected frequency  and can alter the tonality of the entire mix.  Gentle equalizer slopes  almost always sound more natural  and less harsh than sharp ones.  So Q values of 0.6 and 0.7  are therefore very popular in bow shapes  and gentle slopes in shelving shapes.  Higher or sharper Qs,  greater than two,  are used surgically  to deal with narrow band resonances  or discrete frequency noises,  though we must listen for artifacts  of high Qs,  such as ringing.  Finding the right EQ frequency  for dipping resonant notes.  There are two techniques  for finding a problem frequency  that is resonating and must be dipped.  The classic approach  is to focus the equalizer directly,  starting with a large boost  and a fairly wide or low value Q.  Sweep through the frequencies  until the resonant is most exaggerated.  Then narrow the Q to be surgical  and finally dip the EQ the amount desired.  It is, however,  very difficult to sweep in the bass region  because the distance between F sharp and G  is only three Hertz,  while in the mid range,  the distance is 22 Hertz.  We've all heard the phrase one note bass  and there's a reason why this problem occurs.  Many rooms have standing wave problems in the bass  that give the mix engineer the wrong impression  that a note is too weak.  So we boost it unnecessarily.  The cure for one note bass on the mastering side  is quite delicate.  We have to construct a bow filter  that's only a few Hertz wide.  Narrow filters can ring,  so be exceptionally careful.  EQ yin and yang.  As I stated in the introduction of this video,  mastering is a compromise.  With equalization using the mastering stage,  it's important to remember the yin and the yang.  Contrasting ranges have an interactive effect.  For example,  adding low frequencies makes the sound seem duller  and reducing them makes it seem brighter.  Adding extreme highs between 15 and 20 kilohertz  makes the sound seem thinner in the bass  and lower mid range and vice versa.  A slight dip in the lower mid range around 250 Hertz  reduces warmth and has a similar effect  to boosting in the presence range around five kilohertz.  A thick vocal can be helped  either by reducing the lower mid range  or by adding presence or both.  Yin and yang considerations allows to work  in either or both contrasting ranges,  whichever is most effective.  When the overall level is too high,  pick the range you need to reduce.  When an instrument exhibits upper mid range harshness,  pick the frequency range that will have the least effect  on other instruments playing at the same time.  Using Baxnodal for air.  The air band is the range of frequencies  between about 15 and 20 kilohertz,  the highest frequencies we can hear.  The third and important shape  that's extremely useful in mastering is the Baxnodal.  Hi-fi tone controls are usually modeled around this curve.  Like shelving equalizers,  a Baxnodal curve is applied to low  or high frequency boost or cuts.  However, instead of reaching a shelf,  the Baxnodal continues to rise  or dips if cutting instead of boosting.  This gentle shape often meets the ear's desires  better than a standard shelf,  especially for whole mixes,  which is why the Baxnodal is very popular in mastering.  Be careful when making high frequency boosts.  They are initially seductive,  but can easily become fatiguing.  The principle of yin and yang reminds us  that the ear interprets a high frequency boost  as a thinning of the lower mid range or bottom end.  In addition, when the highs come up,  the cymbals, triangles, and tambourines become louder,  which changes the balance of rhythm to melody  for better or worse.  High-pass and low-pass filters.  Although pass filters can be used  to solve noise problems in mastering,  they can also introduce problems of their own  because they affect everything above or below  a certain frequency range.  High-pass filters can reduce rumble,  thumps, peep-ops, and similar noises.  Low-pass filters are sometimes used to reduce hiss,  but since the ear is most sensitive  to hiss in the three kilohertz range,  a paramedic dip around that frequency is more effective  than a radical low-pass filter.  For hiss removal, we usually prefer  specialized noise reduction solutions over static filters.  The limitations and the potential of the recording.  If you wait until the mastering stage  to fix certain problems, this invites compromise  because there is only so much  that can be done in mastering.  But sometimes mix engineers try to fix things  that don't need repair or over-process a recording,  only making it sound worse.  They do this because the tool is available  and it's tempting to use,  their monitoring is misleading,  or because of lack of experience.  The same thing can happen  to an inexperienced mastering engineer too.  This is where it plays for the mixing engineer  to consult with an experienced mastering engineer  before the mix is done.  There is little we can do to fix a recording  where one instrument or voice  requires one type of equalization  and the rest require others.  Cone filtering is a complex problem  that is not easily cured with an equalizer.  Besides, in mastering, EQ affects the entire mix,  not just the offending instrument or voice.  It's best to first discuss the problem  with the mix engineer to see if he can address  the offending track and remix it.  If that's not possible, then possibly ask for a stem  or as a last resort, try an overall mastering EQ.  For example, a lower mid-range EQ boost  to help a vocal that sounds thin  due to cone filtering,  even if it only touches one band of the cone filter.  A perfect mix needs no mastering processing at all.  Because of this, don't automatically begin equalizing,  but listen and evaluate first.  Many recordings that sound great  leave the mastering studio  with no equalization or processing.  Linear phase equalizers, the theory.  All current analog equalizer designers  and nearly all current digital equalizers  produce phase shift when boosted or cut.  That is, signal delay varies with frequency.  The higher the Q, the more phase shift.  This kind of filter will always alter  the musical timing and wave shape,  also known as phase distortion.  Whenever you have to equalize,  you will always alter the signal  in both the time and frequency domains.  There will always be a time artifact.  In the analog-styled equalizer,  which is usually mathematically termed minimum phase,  the alteration will be primarily  to spread the signal downstream,  i.e. it does not lead the original signal by much.  A downstream modification translates  into different delays at different frequencies,  dispersing the original signal.  In some cases, this effect is quite audible.  If one uses a digital approach,  one can either mimic the analog behavior  or use a linear phase, aka constant delay filter.  This filter will equally proceed and follow the signal.  Part of the filter may create a pre-echo effect,  modifying the leading edge of the transients  and signal changes.  A high Q linear phase filter  can introduce audible pre-echo  in the short millisecond range.  It's exactly like a floor bounce,  but without the cone filtering.  Anytime that a high Q filter is used,  careful listening with both types of equalization  may be necessary to decide which choice is best.  If you're looking for an aggressive sound,  perhaps minimum phase is your best choice.  But if you're looking for a natural sound,  particularly at high frequencies,  linear phase is your best choice.  Narrow band peaks and dips  can be accomplished in linear phase,  avoiding the smeary quality  that occurs in minimum phase with sharp bands.  The majority is still out on which type of equalizer  is best for low frequencies.  Mid-side mastering.  The mid-side technique can be used  to gain a bit more control  over the separate elements in our recording.  MS stands for mid-side or mono stereo.  Mid-side recording techniques  are favored by many engineers  for a number of reasons during the recording,  mixing, and mastering stages,  and it's the last that we'll look at here.  With some very simple maths,  it's possible to encode the standard  left-right stereo mixes to mid-side.  You'll still have two channels,  but instead of left and right,  you'll now have control over the mid,  that's anything panned centrally,  and the sides, anything to the left or right,  lumped together.  After independently processing or level balancing,  the mid-side signal can be decoded back to left and right.  Mid-side gives you a very different type of control  during the mastering stage  because new opportunities emerge.  You could EQ or compress wide-panned elements,  such as guitars, or effect returns,  without affecting typically central panned elements,  such as the kick, snare, bass,  or the lead vocal, for example.  Listen carefully for mid-side trade-offs  with experienced ears.  When raising the mid-channel to increase the vocal,  the stereo width narrows, and vice versa.  Changing the width alters the mids.  Mix engineers who rely largely on near-field monitors  often produce stereo-compromised mixes  because listening on near-fields  is like having a big set of headphones,  which exaggerates the stereo separation.  Using mid-side to increase the width  is initially attractive,  but can easily produce an unfocused,  phasy, or vague stereo image.  Therefore, more than about one decibel of mid-side variation  is usually a bad idea.  The Fletcher-Munson curve.  I'd like to quickly mention the Fletcher-Munson curve  as to apply effective equalization,  it's critical that we understand  this interesting phenomena of human hearing.  The Fletcher-Munson effect dictates  that high-frequency energy produces more loudness  than low frequencies at the same sound pressure level.  As the actual loudness changes,  the perceived loudness our brains hear  will change at a different rate,  depending on the frequency.  For example, at low listening volumes,  mid-range frequencies sound more prominent,  while the low and high-frequency ranges  seem to fall into the background.  And at high listening volumes, the opposite is true.  The lows and highs sound more prominent,  while the mid-range seems comparatively softer.  Yet, in reality, the overall tonal balance of the sound  remains the same, no matter what the listening volume.  Here's a graph that illustrates the concept  of the Fletcher-Munson curve.  This is also in the blog post  if you wish to get a better look at it,  the link for which is again in the description.  Using some of the techniques we've just gone over  will definitely help you to achieve depth and dimension  in your masters.  But this subject requires its own chapter  to really grasp what's possible  and how to achieve it in the mastering stage.  Depth and dimension.  Depth, the front-to-back dimension in your mix,  is another area where your mastering engineer  can coax out a lot of hidden magic.  But, as always, we can do the most  to enhance the sense of depth in our mixes  when there is some there to begin with.  To bake plenty of depth into your mix,  you first have to realize that not every sound  should feel like it's glued to the front edge  of the speakers.  And once again, contrast is key to crafting a bold mix  that feels like it lives and breathes  in all three dimensions.  Because when every sound is up front,  nothing gets to take center stage.  Once you've internalized this idea,  the second step is to master the specific tools  and techniques that will allow you  to bring some sounds closer and push others back.  Use of reverbs, delays, compression, and EQ  can all be crucial in making some elements  seem to step back while allowing other,  more central elements to step forward.  The perception of depth.  At first thought, it may seem that depth  in our recording is achievable by increasing the ratio  of reverberant to direct sound,  but it is a much more involved process.  Our binaural hearing apparatus is largely responsible  for the perception of depth.  But recording engineers were concerned  with achieving depth even in the days of monophonic sound.  In the monophonic days, many whores for orchestral recording  were deader than those of today.  Why do monophonic recording and dead rooms  seem to go well together?  The answer is involved in two principles  that work hand in hand.  The first is the masking principle,  and the second, the Hass effect.  The masking principle.  The masking principle says that a louder sound  will tend to cover or mask a softer sound,  especially if the two sounds lie  in the same frequency range.  If these two sounds happen to be the direct sound  from a musical instrument and the reverberation  from that instrument, then the initial reverberation  can appear to be covered by that direct sound.  When the direct sound ceases,  the reverberant hangover is finally perceived.  In concert halls, our two ears sense reverberation  as coming diffusely from all around us,  and the direct sound as having a distinct single location.  Thus, in whores, the masking effect is somewhat reduced  by the ear's ability to sense direction.  In monophonic recordings, the reverberation is produced  from the same source speaker as the direct sound,  and so we may perceive the room as deader than it really is  because of the directional masking.  Furthermore, if we choose a recording hall  that is very live, then the reverberation  will tend to intrude our perception of the direct sound,  since both will be reproduced from the same location,  the single speaker.  So there is a limit to how much reverberation  can be used in mono.  This is one explanation for the incompatibility  of many stereophonic recordings  with monophonic reproduction.  The larger amounts of reverberation tolerable in stereo  becomes less acceptable in mono  due to the directional masking.  As we extend our recording techniques to two-channel  and eventually multi-channel,  we can overcome directional masking  by spreading reverberation spatially  away from the direct source,  achieving both a clear and warm recording at the same time.  The Haas effect.  The Haas effect can be used to overcome directional masking.  Haas says that in general, echoes occurring  within approximately 40 milliseconds of the direct sound  become fused with the direct sound.  We say that the echo becomes one with the direct sound  and only a loudness enhancement occurs.  A very important corollary to the Haas effect  says that the fusion and loudness enhancement  will occur even if the closely timed echo  comes from a different direction than the original source.  However, the brain will continue to recognize  the location of the original sound  as the proper direction of the source.  The Haas effect allows nearby echoes  up to approximately 40 milliseconds delay  to enhance an original sound  without confusing its directionality.  We can take advantage of the Haas effect  to naturally and effectively convert  an existing two-channel recording  to a four-channel or surround medium.  When remixing, place a discrete delay  in the surround speakers to enhance and extract  the original ambience from a previously recorded source.  No artificial reverberator is needed  if there is sufficient reverberation in the original source.  Here's how it works.  Because of the Haas effect,  the ear fuses the delayed with the original sound  and still perceives the direct of the sound  as coming from the front speakers.  But this does not apply to ambience.  Ambience will be spread,  diffused between the location of the original sound  and the delayed sound in the surround speakers.  Thus, the Haas effect only works for correlated material.  Uncorrelated material, such as natural reverberation,  is extracted, enhanced, and spread directionally.  Dolby laboratories call this effect the magic surround,  for they discovered that natural reverberation  was extracted to the rear speakers  when a delay was applied to them.  Dolby also uses a left minus R matrix  to further enhance the separation.  The wider the bandwidth of the surround system  and the more diffused its character,  the more effective the psychoacoustic extraction of ambience  to the surround speakers.  Using frequency response to simulate depth.  Another contributor to the sense of distance  in a natural acoustic environment  is the absorption qualities of air.  As the distance from a sound source increases,  the apparent high frequency response is reduced.  This provides another tool  which the recording engineer can use to simulate distance,  as our ears have been trained to associate distance  with high frequency roll-off.  An interesting experiment is to alter our treble control  while playing back a good orchestral recording.  Notice how the apparent front to back depth of the orchestra  changes considerably as you manipulate the high frequencies.  Another important tool  for the measurement of depth and dimension,  as well as several other important characteristics  of your composition, are loudness meters.  Loudness metering.  In this chapter, we'll take a look at loudness meters,  how to use and interpret them,  and their relevance to music production.  The revolution in loudness normalized digital radio and TV  has inspired a plethora of loudness meters  in every shape and size to fit every taste.  But because the ear and brain are complex,  no meter can anticipate our ears' perceptions  at every moment in time.  For example, when mastering an album  that has a tune with a soft, delicate ending,  the beginning of the next tune usually sounds too loud.  But if you play the middle of the latter tune for a moment,  it doesn't sound too loud.  And when you play its beginning again, it also sounds fine.  You feel no need to turn it down.  What's happening is that, in general,  the ear does not react to absolutes,  but it is very sensitive to contrasts.  And because the ear says so,  the beginning of the following tune is too loud,  regardless of what the meter says.  One solution is to try to increase  the space between the tunes.  Another solution is to slightly turn down  the introduction of the next tune,  just enough to reduce the disturbance on the ear,  making sure that you can lead the listener  on an upward journey through the next tune.  This is why it is important to use meters,  but not to depend on them.  Your ears should always be your last port of call.  Audio meter weaknesses.  Because of the aural sensitivity to contrast,  a sudden burst of loud sound in the midst of soft sounds  has a much greater impact than something of equal intensity  in the middle of other loud sounds.  No meter can make this type of loudness judgment  or substitute for a set of trained audio engineer's ears.  Also, meters do not take into account  the listening sound pressure level.  Our ears are not flat,  and depending on how loudly we're listening,  meters may over or underestimate  our sensitivity of low frequency sounds.  Audio meter strengths.  However, despite these shortcomings,  meters are absolutely necessary.  Our ears can be temporarily reset  by listening to a loud passage for a long time,  but a meter can help us verify  that the soft part of an album is not too soft.  Meters are very good at detecting a domino effect,  wherein a tune becomes slightly louder than the previous one,  and the ear gets accustomed to the escalation.  We must learn to work with the audio meter  to take advantage of its strengths  and to understand its weaknesses.  In general, audio metering has matured greatly.  It is far more effective now  than it has been since the time of Dawn for audio recording.  Learning how to read loudness meters  when both mixing and mastering your tracks  can make the biggest difference to the overall balance,  especially if you haven't got  the best monitoring system in the world.  I'm going to cover some of the most important terms  you'll find on meters now,  but I've made a much bigger list over on the blog post  to go with this tutorial.  It's listed and linked in the description below.  Voxengo Span Controls.  Because this plugin is free,  as well as it being one of the best,  I'm going to use it as the example.  It's called Span, and it's made by Voxengo.  The plugin is listed and linked in the description.  I'd like to take you through  some of the statistical information that Span displays  so you can better understand what's going on  in your own compositions.  RMS.  The RMS indicator here  displays an unweighted RMS signal power estimation.  In simple terms, it displays the average level.  It does this by taking an average of the level  over a short window of time.  RMS level gives us information  that relates to our perception of volume.  Our brain processes information  during a short window of time  to evaluate how loud something is in our environment.  An RMS level is a way to attempt  to give feedback about that.  However, RMS doesn't relate directly to perception  in the case that it doesn't take frequency content  and balance it into account.  I'll come back to RMS in just a second  as it will tie in with the peak level display.  I guess the clue is in the name.  The peak level will display the level of signal  from moment to moment.  In the case of Span,  the peak indicator displays a one sample output peak level.  Both the RMS and peak readings are important,  but for different reasons.  Peak level tells us how close the signal is  to the point of distortion.  It's a way of helping us understand  whether we have any headroom to bring the signal up  without changing anything else about it  and staying below the point of distortion.  The relationship between the peak level and RMS level  will vary widely depending on the dynamics of the mix  and the drama of the music.  That makes it hard to generalize too much.  However, if I had to give you a general idea  about where the RMS level should be sitting  in respect to zero dBFS,  I would recommend minus 10 for any electronic music,  certainly nothing under minus 12.  But of course, if you can get it louder than minus 10,  do by all means.  I myself will usually aim for no more  than minus three decibels of gain reduction on my limiter.  Crest factor.  This list is in no particular order,  although I do regard peak and RMS  as the two most important readings  you should be paying attention to.  The next in the list is crest factor, perhaps my favorite.  The display you're looking at on the span here  is the max crest factor,  which shows the maximum crest factor difference  between the RMS and peak RMS values reached.  Peak RMS value is not displayed anywhere  on the user interface.  There is also a 50 millisecond time window  used to estimate the peak RMS value.  You can add the RMS and the max crest factor values together  to obtain a peak RMS value.  But please note that when comparing  the max crest factor value in span to other plugins,  the peak RMS time windows  should be matched in the plugin comparison,  or otherwise the readings will be different.  Correlation meter.  This panel contains a meter  that shows average real-time correlation  between two first input channels.  A correlation meter, also known as a phase meter,  can tell that something is amiss,  and even indicate that some information in the left channel  is out of time or out of phase with that in the right.  The meter is marked from minus one at the left  through to zero in the middle and plus one at the right.  Zero means random correlation.  This means that the left and right channels  are only randomly related to each other.  A zero reading can occur  when the sound consists of stereo ambience, delay,  or if there are two entirely different programs  in the channels.  Plus one means that the sound is completely mono,  and that there is 100% correlation to all frequencies  between the left and right channels.  A constant reading of minus one  means that the channels are correlated,  but one channel is out of polarity with the other.  In that case, it is also correct to say  that all the elements in one channel  are 180 degrees out of phase  with the elements in the other at all frequencies.  If the channels are partially out of phase,  the meter would be to the right of the minus one position.  Meter movement towards the middle is desirable.  It tells you that there is a lot of random phase information  in the stereo field,  and that the stereo image  will likely sound rich and spacious.  If the meter is slightly off its extremes,  you'll know that there is some phase shift  between the channels.  But don't try to correct time and information  unless all of the information in the left channel  is out of time with that in the right.  Additional metering jargon.  When mastering, there will be other terms  that you will come across  when using different types of plugins  to measure and analyze your composition.  These will include sample peak versus true peak.  The maximum possible sample peak or digital peak  of a recording is defined as zero dBFS,  the level that real world digital audio samples  cannot exceed.  This is why we call it full scale.  Sample peak, the actual peak numerical value of the samples  was the traditional method of peak metering until recently.  But sample peak is not a very effective measure  of judging overloads because real world devices  such as digital audio converters,  filtered processors such as sample rate converters  and any kind of lossy codec  produce higher output levels than their input.  Thus, a PCM signal can represent a signal  that has higher amplitude than the highest PCM value.  A more effective measure of protecting from overloads  specified in BS 1770-3 is called true peak,  achieved by upsampling methods.  True peak is a measure, well, actually an estimate  of inter-sample peaks, literally peaks between the samples,  which will occur after some kinds of filtering.  In practice, this means that recordings  whose sample peak is at or below full scale  may overload after further conversion or encoding.  These over full scale peaks  are known as zero dBFS plus fields.  In the absence of true peak metering,  it is wise to keep sample peaks at or below minus one dBFS.  If mastering studios ignore true peak,  they will soon discover that the PMC recordings  that sounded so good in the studio  become congested or distorted after conversion,  especially to a lossy format.  A true peak meter does an effective job  of predicting the output level of filtered processes  such as digital audio converters and sample rate converters,  but lossy formats are a special case.  Lossy formats not only filter, but also add noise,  which increases the sample peak level  and the inter-sample peak.  Because of this, a true peak meter  cannot predict what a codec will do.  So be sure to measure the true peak level of the codec.  The lower the bit rate of the lossy medium,  the higher the amount of overload distortion.  So be mindful of those lower bit rate broadcasters  like satellite radio and some streaming services.  Integrated momentary short-term loudness.  Integrated loudness is the same as program level,  which is the quantity you should be aiming for  when taking a measurement of the whole program.  The EBU has defined two other timescales  for use with loudness meters.  The first is momentary loudness, abbreviated M,  which is the loudness you hear now.  M is averaged over a 400 millisecond period,  which corresponds well with the VU meters many of us use.  The second is short-term loudness, abbreviated S,  with a time window of three seconds.  They perhaps should have come up with a different term  because it is not obvious that short-term  is longer than momentary.  Loudness range.  Loudness range, abbreviated LRA,  is a well-defined statistical measure of dynamic range.  Essentially, the difference between the highest  and the lowest gated loudness values  in a particular program.  True peak level.  Another term newly standardized by the ITU  is an estimate of the peak level that will be encountered  at the output of a DAC or any other filtered process,  such as a sample rate converter, or SRC.  It's abbreviated DBTP.  Compare this to sample peak level,  which is the peak value of the digital sample  measured by traditional digital meters,  but traditional digital peak meters  are no longer recommended for program measurements.  Headroom.  The ITU standard permits us to define headroom  in a more usable way than ever before.  Headroom is the difference between the program loudness  and the peak level capability of the medium.  Zero dBFS.  Peak to loudness ratio.  Peak to loudness ratio, or PLR for short,  is the ratio between the highest true peak,  not exceeding zero dBTP,  and the long-term average loudness of the song  or album in LUFS.  There will be many more,  but I feel these are the most essential.  I've included the full list in the blog post for this video.  Clipping, soft clipping, and oversample clipping.  I'd like to finish this tutorial on the last element  in the mastering signal processing chain, distortion.  Distortion has been part of the language of music  ever since the electric guitar was invented in 1931.  It's important to realize that distortion and compression  are tightly interrelated.  More compression means more distortion.  More distortion means more compression.  A distorted waveform has a higher average level  with the sample peak level than a pure sine wave tone.  Higher average level indicates more compression.  The relationship between the two  is called peak-to-average ratio, or crest factor.  The more compressed the sound,  the lower the peak-to-average ratio.  Peak-to-average ratio is an indicator  of a recording's compression, clarity,  microdynamics, and impact.  The higher the peak-to-average ratio,  the more headroom we leave for its peaks above the average,  which means there's more room  for the percussion to sound natural.  However, the distorted recording sounds much louder  because the average level of a recording  governs its perceived loudness.  Distortion usually generates more high-frequency information  which the ear perceives as louder.  Digital clipping is the result of attempting  to raise the level higher than zero dBFS,  producing a square wave, a severe form of distortion,  also known as clipping the medium.  Analog clipping is the result of overdriving  an analog processor beyond its peak headroom.  Analog clipping generally comes on gradually,  while digital clipping becomes quite severe rapidly.  Therefore, analog clipping sounds easier on the ear  than digital clipping,  but they are both forms of distortion.  Clippers are specialized devices  that electronically cut off momentary peaks  out of the waveform to allow the overall loudness  to be raised, which is a better-sounding approach  than just clipping the medium.  Soft clipping attempts to do this only a little bit  with less distortion.  Digital peak limiting.  Keep in mind that if every mastering engineer  is already using peak limiting,  every runner in the race has already taken steroids,  so there is no performance advantage.  One or perhaps two decibels of limiting  may get the program level up  without taking in sound quality downhill.  More than that can easily produce wimpy, unclear,  less effective sound quality.  Carefully compare with less limiting,  and you may be surprised.  When raising the level, listen,  don't look at the loudness meter.  Does this raised version actually sound louder  or just measure louder?  Carefully compare this with less limiting,  and you may be surprised.  Clipping.  Clipping can add an edge,  increasing apparent loudness and definition,  but it can also have the opposite effect.  Since clipping is a form of limiting  without a defined attack or release time,  it can be a lesser evil with less clamping effect  than a peak limiter.  But like limiting, it can rob a recording of impact  and important microdynamics.  At double sample rates, digital clipping produces  less mid-band distortion and artifacts  than at single sample rates.  Be aware, though, that what sounds loud  due to clipping in the control room  will sound very harsh once it hits the codec  or FM radio station processor.  You can't vault the laws of physics.  Make sure you audition clipping through the codec.  All right, that concludes this complete mastering tutorial.  Without a doubt, I'm sure to have forgotten  to include something.  I did my best to cover the essentials  while giving you the philosophy  and the why behind everything we've gone through.  Learning how to do something is important,  but the why is what ingrains it in your brain  and allows you to recall it  when you next need that piece of information.  I'll be around in the comments,  so leave any questions you have down below  and I'll do my best to answer them all.  As always, I hope you learned something new  and until next time, I'm out, peace.  ♪ Ooh, ooh, yeah, woo, yeah ♪  ♪ I'm a machine like the beyond, yeah ♪  ♪ I'm a machine like the beyond, yeah ♪ 