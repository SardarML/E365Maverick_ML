 Hello and welcome to this course on statistics. In this course, our goal is to make learning  statistics fun and enable you to apply statistical methods for data analysis and data science.  My name is Murtaza Herer. I'm your instructor for this course. I'm also an associate professor  at the Ted Rogers School of Management at Ryerson University in Toronto. I'm the author  of Getting Started with Data Science, Making Sense of Data with Analytics. My research  interests are in urban economics as they relate to housing markets and transportation. I blog  regularly and you can find my blogs on Huffington Post. By way of training, I have a master's  in transportation engineering and planning and a PhD in civil engineering with a focus  on urban systems analysis. And my name is Ije Egwahide. I am the co-instructor for this  course. I am a senior data scientist and statistician with the IBM Developer Skills Network team.  I have field experience working on supervised and unsupervised machine learning algorithms  for oil and gas clients. During my high school in Nigeria, it was easy to put off mathematics  and statistics and focus on the seemingly easier courses. I always love a good challenge.  My interest in statistics and mathematics sparked as a result of people around me saying  it was hard. So I made my parents invest in textbooks and I always made sure to be ahead  of the class. When I got to the University of Manitoba for my undergrad, picking statistics  alongside economics was easy. And when I had to do a post-grad, picking data science and  business analytics was a no-brainer. On my off days, I'm a fashion and career blogger  on Instagram. This course consists of five modules, Introduction and Descriptive Statistics,  Data Visualization, Introduction to Probability Distribution, Hypothesis Testing, and Regression  Analysis. Each module will comprise of four to six videos and will include exercises for  you to practice on. The hands-on lab will utilize Jupyter Notebooks using the Python  programming language, which is one of the easiest programming languages to learn. Let's  get started and happy learning. Welcome. In order to do data analysis in Python, we  should first tell you a little bit about the main packages relevant to analysis in Python.  A Python library is a collection of functions and methods that allows you to perform lots  of actions without writing your code. The libraries usually contain built-in modules  providing different functionalities, which you can use directly. And there are extensive  libraries offering a broad range of facilities. We divided the Python data analysis libraries  into three groups. The first group is called Scientific Computing Libraries. Pandas offers  data structure and tools for effective data manipulation and analysis. It provides fast  access to structured data. The primary instrument of pandas is a two-dimensional table consisting  of columns and rows labels, which is called a data frame. It is designed to provide an  easy indexing function. NumPy library uses arrays as their inputs and outputs. It can  be extended to objects for matrices. And with a little change of coding, developers perform  fast array processing. SciPy includes functions for some advanced math problems as listed  in the slide, as well as data visualization. Using data visualization methods are the best  way to communicate with others and show the meaningful results of analysis. These libraries  enable you to create graphs, charts, and maps. The matplotlib package is the most well-known  library for data visualization. This package is great for making graphs and plots. The  graphs are also highly customizable. Another high-level visualization library is Seaborn.  It is based on matplotlib. It's very easy to generate some sort of plots like heat maps,  time series, and violin plots. With machine learning algorithms, we're able to develop  a model using our data set and obtain predictions. The algorithmic libraries tackle some machine  learning tasks from basic to complex. We introduce two packages. The scikit-learn library  contains tools for statistical modeling, including regression, classification, clustering, and  so on. It is built on NumPy, SciPy, and matplotlib. StatsModels is also a Python module that allows  users to explore data, estimate statistical models, and perform statistical tests. Now  let's get into statistics. Thanks for watching.  Welcome to statistics. In this module, we will explain how statistics surround our daily lives.  All we have to do is to think of the conversations we have on a regular basis. A day start with this  concern about rain or snow. We turn to the weather channel to see whether it will rain or snow today  or tomorrow. When the weather channel informs you that the chance of rain is 35% or 60%,  you are essentially relying on statistical tools and technologies to come up with those forecasts  so that you may be better prepared for either rain or snow. If you happen to live in a large  city in North America or Europe in East Asia, housing affordability is likely to be a concern.  And when you hear in the news media that housing is becoming more expensive over time, this analysis  is coming out of statistical analysis. At the same time, you will hear if the unemployment rate has  fallen or has risen over time, or how millennials are looking for jobs that may not be full-time.  And when we track those numbers over time, we realize we are using statistics. Statistics are  not just confined to economics. We appreciate players based on their performance and we judge  their performance using statistics. Millennials and the sharing economy is perhaps redefining the  way we understand economics and business these days. The way millennials have defined their  preferences different from previous generations is something of interest. Many say that they  would like to rent than to own a house. That didn't used to be the case in the past, but the  norms are changing. Who gets paid, how much, and not just at your work, but in the Hollywood is  again coming out of statistics. Similarly, if you're thinking of pursuing business analytics  as a career, you may be interested in knowing what is the average salary of a starting business  analyst. And again, this comes out of statistics. Any comparison of salary between two professions,  such as an engineer or an economist, would require statistics. And if you happen to be in Chicago,  you probably have not missed that crime has spiked in recent years. Similarly, we compare crime,  especially violent crime over years, and this comparison requires us to use statistics. So,  when we say average income, average age, average height, we're relying on average,  which is a statistical parameter. Highest paid athlete, we're looking at the maximum salary.  Fastest sprinter, we're looking at the maximum speed. Lowest unemployment rate of all the OECD  countries, we're looking at a minimum value. Percentage of females who study engineering  requires us to compute percentages. The chance for rain tomorrow is in fact likelihood. And  how consistent is a stock performance over the past three months, we're concerned about variance,  which again is a statistical parameter. And then the question of on average, do men spend more on  clothes than women? We probably would use a t-test to determine this difference, again,  relying on statistics. If you were to recall your conversations in a given day, you probably  realize now that you have been using the language of statistics on a daily basis. At the same time,  the news media use statistics all the time to demonstrate how trends are changing. 2016 was  the year American presidential elections were held. Big surprises there between what the polls  forecasted and what the outcome was. But again, you see these numbers portrayed in the newspapers.  At the same time, you have other publications that show you how housing prices or other  development-related statistics vary over countries in a nutshell. The information we consume  and the conversations we have every day involve a lot of statistics. So it pays one to learn some  statistics.  The first step in analytics or statistics is to have a good look at your data. And before you  begin, try to understand what kind of variable you're working with. And based on the type of  variable, you will decide what kind of analytics could be performed with it.  Let's have a look at various different types of data that we encounter and is commonly used in  our daily lives. The most common one would be a cross-sectional data, which is basically looking  at a measurement taken at one point in time. Census in a given year is a cross-section of  the society. As students evaluate course, an instructor, that's a cross-section at any given  point. Compared to the cross-sectional data, we can have panel or cross-sectional panel data,  which is essentially asking the same group of individuals the same questions repeatedly over  time. So you may pick a group of people, constitute it as a panel, and then ask the same  questions once every year over a given period of time. The time series data is rather different.  You're looking at a particular phenomenon, such as unemployment rate, and then you measure it  every month and then display that data or analyze that data, which is repeated measurements on the  same phenomena over time. So you may have monthly data going back to 1940s or climate data going  back to hundreds of years. So based on the type of data, cross-sectional, panel, time series,  we will pick appropriate tools, statistical tools, to deal with that. If your data set has only one  variable, it's called a univariate data set, and if you have multiple variables in your data set,  then it's a multivariate data set. Let us now look at variable types and start with categorical or  nominal variables. Let's consider home ownership, for instance. One can either own a home or  rent a home, and knowing that there are only two categories here, owning and renting, that is a  categorical variable. The tenure status of an individual is essentially a categorical variable.  In this particular case, because you only have two choices, own or rent, it's a binomial variable.  Consider travel choices. You can go to work by driving or by someone drives you there, so you're  a passenger, you can take public transit, or you can walk or bike. So in this particular case, you  have four choices, more than two, so we call it multinomial. So both binomial and multinomial  variables are part of categorical variables. You cannot have any quantitative relationships  among categories, and for these types of variables, averages are usually meaningless. So if you have  a mode of travel and you have four categories, an average category would mean absolutely nothing  of use. A particular type of categorical variable is ordinal data, where data are ranked or ordered  in some particular fashion. So, for instance, number of cars owned by a household. A household  may have 0 car, 1 car, 2 car, 3 or more cars, and that essentially is an ordinal data where  0 represents 0, and 0 cannot be coded as 1, and 1 cannot be coded as 0. So the order in which  variable has been recorded matters. Categories can be compared with one another, and you still  cannot use regular statistics. The differences are also meaningless in this particular case.  Another type of data is called ratio data, which is data set that have a natural zero. For example,  sales dollars, length of a distance or weight of an object. These are all examples of ratio data,  and I often would use the term continuous data, or continuous variable. So a variable such as  distance from point A to B could be 8 kilometers, 8.5 kilometers, 6.2 miles, and so on.  The variable is continuous, and 0 makes some logical sense in this particular variable. So,  for instance, you say I have 0 dollars, 0 means something here. It's the strongest form of  measurement, and you can compute ratios and differences. And another type of variable is  interval data, or interval variables, that are ordered and characterized by a specific measure  of distance between observations, and it may not have a natural zero. So, temperature is a good  example, and when you say that it's 0 degrees Celsius, it does not mean that there is no  temperature. It's freezing, but it is measuring something that exists. So, ratios are also  meaningless. So, for example, if someone said, well, you know, the temperature in some African  countries is 50 degrees, compared to somewhere tropical where it was 25 degrees, it doesn't mean  that the temperatures in the African desert is two times or twice as hot as it is in the tropics,  but we can say that there's a difference of 25 degrees between the two places.  The measures of central tendency are the most commonly used in statistical analysis.  We know them as mean, median, and mode, and their use is ubiquitous in statistical analysis.  So, let's see how it works. Before we begin, let's take a quick look at our data set in this course.  We have been using the teaching evaluation data from the University of Texas. The data set  comprises of 463 courses in which we have information about the teaching evaluation  score received by the instructor, and we have information about the attributes of the instructor,  as well as the characteristics of the course. Once you have imported a CSV file with a Pandas  Python library, the first step in getting to know your data is to discover the different data types  it contains. You can display all columns and their data types with DataFrame.info. In this case,  we have named our DataFrame ratings underscore df. It tells you how many rows you have.  For the teaching rating data, we have 463 entries from 0 to 462, because Python starts counting from  0. And then it also gives you information about the data types. Object represents strings, int64  represents integer or whole numbers, and float represents real numbers, which could take on  decimal points. Before we begin, let's have a conversation about population and samples.  Essentially, if you have all the information of interest for a particular decision  about every individual that is supposed to be involved in that decision, that is called a  population. So if you're interested in looking at some attribute of driving, and we have information  about all possible automobile drivers in the US, and then we call this the population. The sample,  on the other hand, is a subset of population. So for example, if we have data on all married  drivers over the age of 25, then that's a subset. And within that subset, if we were to randomly  select 5% of those married drivers over the age of 25, that would be our sample. We use samples,  especially in cases where we do not want to incur the cost of collecting data for the entire  population. Now, let's consider that there are 230 million individuals in the country,  a sample size of say 330 to 500 individuals randomly selected would suffice.  This reduces the cost, especially in cases where you cannot collect information for the entire  population. Therefore, using samples, it's really helpful and cost effective.  Here you see some Greek symbols on the screen. But don't be afraid, they mostly show the formula,  we will then proceed from here. While they may differ in notation, essentially the mean for a  population and sample are the same. It is the sum of all the observations, then divided by the number  of observations to get the mean, which we call averages. There are several properties of the  mean and they're meaningful. But one of the characteristics of a mean is that if you take  the difference between the average value for a variable and subtract from all the observations,  and sum them up, that sum would be equal to zero. The median is different from the mean,  when you order the data from the smallest value to the largest value, the result is in the middle.  That is the value in the middle indicating that there are an equal number of observations that  are above and the equal number of observations are below that family. That value is called the  median. So if the median salary in some city is $45,000, it means that 50% of the people make  more than $45,000 and the other 50% make less than $45,000. Mode is essentially the value that  occurs most frequently. Therefore, if the most common age in a class of students is 16, then  that's the mode. We will now turn to Python for our hands-on training to estimate the summary  statistics values for beauty score, teaching you about evaluation and age. We will use the  data frame dot describe function to find the summary statistics. This prints out the number  of rows, mean, standard deviation, minimum value, 25th, 50th and 75th percentile, and the maximum  value. To find the summary statistics for a subset of the variables, you will have to state  the column names as we can see here. Otherwise, for the full population, we will call the dot  describe function on the data frame.  Dispersion, which is also called variability, scatter or spread, is the extent to which the  data distribution is stretched or squeezed. The common measures of dispersion are standard  deviation and variance. And if you are at a university or college, you may have heard about  the bell curve, which looks like this. And you will often hear this is within one standard deviation  of the mean or within two standard deviations of the mean. So let's see what that means.  Let's look at the age of an instructor. Let's say the average age is 52. This means that the  individual ages may differ. Some may be 48 or maybe 55 or 75. So the average age is an estimate.  But what we also need is an estimate for the dispersion in the data set. The other thing to  note is the range in our data set. For example, the difference of the range is from a minimum of  29 years of age to a maximum of 73 years. And this to you refers to a distance or the difference  between the minimum and the maximum. Unlike the difference between population and sample mean,  the difference between a variance for the population and a sample is that when you compute  the population variance denoted as sigma squared, you divided by the total number of observations.  These are the deviations between observation and the mean squared, then added, and then divided by  the total number of observations. For sample variance, which is denoted as s squared, you  divided by n minus one. The purpose of using n minus one is so that our estimate is unbiased in  the long run. That means that if we take a second sample, we'll get a different value of s squared.  If we take a third sample, we'll get a third value of s squared, and so on. We use n minus one so  that the average of all these values of s squared is equal to sigma squared. We usually talk about  squares called standard deviation rather than the variance. Standard deviation is essentially the  square root of the variance or the variances in square units. So it's good to use the standard  deviation because it's exactly the same units as the variable. The standard deviation of age will  also be measured in years rather than years squared. Here you see that we just took the  square root of variance and this becomes standard deviation. We will return to our data set and  we'll look at the variables that we computed before. You can see that the standard deviation  for beauty, evaluation scores, and age were computed with the descriptive statistics using  the describe function in Python. Let me explain why mean and standard deviation have to go hand  in hand. I will use the example from basketball about the two giants, Michael Jordan and Wilt  Chamberlain, who preceded Michael Jordan. Now if you consider their average score for game, you  would notice they didn't differ much. Their average was around 30 points for both Jordan and Chamberlain.  However, when you look at the standard deviation of their performance, Jordan was around 4.76  compared to Chamberlain who was at around 10.59. If you were to plot this distribution to see  Michael Jordan's scores using the mean and standard deviation, assuming that their scores are  normally distributed, you would notice even though both players had around the same mean, the tighter  distribution for Jordan suggests that he was more consistent in his performance than Chamberlain.  The main takeaway is that average will only paint a partial picture. If you really want to understand  the complete picture about a variable or data set, it is important to compute both the average and  the standard deviation to get insights on what the data is telling us. So a mean with a standard  deviation means something more useful than the mean by itself.  Let us begin with the fundamentals of visualization. What you see on the right  side is a pictograph or a bar chart prepared by students in a kindergarten class, four-year-olds,  at the St. Luke Catholic School in Mississauga. This bar chart presents the frequency for  transportation modes the kids have used to come to school. Some have been dropped to school by car,  others by bus, and two students walk to school. In the brave big world of big data, we can see  that children are being trained at the earliest possible age with data and data science.  The most important thing to realize is that the type of visualization you will use depends upon  the type of variables you're trying to analyze. For instance, if you're working with categorical  variables such as gender, you have to rely on a certain type of charting tools than if you were  to be working with continuous variables such as age and income. In one case, you may be using bar  charts. In another case, you will be required to use scatter plots. I would like to draw your  attention to the extreme presentation method developed by Dr. Andrew Abella. Essentially,  it lays out the possible ways of depicting data based on what kind of variables you have at your  disposal. This visualization, this graphic, is developed by Dr. Abella that shows that if you  are interested in comparing variables or demonstrating their distribution or composition  or the relationship between two variables or more, you have to rely on a specific type of graph.  If you're comparing items with few categories, you can use bar charts or column charts. If you are  comparing behaviors over time and if you have the time periods running into months, several months,  you may have to use a line chart, and if the time periods are not that many, then you can  use columns and other approaches. If you're trying to depict relationships between  two continuous variables, your choice is a scatter plot. The bubble chart will depict two variables  on the x and y-axis, and the third variable will be depicted by the size of the circle,  so you can essentially have three variables. If you're interested in depicting the distribution  of the data set, you can use a histogram, which could be a bar chart type of histogram or a line  histogram, and the distribution could also be shown through scatter plots. If you would like  to show the composition, then if it's a static data, you can use pie charts. If you're showing  the composition that changes over time, for few periods, you can use stacked columns,  and if you have several periods, you can use the stacked area charts.  For hands-on in Python, we will be using the Seabourn library and the matplotlib library  to create visualizations in the labs. We will learn how to use different functions  within the library to create different kinds of charts.  We have learned to compute averages and standard deviations,  but now we will use the same information or same knowledge to make comparisons between groups.  So we will use the same data set that we have used so far, that is the teaching evaluation  data from University of Texas, comprising 463 courses. We are looking at the teaching  evaluation beauty and age, and we're comparing the averages for these three variables for  female instructor variables, so those who are females, their average teaching evaluation was  3.9 compared to those of men, 4.06. Here, we are looking at the average teaching evaluation  for tenured professors, 3.96 versus untenured, 4.13. The average age of untenured professors  was 50.2 years, and that for tenured professors, 47.85 years. One thing that is very important  in statistical analysis is to think about the question and to think about the population  or sample that you are working with. We are computing averages across 463 courses,  and we find the average age or beauty, but these are the attributes of instructors.  We know from our data that there are 94 instructors who have collectively taught 463 courses,  and we know that there are duplicates, that is the same instructor who has taught multiple courses.  So when I compute the average age using 463 courses, it's not necessarily the average age  of the instructors because it could be true that older-aged individuals may have taught more  courses than younger individuals, resulting in a higher average. That is not necessarily  the average age of the instructors. So to avoid this problem, we have to subset the data so that  we remove the duplicates and have only one observation per individual instructor in the  data set. So instead of 463 observations, we should have just 94 observations. Now let's  look at the comparison. When we use 94 observations where no instructor is repeated in the data set,  the average age or average beauty score is 0.25. When we look at the 463 courses,  the average value is 0.11. Or let's compare the age. The average age using 94 observations for  males is 49.4, and for females is 44.9. And you see here that as for age, we don't see much  difference whether we use 463 observations or 94, but we certainly see much difference in the beauty  scores if we were to use the wrong data set, that is the data set where individuals are repeated  multiple times. Data visualization is a critical piece of modern-day statistical analysis.  Their staples are helpful so you don't have to eyeball the output to figure out what the trends  are. The visual displays are much easier to understand. We will use the same data sets of  teaching evaluations and ask this question. Do instructors teaching single credit courses get  higher evaluations? We see that yes, they do. By mean evaluation, when plotted as a chart,  you see that instructors who teach single credit courses have a slightly higher average teaching  evaluation. Let us start by determining how many courses were taught by male instructors  and how many by female instructors. For this, we can use a bar chart. Notice that the information  is complete from a statistical point of view in that we know how many courses were taught by males  versus females, but we do not have some critical information from this chart as it relates to  communication. Therefore, we can say this chart serves a statistical purpose, but it doesn't serve  a communication purpose. Let me illustrate this with an example. Here you are looking at a street  map. You can see the streets and the buildings and the highways, but you don't see the street  names and without street names, it is hard to determine where you are and in which direction  you should be heading. Even though it is according to scale, it may be accurate in its depiction of  the streets in the neighborhood, but it still lacks the ability to communicate information to you.  To add communication value to this map, you can simply add the street names.  Let us apply the same philosophy to our graphic, but once we add information about this infographic,  for example, adding just a title makes this chart more informative. To do this in Python,  we will use the count plot function in the Seaborn library and set the title label.  This helps your graph to be more informative. We can also add more dimensions to the data.  In addition to the gender of the instructors, we could add the tenure status of the instructors  as well to the graphic. To do that in Python, you add the hue argument to the count plot.  We can add another dimension to the data, regenerating the same graphic with the same  information. That is the number of courses taught by gender and tenure, and then adding the dimension  of courses being upper division and lower division and presenting them in two rows or columns.  To do this in Python, we can specify the rows argument using the cat plot function.  Now let's look at the situation where our primary variables of interest are continuous variables.  We would like to explore the relationship between the two while adding further categorical variables  as an additional dimension. Using the teaching evaluation data, we ask this question,  does age affect teaching evaluations? We then add two additional dimensions, which are gender and  tenure. So our data set consists of age and teaching evaluation, which are the two primary  variables of interest and are continuous. And then we add two other dimensions, i.e. gender  and tenure. These are categorical variables. Age is on the X axis and the teaching evaluation score  is on the Y axis. The orange colored circles represent males and the blue colored circles  represent females. The top panel is for tenured professors and the bottom panel is for the  untenured instructors. To do this in Python, we use the facet grid option, which works for  multi-plot gridding and allows tweaking the plot. You create the row and hue for the categorical  variables, in our case, tenure and gender. And then we use the map to apply a plotting function  to each subset of the data.  So far we have displayed data as averages and counts. Now let's look at some other statistical  parameters that we will illustrate as graphics. We have not yet shown anything about variance  and I think the first thing that one should look into beyond averages is to look at variance or  how the data are distributed. And a good way of looking at the distribution of data, especially  if it were to be a continuous variable, is to look at histograms. And if you are interested  in displaying something more than the average, maybe the median and the quartiles, then perhaps  box plots should be our choice. Using the teaching evaluation data, we have plotted a histogram of  teaching evaluation scores. You could see that the mean score is around 4, but then you could see  very low teaching evaluation scores, not many frequently, but most frequently it's around the  average and then you see that some have lower teaching evaluation scores and some have fairly  high teaching evaluation scores. The histogram approximates the normal distribution curve.  Essentially you have 3.9904 as the mean, the standard deviation of 0.55 looking at 463 records.  This gives you a good idea of how your data are distributed. You can in fact plot multiple  histograms such that you can see the difference between the subgroups. So here you have the  histograms overlaid for males and females. These frequent lower teaching evaluations for females  is likely to influence the average teaching evaluation score for females versus the males.  A box plot essentially looks like this. The thick line in the box represents the median.  The top part of the box is the third quartile. The bottom part of the box is the first quartile.  The line at the bottom is the minimum value and the line at the top is the maximum value.  And the range between the first quartile and the third quartile is called the interquartile range.  In this graphic we have created the box plots for the age variable.  We can see that the median age of males is higher than the median age of females.  Also the maximum age of the males is higher than the maximum age of females. To do this in Python  we use the box plot function in the Seaborn library. We will put the gender on the y-axis  and the age of the instructor on the x-axis. You can play around with the x and y-axis.  If you wanted a horizontal style box plot for readability, I like to use vertical box plots.  We can also add another dimension. Here we will add tenure so those who are tenured are plotted  on the right and those who are not tenured are plotted on the left. And the blue color represents  the female instructors and the orange color represents the male instructors. We can see  the differences between male and female. Instructors, male tenured instructors are  older than male untenured instructors, whereas female tenured instructors are younger than  female untenured instructors. To do this in Python add the hue argument to the box plot function.  A pie chart is another way of looking at your data. You can see here in this graphic that the  number of courses taught by male instructors is larger than the number of courses taught by female  instructors. To do this in Python we will use the matplotlib library. First we specify the labels.  Get the number of courses taught both by male and females and assign it to a sizes variable.  Create a subplot, insert the sizes, labels, and percentage to one decimal place in the pie function  and print out the pie chart with the show function.  In this video I will introduce the teaching ratings data. We have been working with the  teaching ratings data from University of Texas and the underlying question is if students'  teaching evaluations are influenced by the looks of individual instructors or you can ask if their  teaching evaluations differ by gender or if good-looking instructors are influenced by  by gender or if good-looking instructors get higher teaching evaluations.  I obtained this data from Professor Daniel Hammermich who has written a paper about  how beauty may impact an instructor's teaching evaluation. In fact he has written an amazing  book called Beauty Pays in which he answers these questions such as do you think good-looking  employees get higher pay or faster promotions? Do you think good-looking instructors get higher  teaching evaluations? And the data comes from University of Texas. It's a survey  data obtained from 463 courses.  So the data is first referenced in the book Getting Started with Data Science,  Making Sense of Data with Analytics in Chapter 4. There are variables that essentially define  the attributes of instructors and characteristics of the courses. Some variables are continuous,  others are dichotomous or categorical variables. So the primary two variables of our interest are  beauty score which is basically the physical appearance of an instructor which was ranked  by a panel of six students and I think that they have normalized the the beauty score such that it  had a mean of zero and variance of one or a standard deviation of one. It's the same as  z transformation. The dependent variable, the variable of interest is evaluation which is  basically the teaching evaluation ranging between a scale of one to five, one being very unsatisfactory  and if the student found the course to be excellent then it's five. And then there are other  dichotomous or binary or categorical variables such as minority and if the instructor was a  non-Caucasian. Age is a continuous variable and it's the professor's age or instructor's age,  gender being male or female. Native stands for native English speaker. If the instructor was a  native speaker of English language, one zero otherwise. If the professor was tenured, one zero  otherwise. I have produced some descriptive statistics for your reference. For instance,  for the continuous variables such as age and beauty and teaching evaluation and the number  of students who were enrolled in the course on the number of students who performed the teaching  evaluations, I produced the descriptive statistics such as the minimum, the maximum, mean and standard  deviation. For categorical variables such as gender, female, yes or no, visible minority,  yes or no, person being a tenured professor or otherwise, I produced the frequency distributions  and percentage of individuals falling in one category or otherwise. Notice that the teaching  evaluation score is 3.99 with a standard deviation of 0.55 and let's see if I were to produce a  histogram of this variable teaching evaluation, how will it look like with raw data and if I were  to use the normal distribution and feed the two parameters that is the mean and the standard  deviation, how will the same distribution look like using a normal distribution? I am presenting here  the distribution of the raw data on left side and the presentation of the same data with the  same parameters, the mean and standard deviation using normal distribution. You could see that the  data not exactly following a bell curve, this is the raw data and it seldom does but then the  theoretical distribution looks like this. Essentially the same data set with a mean of 3.998  and standard deviation is presented here and then a normal distribution drawn from these two  parameters appears here. So it's much smoother, theoretical distributions are much smoother than  the raw data.  Now let us visit some basic definitions about probability as it relates to the most commonly used  concepts in statistics. Essentially probability is a measure between 0 and 1 for the likelihood  that something or some event might occur. For instance you may hear that the stock markets,  the chance of stock markets rising above some point or falling below some point is x percent or  you may hear that the chance for rain is 45 percent tonight. These are all coming from this  very concept of probability. Essentially probability is a measure between 0 and 1 so 45  percent would be 0.45. The discussion about probability is not complete without a discussion  about random variables. Essentially a random variable is a quantity whose possible values  depend in some clearly defined way on a set of some random events. It's a function that maps  out outcomes that is points in a probability space. So probability space essentially is all  possible outcomes. If you roll a die it can have one out of six outcomes so that's the probability  space there. If you roll two dice you can have one out of 36 outcomes where each outcome could  be considered a random outcome. And a probability distribution is a theoretical model that depicts  the possible values any random variable may assume along with the probability of its occurrence.  We'll define this more with examples using two dice. So consider two dice. A die has six faces  if you roll two dice it can assume one out of 36 discrete outcomes. So if you were to roll  two dice the probability that both die would have one as the outcome would be  one plus one two and there's only one possibility of getting that and that's one out of 36.  So here we have two die one black and one white and if you were to look at the possibility of  getting one on black and two on white and that's one outcome and so that's one plus two is three  or you can have two on black and one on white that's two plus one three again so there are two  ways of getting three on by rolling two dice so the outcome or the probability is two out of 36  possible outcomes that are mapped out here. So if you think about the sum of two dice being two  there's only one possibility out of 36. Getting a three you have two possibilities. Getting a four  you have three possibilities. Getting a five you have four possibilities and so on so forth. The  maximum most frequently possible number to have as a sum of two dice is seven and the probability  is six out of 36 which is 0.167 and if you were to sum these probabilities up that is 0.02 plus  0.056 you get 0.083. So if you sum up all these they all sum up to one and the probability of  getting six or less and greater than six is 0.58 or getting less than equal to six is 0.417.  You sum these up it's one 0.028 plus 0.972 is one this plus this is one this plus this is one and  obviously one plus zero is one. The probability of getting 12 that is both dice show six is one  out of 36 possible outcomes which is 0.028 so the probability sums up to one and the probability of  getting more than 12 is obviously zero because the two dice can maximum produce this number.  So nice way of looking at the way a probability distribution space is created by rolling two dice  and if I were to look at the the probability of say getting some number or less than some  number that's called the cumulative distribution function. If you were to simply chart this  probability outcomes in a chart you get this graph here.  So here we have age as our variable and I created a histogram of age and then using the mean value  of 48.37 which is the minimum mean average age and a standard deviation of 9.8 years  I can fit a theoretical normal distribution with these two parameters.  In this video I will illustrate how to state your hypothesis when you're comparing the averages  between two entities. We will use basketball as an example using Michael Jordan and Will Chamberlain  the two highest scorers in the history of basketball as examples. If you were to recall  you would know that Michael Jordan on average scored 30.12 points in each game and Chamberlain  averages around 30.06 points per game and if you were to compare the two averages between  Michael Jordan and Chamberlain and even though they are very similar looking numbers  we need to set up a statistical hypothesis testing. We are interested in comparing the  average points scored by the two basketball players and the comparison of means or averages  is available in three flavors. First we can assume that the average points per game scored by the  two players Jordan and Chamberlain are the same that is the difference between their mean scores  is zero. If the averages are the same so average of one minus average of other should be zero  and this becomes a null hypothesis. Let's say if mu j mu subscript j represents the average points  per game scored by Michael Jordan and mu subscript c represents the average points  per game scored by Will Chamberlain we can state the null hypothesis to be mu j equal mu c that is  the average scored by Jordan and average scored by Chamberlain are the same and the alternative  hypothesis would be that no these averages are not the same they're different so the alternative  hypothesis h subscript a compared to null hypothesis h subscript zero or o the alternative  is that the averages are not the same their average scores are different. Now the other option  the second option is to assume that Jordan scored better or higher and in that case our null  hypothesis is the average score by Michael Jordan is greater than or equal to the average score by  Will Chamberlain and in this particular case the alternative hypothesis would be different it  wouldn't be not equal to but it would be less than so the alternative would be that the average  scored by Jordan is less than that by Will Chamberlain and by the same account our third  option would be that the null hypothesis is that in fact Jordan scored less than Will Chamberlain  and the alternative hypothesis will be the reverse of it saying that no Michael Jordan scored higher  than Will Chamberlain so in a nutshell we have three options we can say the averages are the  same and the null would be no they're not the same not equal we can say the average is less than the  null is that Jordan's average is higher than Chamberlain and the alternative would be the  reverse of it and the third option is to say that the average scored by Jordan is less than average  scored by Chamberlain and the reverse of it will be the alternative hypothesis. So three ways of  defining a hypothesis let me introduce you to normal distribution which is one of the most  commonly used distributions in statistical analysis and even in everyday conversations  a large body of academic scholarly and professional work rests on the assumption  that the underlying data follows a normal distribution the defining characteristics  of normal distribution is this bell-shaped curve which you are familiar with from your textbooks  mathematically normal distribution is presented by this equation we can say that the normal  distribution relies on three inputs and f stands for functions a function of x mu and sigma x is  your data and x is a random variable and it can attain any very reasonable value mu stands for  the mean and sigma is standard deviation and the mathematical formulation is right here which is  1 divided by sigma times and then the square root of 2 times pi pi is the 3.14 22.7 22 divided by  7 and then you have the exponential here do not forget this minus sign so exponential of this  entity which is minus and in the numerator x minus mean or x minus mu the whole square divided by 2  times sigma square so let me explain 1 divided by the standard deviation and then the square root of  2 times pi 2 is known and pi is known so is the value for exponential and what is not known is  the sigma which you obtain from the data that is standard deviation and the mean which is also  coming from data so you have the mean and the standard deviation and x is the random variable  whose mean and standard deviation you're using you put this all together and then you get the  normal distribution the bell-shaped curve that you saw earlier i also introduce you to the standard  normal a standard normal is when we say that there's x is a variable that has a mean 0 and  standard deviation of 1 so what's mean 0 and standard deviation 1 look like if you replace  mu with 0 and standard deviation or sigma with 1 the equation is reduced to this entity which is  1 divided by 2 times pi notice the sigma here which i've grayed out a bit so that it doesn't  interfere sigma is 1 so 1 times anything would be the same so i've removed this and then e to the  power minus x minus mu but because mu is 0 so x minus 0 is x so x squared divided by 2 times sigma  square sigma remember is 1 the square of 1 is 1 so 2 times 1 so removing sigma because it's 1 and  anything multiplied with 1 is is the same entity so how do i generate this this normal or density  or a bell curve let's assume that the underlying variable has a mean 0 and the standard deviation  of 1 and x varies between minus 4 and 4 so the mean is 0 and the minimum value is minus 4 the  maximum value is 4 and i would substitute these minus 4 to 4 values in this equation this is the  only thing that's changing the x here is the only entity that is changing and let's see if this  could generate the standard normal curve let us do this in python we'll use the matplotlib function  which you are already familiar with for the graphics numpy library as well as the norm.pdf  function in the scipy stats library in this example i have used increments of 0.1 this  will generate the standard normal curve that you hear about in statistics  the other commonly used statistical distribution is known as the student's t distribution  william seeley gossett specified the t distribution in fact he published a paper in biometrica in 1908  and he published it under the pseudonym student he worked for the guinness brewery in durban  ireland where he worked with small samples of barley mr gossett is the unsung hero of statistics  he published his work under a pseudonym because of the restrictions from his employer apart from  his published work his other contributions to statistical analysis are equally significant  the cult of statistical significance a must-read book for anyone interested in data science  chronicles mr gossett's work and how other influential statisticians of the time  namely ronald fisher and egon pearson by way of their academic bona fides ended up being  more influential than the equally deserving mr gossett the normal distribution describes the  mean for a population whereas the t distribution describes the mean of samples drawn from a  population the t distribution for each sample could be different and the t distribution resembles  the normal distribution for large sample sizes here i present normal distribution which is  drawn in blue and the t distribution with a degree of freedom of one as the degrees of freedom  increase the t distribution curve becomes more similar to the normal distribution  in statistical analysis several statistical tests rely on t distribution for instance a comparison  of means test used the t distribution and it's also known as the t test we have been working  with a data set comprising teaching evaluations of instructors from university of texas  and i will illustrate the use of t tests or t distribution with the question of does instructor  evaluation score differ by gender do males and females get different teaching evaluations from  students now if i were to take the same data set and compute the means and standard deviations  i can test this statistically i have computed the visual representation of the average teaching  evaluation score for male and female instructors the blue bar represents the average teaching  evaluation value for females the orange bar represents the average teaching evaluation  value for males by eyeballing it it is around four and slightly less for females now it's a  small difference between males and females the question is is this difference statistically  significant to use a t test you have to make sure some assumptions are met the first assumption for  a t test is that the scale of measurement applied to the data collected follows a continuous or  ordinal scale the second assumption is that the data is collected from a representative  randomly selected portion of the total population the third assumption is the data when plotted  will follow a normal distribution and the final assumption is homogeneity of variance to avoid  the test statistics to be biased towards larger sample sizes there is a test for this which will  be discussed later before we go perform the test in python first we will state our hypothesis  the null hypothesis is as follows there is no difference in evaluation scores for males and  females the alternate hypothesis is there is a difference in evaluation scores between males and  females then set alpha level to 0.05 to do this in python we will use the t test independent sample in  the scipy stats function the function takes in the two samples it is trying to test the statistical  difference of means for in our example is the females evaluation scores versus all the males  evaluation scores it will return a t statistic and a p value since the p value is less than 0.05  the alpha level we reject the null hypothesis as there is enough evidence that there is a  statistical difference in teaching evaluations based on gender  let me illustrate how to obtain the probability of getting a high or low teaching evaluation score  from our data set first an important concept is the standardization of a variable such that it  returns a data set with a mean of zero and a standard deviation of one i use the formula in  equations shown here where the standardization is taking a variable x and subtracting from it the  average value mu then dividing it by the standard deviation so that if the teaching evaluation score  of an instructor on a scale of one to five is 4.5 we subtract the average teaching evaluation of 3.998  from it and divide it by the standard deviation which is 0.554 resulting in a z score of 0.906  if we were to just display the data as a histogram you would see that it has a mean around zero and  the spread is shown on a scale where the x-axis varies from minus 3 to 2 in a case where you do  not have access to a computer with statistical software you can still compute probabilities  from a probability table using a simple and standard normal table found in statistics textbooks  or download it online a copy of such a table is on the right notice that the normal distribution  graph to the left is grayed out in some parts that grayed out area represents the probability  of getting some value z in this case z this value of z or less than we will need to first  standardize the variable to determine the probability of a teaching evaluation score  higher than 4.5 or less than 4.5 so let's say we have a data set where the average teaching  evaluation is 3.998 and the standard deviation is 0.554 and we are interested in determining  the probability of getting a teaching evaluation score of 4.5 or less so from the table that I  showed in the last slide we can determine this if we were to standardize the data it becomes 0.906  because the accuracy of this table is only good for two decimal places so 0.906 effectively becomes  0.91 so we get a 0.8186 value here hence the probability of obtaining a teaching evaluation  score of 4.5 or less is 0.8186 if you were to look at this graphic you will see that I have  plotted the area under the curve by shading it gray that's the area that depicts the probability  of an instructor receiving a teaching evaluation of less than or equal to 4.5 and that probability  is 0.8176 or 81.76 percent now what will be the probability of receiving a teaching evaluation  score of greater than 4.5 in fact you can see from the next graphic that the probability is  the reverse of one that we saw earlier and hence the probability of obtaining a teaching evaluation  score of greater than 4.5 is 18.24 percent which is the area shaded in gray the reason for this  is because the area under the normal distribution curve is equal to one so one minus 0.8176  will give you the area for evaluation scores greater than 4.5 let me illustrate the example  of getting a teaching evaluation score of greater than 4.5 in python we will use the norm dot cdf  function in the scipy stats package after finding the mean and standard deviation we plug it into  the function with the x value of 4.5 and we will get the area to the left which is the less than  4.5 area because we want the area to the right of 4.5 that is the probability of greater than 4.5  we will remove the value from one as indicated here  in traditional hypothesis testing one has the option to perform z-test or a t-test and the question is  under what circumstances should one perform a z-test or a t-test well the answer is rather  simple if one is aware of the population's standard deviation or variance we use the z-test  and that is when we are comparing the sample mean to a hypothetical or a population mean  and if the sample the population standard deviation is not known and we are comparing  the sample mean against the population mean with an unknown standard deviation  then we use the t-test now there are four scenarios in which we perform these tests  first scenario is where we are comparing a sample mean to a population mean and the  population standard deviation is known in that particular case we use a z-test  and in cases where we are comparing a sample mean to a population mean with an unknown standard  deviation we use a t-test now this i covered earlier in the last slide the new thing here  is that when we compare the means of two independent samples that is comparing the  means of two independent samples with unequal variances if we are using if we have facing with  this kind of a question we use a t-test again if we are comparing the means of two independent  samples with equal variances we still use a t-test the underlying theory is that when you're using a  z-test you're basing your results on normal distribution and when you are deploying t-test  you're basing your results on t-distribution and this could be made the the process of hypothesis  testing could be made rather simple by looking at these thresholds if you are comparing the means  and in the particular case you are looking at the null being that the two averages are the same  against two averages not being the same then you're using a two-tailed test and in that particular  case you're looking for a t-statistic or a z-statistic of 1.96 the absolute value of 1.96  if that were to be the case you reject the null hypothesis that is you're comparing you're  conducting a two-tailed test you can be using normal distribution or a t-distribution and you  get the calculated z or t statistics of greater than absolute value of 1.96 and the expected p  value the probability of that happening would be less than 0.05 and you reject the null the null  being that the two means are equal in the case of one tail test where you're testing whether the  mean or average of one entity is greater or less than the other here the absolute value for z or  t statistic is 1.64 and the probability would still be less than 0.05 if that were to be the  case you reject the null if the calculated value for z or t statistic is less than 1.96 you fail  to reject the null hypothesis and the null being the two averages are the same in case of a one  tail test and the value calculated value for z or t statistic is less than 1.64 you fail to reject  the null and the null could be that one value one average is less than equal to the other  or is greater than the other  one needs to understand the theory behind the hypothesis testing and how do you reject a null  hypothesis or otherwise there are rules of thumb that is in case of a two tail test one can use  1.96 as the calculated threshold for either z or t statistics to reject a null hypothesis  or for a one tail test the absolute value of 1.64 to reject a null hypothesis but what does it mean  and the the how do you get to these 1.64 and 1.96 there is some some theory to it it involves  statistical distributions and now perhaps is a good time to to learn about those now imagine if  the mean values of two variables is the same that is we are assuming that the difference between the  two means is essentially zero let's say the mean of variable a and the mean of variable b we assume  that they are equal that is mu a equals mu b and if that were to be the case the difference between  the two should be equal to zero so the alternative hypothesis could be that the differences mean is  not equal to zero so you would say that mu a is not equal to mu b or the difference is greater than  zero that is mu a is greater than mu b or the difference is less than zero where mu a is less  than mu b and in these three circumstances the rejection region or the how do you reject the  null hypothesis means three different things and for this we were to revert to the normal  distribution curve imagine that you're conducting a z-test using a normal distribution and the shape  of the curve would be very similar this image would be very similar if we were to do a two-tailed  test for t distribution but let's assume that we are working with normal distribution  and you have a rejection region that is to the left and to the right of the of the curve as you  saw the normal distribution curve let's say our alternative hypothesis is that the mean difference  is not equal to zero it could be greater than or less than zero but it's not equal to zero so we  will call this a two-tailed test because we are not making an assumption of the difference being  greater or less than zero and then we have to define the rejection region in both tails that  is the left tail and the right tail of the normal distribution and remember we only consider five  percent of the area under the normal curve to find to define the rejection region and for a  two-tailed test that five percent gets divided into half of it goes into the left tail and the  other half goes into the right tail so two and a half percent under the curve in each tail and  graphically you could see this again as the same as we saw earlier that this is a normal distribution  curve and two and a half percent is in the left tail and the other two and a half percent is in  the right tail and if the test statistic is 1.96 if the absolute value of the test statistic is  greater than 1.96 or less than 1.96 it falls in the rejection region and you can safely reject the null  and the null would be that the difference of mean equals to zero or in common parlance what we are  saying is that the two means are not the same now let us work with the assumption or the situation  where we we are testing if the difference of means is less than zero we are only interested  in the left tail our alternative hypothesis is that the difference of mean is less than zero  in this case the entire rejection region that is five percent of the rejection region  is to the left and in any situation in for a one tail test if we were to get the t stat of 1.64 or  less we would reject the null that the mean is greater than zero in favor of the alternative  that the difference is less than zero and the exact opposite to this would be the right tail  test where the alternative hypothesis is that the mean is greater than zero and if you get the t  or test statistics of greater than 1.64 for a right tail test you reject the null  in favor of the alternative that the mean difference is greater than zero.  A t-test is the comparison of average values between two groups for example you could be  comparing whether teaching evaluations of male instructors is the same for female instructors.  You can either assume that the variance or standard deviation is equal or unequal.  How do we determine this? We have the teaching evaluation data. We calculated the average  teaching evaluation for female instructors to be 3.9 with a standard deviation of 0.53.  The average teaching evaluations for male instructors on the other hand  was calculated as 4.06 with a standard deviation of 0.55. When we conduct a t-test we are faced  with whether to assume equal or unequal variances. We have a t-test called Levene's test to determine  the equality of variances. The null hypotheses of the Levene's test is that population variances  are equal. If the p-value of the test is less 0.05 reject the null hypothesis of equal variances  and assume that the variances are unequal. Let us look at the example for that of teaching  evaluation scores for male and female instructors. We will use the Levene's function in the scipy.stats  package. We will run it against both samples and we will specify the center argument as mean since  our t-tests we test for mean differences. We will get a p-value of 0.66 which is greater than 0.05.  That means we will fail to reject the null hypothesis and we will assume equal variances  when conducting our t-test. So when you run your t-test you set the equal underscore var  option to true and if you got a p-value less than 0.05 you set that option to false.  If you were to do a t-test by hand the formula would be different for calculating equal versus  unequal variances. You must calculate the pooled variance as shown here then calculate the standard  deviation that you will use in the t-test. If the variances were unequal calculate the t-test  with each of the individual standard deviations and sample size. You will need to find the degree  of freedom to check the t-test table. With this formula the rule of thumb for assuming  equal variance when calculating by hand is defined by the ratio of the larger  group's variance to the smaller group's variance to be less than 1.5.  Most of us are familiar with comparing the average values between two groups.  For example the average teaching evaluation score for male instructors compared with that of female  instructors and the groups are two and we know that such comparisons are made using the t-test.  But what if you're dealing with more than one group and you're comparing the two groups  and you're comparing the two groups and you're comparing the two groups and you're comparing  such comparisons are made using the t-test. But what if you're dealing with more than two groups?  What if there are three four or more groups? In that particular case we would use ANOVA or  analysis of variance where our intent or goal is to compare the means of more than two groups.  So in order for do that in order to accomplish this we return back to our teaching evaluation  data and in that particular case we have a variable called age where the age of the  instructor is is recorded in a number of years but we will discretize this age variable that  is we will create three groups. So instructors who are 40 years and younger we put them in one group  those between 40 and 56.5 years of age they are in another group and those who are 57 years and  older we put them in the third group. So you have sort of younger instructors middle-aged instructors  and rather slightly older instructors and the number of observations or courses taught by each  group is is reported under n and what we also have here is the teaching evaluation score for each  group which is not differing much it's pretty much four for each group and for the older professors  slightly less at 3.9 and the respective standard deviations are there. So we have three groups and  let's say what we are interested in is to determine if the average of these three three averages for  the three respective age categories if these different averages are statistically the same  or they are different. So we use the one-way analysis of variance or ANOVA and using the  ANOVA we use the F-distribution to compare the mean values for more than two groups. Our null  hypothesis is that samples in all groups are drawn from the same populations with the same mean values  and we fail to reject the null hypothesis if the p-value or the significance for the F-test  is greater than 0.05 and we then infer equal means. Let's say we are interested in determining  if the beauty score for instructors differs by age. We have three groups younger middle-aged and older  professors. We have the summary statistics for the standardized beauty scores. We see that there is a  difference as the age goes up the average value for the beauty score goes down. So let's run an  ANOVA to see if the differences are statistically significant. Our null hypothesis will be mean  beauty scores for instructors don't differ with age and the alternative hypothesis will be at least  one of the means is different. First this variable does not exist in our data. We will need to group  or bin the continuous age data using the dot lock function in pandas. Then use the F underscore one  way function in the SciPy stats library to perform the ANOVA test. We will then print out the F  statistics and the p-value. What we can see is that the p-value is 4.32 times 10 raised to the  power of negative 8 and that is less than 0.05. We will reject the null hypothesis as there is  significant evidence that at least one of the means differ. If I do the same test for the teaching  evaluation scores that we observed for the three groups and we run ANOVA on these three mean values  we find out that the p-value is 0.295 which is greater than 0.05. We will fail to reject the  null and infer equal means that is that the three means are not statistically different.  Here we have the analysis of variance performed on two samples. One is the beauty score. We notice  that the difference in means for beauty scores between the three groups is based on the significance  value. This leads us to conclude that at least one mean is different and we reject the null hypothesis  that states equal means. And here because the p-value for teaching evaluation scores between  the three groups is greater than 0.05, we fail to reject the null hypothesis.  We believe that these three means are statistically equal.  Now moving ahead from comparing the average values between two or more groups  we are looking at two variables. We want to know if there is a statistically significant correlation  between these two variables and what is needed for this to happen.  We would need to look back to the earlier definition of types of variables.  We generally define the variables in two groups the categorical variables and continuous variables.  So if we were to go back to our teaching ratings data we have instructors who are male and female.  Some instructors are visible minorities and some are Caucasian. So we have two variables,  male and female, and a visible minority status. These two variables are examples of categorical  variables and if we are comparing or trying to determine the correlation between two categorical  variables we would use the chi-square test. We would begin with a cross-tabulation between the  two values. If we have two continuous variables, for example, the teaching evaluation score and  the beauty score of an instructor, then these are two continuous variables and they can assume  any reasonable value within the range. In this case we use a Pearson correlation test.  We usually begin with a scatter plot to see what's the nature of the relationship between  the two variables. Let us start with categorical variables. We will use the chi-square test for  association. First we state our hypothesis. We will test the null hypothesis that gender and  tenureship are independent against the alternative hypothesis that they are associated. Let's begin  with a cross-tabulation between gender male and female and tenure, that is, tenured profs,  then followed by a chi-square test. So we do the tabulations. In the rows we have tenured no  versus tenured yes and female instructors and males. We would like to eyeball these numbers  before we turn them into percentages. Looking at instructors who are non-tenured, we notice that 50  of the instructors are female versus 52 who are male. But for the instructors who are tenured,  145 of them are female and 216 of them are male. So within the tenured group we see greater  probability for males to be tenured, but in the untenured group the distribution between males  and females looks similar. Before we go to Python, let's do this by hand to understand the concept.  The formula for chi-square is given as follows. The summation of the observed value, i.e. the  counts in each group minus the expected value all squared, divided by the expected value.  Expected values are based on the given totals. What would we say each individual value would be  if we did not know the observed values? So to calculate the expected value of untenured female  instructors, we take the row total, which is 102, multiply by the column total, 195,  divided by the grand total of 463. This will give you 42.96. If we do the same thing for tenured  male instructors, we will take the row total, 361, multiply by the column total, 268 divided by 463.  We get 208.96. If we repeat the same procedure for all of them, we get these values. If we take  the row totals, column totals, and grand total, we will get the same values as the totals as the  observed values. Now going back to this formula, if we take a summation of all the observed minus  the expected values, all squared, divided by the expected value, we will get a chi-square value of  2.557, and the degree of freedom will be one. On the chi-square table, we check the degree of  freedom equals row one and find the value closest to 2.557. Here we can see that 2.557 will most  likely fall in between a p-value of 0.1 and 0.25. Therefore, we can say that the p-value is greater  than 0.1. Since the p-value is greater than 0.05, we fail to reject the null hypothesis that the two  variables are independent, and therefore we will conclude that the alternative hypothesis that  there is an association between gender and tenureship does not exist. To do this in Python,  we will use the chi-square contingency function in the SciPy statistics package. That is a chi-square  test value of 2.557, and the second value is the p-value of about 0.11, and a degree of freedom  of one. If you remember, the chi-square table did not give an exact p-value, but a range in which it  falls. Python will give the exact p-value. We can see the same results as on the previous slides.  It also prints out the expected values, which we also calculated by hand.  Since the p-value is 0.11, which is greater than 0.05, we fail to reject the null hypothesis that  the two variables are independent, and therefore we will conclude the alternative hypothesis  that there is an association between gender and tenureship does not exist.  This was an example of testing independence between two categorical variables.  Now to continuous variables using a Pearson correlation test from the teaching ratings data.  We will test the null hypothesis that there is no correlation between an instructor's beauty score  and their teaching evaluation score against the alternative hypothesis that there is a  correlation between both variables. We had the normalized beauty score on the x-axis and the  teaching evaluation score on the y-axis. You can eyeball a positive upward sloping curve,  but let's run a Pearson correlation test to find out.  We will use the Pearson r package in the scipy.stats package and check for the correlation.  We will get a coefficient value of how strong the relationship is and in what direction.  Correlation coefficient values lie between minus 1 and 1, where minus 1 means a strong negative  correlation and visually represented by a downward sloping curve, and 1 means a strong positive  relationship and visually represented by an upward sloping curve. In our case, we have a  Pearson coefficient of 0.18 and a p-value of 4.25 times 10 raised to power minus 5.  Since the p-value is less than the 0.05, we reject the null hypothesis and conclude that  there exists a relationship between an instructor's beauty score and teaching evaluation score.  In this video, we will introduce the fundamentals of regression analysis,  which we believe is the workhorse of statistical analysis.  Now, in terms of hypothesis testing, these tests measure the strength of relationship between two  or more variables and you have to run them independently, but if you know how to run  regression, we say as a practical data scientist, you can forego these tests and go straight to  regression, which is available in most spreadsheets and also in all statistical software.  So, here are the fundamental, the basics of regression model. First of all, you need a  question to answer using regression model. For instance, do male instructors get higher teaching  evaluations than female instructors, or does the beauty score decrease with the age of the  individual instructor, or is there an association between an instructor's looks and the teaching  evaluation score they receive? Do good-looking professors get higher teaching evaluation scores?  So, with these questions in mind, we focus now on the terminology of regression model. So,  there are two types of regression variables that we use. One is a dependent variable,  that is, the variable that we are really interested in, for example, the teaching  evaluation score of an individual instructor, and the explanatory variables that explain the  variance or differences or values of the dependent variable. So, for example, teaching evaluation  score could be explained by the looks or the gender or the English language proficiency of  an individual instructor. So, you have two types of variables, dependent and explanatory.  Now, let's look at the notation for a regression model. The dependent variable is donated as y,  so this y would be the teaching evaluation score, and the explanatory variables are denoted as x's,  so beauty, the gender, and the English language proficiency would be an x,  and the underlying assumption is that y is explained by x, that is, teaching evaluation  score y is explained by x, that is, the beauty score, or y is a function of x, which we write  as y is equal to function of x, that is, the teaching evaluation score is some function of  beauty. Statistically, if you run an estimate regression model, y is equal to some constant,  and then a weighting factor for the variable x, if it's a beauty score, then the weighting factor  for the beauty score, and the error term. An error term is whatever we cannot explain by the model  that goes into error term, and I will explain this a little more in a minute. So, y is equal to,  let's say the constant is beta naught, plus some factor or weight, which is beta 1 for x,  plus the error term, which we represent as epsilon, and then if you are familiar with your  basic statistics text, if there are more than one variables, then y is equal to beta naught,  that's the constant, plus beta 1, x1, beta 1 is the factor for one variable, that could be beauty,  plus beta 2, the other weight, explaining another x2, another variable such as English language  proficiency, so the weight for English language proficiency would be beta 2, and plus the epsilon,  which is the error term, explaining or capturing whatever the model couldn't capture.  If I had estimated a regression model using the dataset, teaching evaluations dataset,  it would look like the following. It would be the teaching evaluation score of an individual  instructor is equal to some constant, plus the weight for the beauty variable, and then times  the beauty score, plus the error. So here, teaching evaluation score is equal to, according to the  model, 3.998, that's the constant, plus the weight for beauty score, which is 0.133, plus the error,  and the error is epsilon, which is essentially the difference between the actual teaching  evaluation score that we have recorded in the dataset, and the one that we have forecasted  using this model. So the difference between the actual values and the forecasted values  is the error term.  In this video, we will illustrate how to use regression analysis in place of a t-test.  We will begin with a question, and the question is, is there a statistically  significant difference in teaching evaluation scores for men and women?  When we compute the averages while using the teaching evaluation dataset,  we find that the teaching evaluation score for women is around 3.9, and for men it's around 4.06.  The question is, is this difference, even though it's small, statistically significant?  We can run a t-test using Python and compute the statistical significance for the t-test.  Here, our conclusion is that the teaching evaluation score's difference between men and women  is statistically significant.  What if we were to do the same thing with the regression model?  We will do the linear regression in Python. We will be using the stats model library.  We will create a list for the independent variable, that is, the female variable,  which has been turned to a binary variable, where 1 equals female and 0 is male.  We will also create a list for the dependent variable, teaching evaluation score.  We will manually add the constant, beta zero.  Then we will fit and make predictions, and print out the model summary.  The model summary will print out a table like this.  But we are only interested in this part of this table for the t-test.  It prints out the coefficient error, t-statistics, and p-value.  We can see the t-statistics for the female variable is negative 3.25,  and the p-value is less than 0.05.  That means that there is a statistical difference in mean values for male and female instructors.  The coefficient means that you are most likely to lose about 0.17 marks for being a female.  We can see that the results from using a regression model and the conclusion is identical,  if we run a t-test.  When we are comparing the difference in means, or when we are comparing the averages between  groups that are more than two, we will use ANOVA, or analysis of variance.  We know that if there are only two groups, we can use the t-test.  But when we are comparing averages for more than two groups, we use analysis of variance.  Working with our teaching evaluation data set, we took the teaching evaluation scores and then  we wanted to see what would happen if we took the instructors and divided them into three groups,  40 years and younger, those between 40 and 57 years of age, and those that are 57 years or older.  We computed the average value for teaching evaluation score for the three groups.  We wanted to determine if the three mean values were statistically different.  To recap, we ran the analysis of variance test, which uses F-distribution.  The p-value is less than 0.05, so we rejected the null hypothesis that averages of the group are  equal and concluded that the differences are statistically significant.  Now, let us do this with a regression model.  We will use the stats model library and also import the OLS function.  We will create or initiate a linear model of the beauty score, which is our Y variable.  Please note that when dealing with a linear regression model,  the Y variable has to be a continuous variable. Otherwise, results will not be accurate.  Now, create the linear model and fit it using the fit function.  Use the ANOVA underscore IM function to create a table  that prints out the results of the test statistics.  The results will look like this.  It will print out the degree of freedom, the sum of square, F-statistics, and the p-value.  Like ANOVA from the SciPy package, we get the same results,  which is that we will reject the null hypothesis that averages of the group  are equal and conclude that the differences are statistically significant.  You can also turn the age group values into dummy values and run it like you run the regression for  t-test. To do that, you will need to create dummy variables for the age groups using the get  underscore dummies function in pandas. It will look like this, where one means they belong to  that group and zero means otherwise. Just like a binary variable, values can only belong to one  group. Run the same as you did for the t-test by fitting the variables into an OLS function,  predict, and print out the model summary. We will get results like this.  Taking a closer look, we can see the same results for the F-statistics and the p-value.  In this video, we will illustrate how one can use regression models in place of tests  conducted for correlation analysis. We will return to the basics. There are two types or mostly two  types of variables. First are the categorical variables for which we use chi-square tests  to determine if there is an association between the two. And second are the categorical variables,  or we could have continuous variables where we use the Pearson correlation test.  We will focus on just the continuous variables. We can plot two continuous variables in a scatter  plot. The teaching evaluation scores are on the y-axis and the normalized beauty scores are on  the x-axis. You could sort of see a relationship between the two variables. It's an upward sloping  type of a relationship. We see that as the beauty score increases, so does the teaching  evaluation score. Remember, we use the Pearson correlation test to determine the relationship  and its significance level. Now let's do the same in regression. Just like we did with the t-test  and the F-test, we will fit a linear model for both the beauty and evaluation score values,  and print out the model summary.  Taking a closer look, it prints out a p-value of 4.25 times 10 raised to power negative 5,  which is less than 0.05. That is very similar to when we run the Pearson r function.  It will also give us the r-squared value. That is, if we took the square root of 0.036,  it will give us 0.189, which is the same value as the correlation coefficient  from computing the Pearson r. 