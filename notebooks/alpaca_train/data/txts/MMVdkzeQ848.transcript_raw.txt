 Now I've been getting quite a lot of questions about logging for containers and Kubernetes.  People talk about the EFK stack, they talk about the ELK stack.  For many folks who are starting out and new to the technology, there's quite a lot to  understand about these technologies and why and how they're used.  This is a movie about centralized logging.  In traditional systems, applications write logs to files.  This can be troublesome when the disk gets full.  Log collectors can be used to ship logs away from the disk, but this adds extra engineering  overheads to install and maintain.  Applications can also be programmed to send logs directly out of the system, somewhere  else.  This avoids writing to disk, but results in a lot of duplicate code in each of the applications  for pushing logs out of the system.  So I thought this would be a good time to start a series about logging.  Now with logging, there are two categories that I want to discuss before looking at Docker  and Kubernetes.  Number one is standardized logging.  Number two is centralized logging.  I like to keep my applications as simple as possible.  Applications should have very little knowledge about where the logs go.  If your company decides to use log system A, you don't want code about log system A  in every application that you build.  That means if your company wants to change from log system A to B, you have to rewrite  all your applications.  If log system A changes their API or SDK, you have to change all your applications.  To improve all of this, in Linux, processors write to standard out.  Standard out is the default file descriptor that processors can write to.  Instead of writing logs to a file, it goes to standard out.  This prevents the disk from getting full and provides simple standardized logic in our  apps to send logs to standard out.  When processors run in containers, the container runtime gathers all these standard out logs  and manages them for us.  The container runtime takes care of the lifecycle of these logs.  So let me demonstrate this.  If I run a container, let's say I say docker run redis and I run a redis container, look  what happens.  Now because I'm running the container with the minus IT flag, docker picks up all these  standard out logs and prints it to the terminal.  You can see all the redis logs are shown here.  If I open up another terminal and I say docker ps, you can see we have redis running.  I can go ahead and grab the container ID and I can say docker logs with the container ID  and I can get the same logs.  We can see the container runtime collects all the standard out logs for each container.  Now remember a container is just a process.  So if I ran redis on top of Linux without a container, I would see exactly the same  thing.  This is because if we look at the documentation, by default, docker logs shows the commands  output just as it would have appeared if the command ran interactively in the terminal.  So whether it's running in a container or not, it's going to behave the same.  Docker picks up all the logs from standard out and standard error.  So if docker is by default writing all the container logs to standard out, how do we  access those?  So if we take a look at the docker host and we ran docker system info, if we scroll down,  we can see that docker root directory is set to var lib docker.  This is the directory that docker stores all its data, including logs.  So if I say docker ps and I grab the ID of the redis container and then let's try to  access that docker directory by running a container and mounting var lib docker into  the container.  So if I go ahead and run that and I say Alice var lib docker, we can see that we have all  of these folders inside of that directory.  And if I say Alice on the var lib docker containers directory, you can see we have all these container  IDs.  And if we say Alice on the var lib docker containers directory, and we use the folder  of that container ID of redis, you can see we have all this data inside of that folder.  And interestingly, we can see that there's a big text file, which starts with the container  ID dash Jason dot log.  So if I take that file, if I say cat var lib docker containers, that folder and that file,  we can see it'll just print out the docker logs.  So the standard out logs of redis is being stored on the container host in that directory.  Now internally, docker writes its logs in Jason format.  And we can confirm this by saying docker system info.  And if we take a look at the logging driver, we can see it's Jason file.  Now it is possible to configure docker and change the default logging driver.  So we can tell docker to write logs to a file to an external system or a database.  But I would always recommend to probably keep the default setting and not have to poke the  docker host.  If you're running in a system like Kubernetes, you would prefer the docker host to have its  default setting.  Now, ideally, we'd like to keep things very simple.  So docker can just write to Jason file and take care of rotating the logs and maintaining  the file system.  And remember, if you run a system like Kubernetes, we're going to have many docker hosts.  So we don't want to have to go ahead and configure that.  And remember, if you're running a container orchestrator like Kubernetes, you're probably  not going to want to interfere with the default settings of the docker hosts.  So I mentioned that we should keep our application simple by just writing logs to standard out.  I also mentioned that we should keep our docker host simple by keeping the default  setting and just pick up the logs from standard out.  This makes a great and effective standard for standardized logging, which is very simple.  But what if we ran many containers on many hosts like in a system like Kubernetes?  How can we consume and bring all these logs to a central place to achieve centralized  logging?  Centralized logging is all about being able to grab the logs easily from all over a distributed  system and bring it together to a single place where it can be analyzed.  With apps writing to standard out and docker reading standard out, we can configure software  like Fluentd to collect logs from the container host and ship it out before the host cleans  it up.  Fluentd can mount the host system folder.  It can read logs and send it to external systems.  It has a plugin system so you can send logs almost anywhere.  So to showcase centralized logging, I have a GitHub repo and everything I've been doing  in the series is in the monitoring logging folder.  And for this introduction, I have a Fluentd folder and inside there I have a basic demo.  Now in the basic demo, I have a simple docker compose file, which is basically going to  run Fluentd.  If I change my directory to this folder, I can say docker compose up and it will start  Fluentd.  Now what I'm doing in this docker compose is I created a volume and I'm mounting the  docker host var lib docker containers into a directory inside the Fluentd container.  I am also mounting a very simple Fluentd configuration file telling Fluentd how to collect logs.  If we take a look at this configuration, it's very simple.  We're just providing a source of where to gather logs from and where to output the logs.  So you can see here we're giving it the path to the containers folder that I showed you  earlier and we're telling it to pick up anything with the container ID dash json dot log.  So Fluentd will pick up all the logs from the docker containers on this host and then  we have a simple output of writing it to an output folder and I've mounted the folder  here so we can see the logs being collected by Fluentd.  So this is a very simple log that for demo purposes shows us how to collect all the container  logs on the system and send it to Fluentd.  So Fluentd will monitor the docker host on var lib docker and pick up any container logs  that any container writes.  And for demo purpose, it'll just dump it all to a file that we can see here.  So while Fluentd is running, let's go ahead and open up another terminal and start a Redis  container.  And if I run that, we can see the logs are writing to standard out as it usually would.  And if you give it some time, you can see in our logs folder here, Fluentd has started  to collect all the logs and starting to write it out to the file system here.  And if we give it some time, we can see eventually the buffer starts filling up and Fluentd starts  writing the logs to the buffers.  You can see here it's collected all the Redis logs in JSON format.  So this is a very simple demonstration of standardized and centralized logging and why  it's important in container environments before you start taking a look at Kubernetes.  And if you like this video, be sure to give it a thumbs up because in the next video I'll  be taking a look at a deep dive into the logging ecosystem.  We'll be taking a much deeper look at Fluentd, the ELK stack and the EFK stack.  We're going to take a look at how to set all of this stuff up locally, understand the fundamental  concepts, and then finally run it all on top of Kubernetes.  So also remember to check out the links down below to the community page.  And if you want to support the channel further, be sure to click the join button below and  become a member to gain extra perks on the community server.  And as always, thanks for watching.  And until next time, peace.  Transcribed by https://otter.ai 