 Okay, so I'll just give maybe a quick, very, very brief intro and hand things over if that  works.  Sounds great.  Looks like we've started.  Great.  Thanks, everyone, for joining us on this October EGC meeting, or geo-enlightenment.  We have Dylan Stewart from MakePath who's joined us, and I invited Dylan after watching  him present at the National States Geographic Information Council, and Dylan was presenting  really some cutting-edge, some really interesting work, but beyond that, I think he had a lot  of really good energy, and he's someone who it was very apparent that he was excited to  share what he's learned and how others can get maybe started in dabbling in machine learning  and geospatial, and I thought that would be a good fit for this crew and for our geo-enlightenments.  So I will stop there and just hand things over to Dylan.  He's got a presentation, and I think we should have some time for some questions and discussion  afterwards.  Great.  Thanks, John.  So I'll just give a brief introduction of myself.  My name is Dylan Stewart.  I'm a senior machine learning engineer at MakePath.  I have a PhD in machine learning from the Machine Learning and Remote Sensing Lab at  the University of Florida, and I've been working for several years now on applying machine  learning not only from a theoretical perspective during my grad school studies, but mostly  focused on geospatial applications.  So today I really want to talk about how we're going to connect machine learning with geospatial  applications and what open source tools are out there, especially in the Python ecosystem,  which is where I really focus on working, that we can leverage to be able to build pipelines  for geospatial projects.  So feel free throughout the talk, if you have any questions, put them in the chat, and hopefully  Tim or John can pick up on them and let me know.  I won't be watching the chat necessarily, but I don't want this to be a long, drawn-out  period of me showing things and then missing questions.  And I'll also take a pause kind of at the end of these slides for some questions, and  then walk through some more technical workflows to show you some of these tools.  So I want to talk about a couple of examples of applying machine learning for geospatial  applications.  And one of them is really about change detection for the Austin area.  And I'll talk about this here in a few minutes.  And the one on the right here, we'll talk about how to do geo-referencing between not  only different time periods, but also between different modalities.  So we've done some work in the past year with the state of Texas, working with their archives  of historical imagery, of being able to match imagery from the late 70s to imagery from  the 90s, where some of our input imagery is grayscale images, and the images we want to  match to that are actually already referenced are RGB images.  And how did we kind of go through framing that problem and putting the tools together  to do so?  So we'll talk first about how do we think about aligning imagery?  How do we think about this pipeline of putting together the tools to do this problem?  So the state of Texas has tens of thousands of images from 1940 to now that have been  captured and need to be geo-referenced.  And they've only gone through and really geo-referenced a couple of years out of all these different  captures.  So we've talked with the group there, and they want to look at applying new machine  learning techniques to try and assist an analyst in geo-referencing this data.  So we looked at some of their hand-aligned data, and we kind of broke it up into types  of scenes, because the state of Texas has various different geographies throughout.  It's almost like its own country.  Looking at the metadata that's available, and also looking at some of the edge cases,  because we might have many, many samples that are of rural regions, but we might have a  few samples that are of things along the coast that can be really hard to try to register  across time, because things like water that are just grayscale pixels of water from the  70s, it's really hard to tell that that's going to be water in the 90s or exactly what  that is, or if it's just a flat area.  So what we ended up doing was putting together what I call machine learning training datasets.  And so this is pre-processing the image, so that way it's able to be used in a pipeline  to learn some features off of it.  That can be used for actually doing the geo-referencing.  And we looked at custom machine learning models, and we were able to extract domain knowledge  about this problem to try to carve a path forward.  So this was kind of a small project, just a few weeks of taking a look at some of the  imagery from Texas, but being able to at least carve out how we could work at putting together  a full pipeline.  So let's take a look at some of the cool data that we worked with.  So like I said, Texas has many different geographies.  We broke those up into different types of scenes.  One of them that we called was just rural scenes, so scenes that have not been too developed.  We also looked at urban scenes, coastal scenes, which can be really, really difficult to try  to geo-reference across time, and then also some mixture.  Part of this was just a question on my end of, could we use some of this data that is  partially included by clouds to still get some kind of feature knowledge off of it?  So what we ended up putting together is really a two-part solution.  So we have some historical imagery, and let me see if I can turn my, yeah, there we go.  We have some historical imagery, and we want to be able to map it to the reference imagery  and end up having geo-referenced imagery.  So we kind of broke this up into two parts.  The first part we'll call rough alignment, which is just trying to find overlapping patches.  So imagine you have an image from the 40s or the 50s or some time period long ago, and  you're trying to match it with some RGB image of the 90s.  Well, the first thing you could think about is, well, let's just find overlapping patches  that are of similar area.  And so that's where we employed this HNet model, which I won't go into all the details  of it now, but if you have any questions, please feel free to ask, and we can look at  some of the intricacies of that model.  What that model basically does is it takes input samples and tells you whether or not  they're a match.  The cool thing about this is that this HNet model was used in the past to do things like  look at an RGB image and a depth image and tell you if they're from the same spot or  not.  And so we took that model and used it to be able to tell you if a grayscale image from  this spot or an RGB image from this spot, although they were from different time periods  and different modalities, if they were a match or not, which I think was a pretty cool application.  And so after having this rough alignment, which is finding the tiles that overlap, but  maybe they're not perfectly overlapped at the output of this model, we then broke it  into a fine alignment step.  So being able to actually finely mosaic imagery from different time periods together.  And so through the combination of these two steps, what we were able to do is get an 80%  accuracy of matching between new template images that our model never seen before from  one county in Texas and the reference imagery that was available from that same county.  And so we're looking to try to improve upon this and continue working on this in the future.  But right now, this is just a proof of concept stage.  So I'm going to switch gears a little bit and talk about some of the change detection  work that we've been doing at MakePath.  So here we have a large change mask over 6,750 square miles of Austin, Texas.  So this grayscale image is an image from Planet, and it's three meter resolution.  And if you know the Austin area here, this gold star is the Texas capital.  And we're able to combine this change image, which we got from a deep learning model with  parcel data to be able to predict areas of future development in the Austin area.  So we can use this pipeline we've put together for things like real estate, for drought detection,  oil and gas, and a variety of other applications.  And we're currently trying to develop a product with Planet and Google partners where you  can select an AOI and get back the change mask that you're interested in.  So we used RGB data from Planet from 2017 and 2022, and we trained a deep learning model  to be able to predict land cover change.  So these green areas that are popping up here, these are all areas of the imagery where  we detected changes over the five year period.  And we scaled up our model in the cloud using open source tools in the Python ecosystem.  And using parcel data from ReGrid, which is a parcel data provider, we were able to find  new parcels within the last five years by looking at areas where parcels split.  And by overlaying the changes in the RGB imagery from the deep learning model and where new  parcels appeared, we could combine those two layers to figure out where areas have  been split into new parcels, but they haven't been developed yet.  So this is the image of combining both the deep learning output of change detection from  the last five years and the new parcels that have popped up in the last five years.  So we're able to use the high res RGB data, and we're also able to use the ReGrid parcel  data, combine those together, and be able to come up with this mask, which shows you  all the parcels that haven't been developed yet.  So this kind of gives you areas of the Austin region, which will be developed in the future.  So we can zoom in on specific neighborhoods that have been built and look at the outputs  from our deep learning model and where new parcels have appeared.  So here we can see on the left, this is an area in 2017 that hasn't really been built  up yet.  In 2022, we can see all these neighborhoods here.  And so this image on the right, we see the green here, which is showing the output from  our deep learning model.  So all of these green pixels were pixels that have changed.  And then this red layer here are all the parcels that have appeared in the last five years.  So this shows that from the RGB imagery, we can detect these changes.  These changes also line up with the parcel data that has appeared over the last five  years.  And so using these layers together, we're able to come up with really cool insights  for the future.  So we use this to validate our approach.  And one of the cool things that we're kind of looking at now is trying to be able to  track construction without having to send people out on a site, which seems to be one  of the stories that people are really interested in looking at.  So I want to talk briefly about how we're actually detecting these changes.  So we're using a combination of four different popular packages within the Python ecosystem.  And really, we use a lot of open source tools and contribute a lot to open source.  This visualization at the top is showing each one of these bars is a single GPU in the cloud.  And we're using four GPUs in the cloud in parallel to be able to run the change detection  over 80 gigabytes of satellite imagery in just under a couple of minutes.  So for this change detection specifically, we trained a deep learning model in PyTorch.  And PyTorch is a machine learning framework for fast research prototyping and production  deployment.  We scaled up our model using Dask and CUDA.  So Dask is a way to horizontally scale your code.  So imagine you write some code and you've got it working on your laptop, but you want  to be able to run it on hundreds of machines in the cloud, or even hundreds of machines  on a server locally if you have that.  So Dask is really good at horizontal scaling.  CUDA is used to be able to run code directly on a GPU.  So for things like image processing and deep learning, which is really a huge focus that  we work on here, and even things like tabular data, CUDA is really great at speeding up  code and running on the GPU.  And then for things like preprocessing imagery, X-ray is great because X-ray works really  well with raster data and adds labels so you can have easier operations.  So I've worked a lot with hyperspectral data in the past, which has hundreds of channels  and hundreds of bands.  So imagine coding up that I want to take the hundredth band of this image.  Well, if you know that hundredth band corresponds to a specific near-infrared band, it's much  easier just to assign that label to that image rather than saying, give me a channel 100  and having to worry about the actual numeric indexes.  So I've talked briefly about a couple of applications that are very specific in applying machine  learning to large-scale geo, but you may be wondering how can we use this for whatever  problem you're interested in that you're working on.  So one of the cool things that John and I spoke about at NISGIC was some of the image  registration stuff that he's interested in and maybe this group's interested in, but  there are various different problems that kind of fit in the geo area where we can look  at applying machine learning.  So I want to kind of zoom out for a few minutes and just talk about generally how do we think  about applying machine learning to other problems.  So a common scenario that we see over and over again, especially in the work that we  do here at MakePath is that people have a lot of data in their database and they want  to get started and get some insights from that data and they're not really sure exactly  what to do from the get-go.  So this data might be some raster data, it might be stored in TIFF files or cloud-optimized  geo-TIFF files, might have some vector data, and that vector data might be stored in Parquet  or CSV or Excel spreadsheets, et cetera.  And hopefully these are sitting on the cloud where we can access them easily and add some  code to be able to process them and give you some insights.  But our goal is really that we want to gather insights from this data.  We don't really want to focus so much on getting in the weeds of what exact implementation  we're going to use, what exact model we're going to use.  We just want to figure out what model should we start with so we can get to work.  So maybe you've heard of machine learning before, maybe you've seen some cool demos  and you want to get started and think about how it can help you solve your problem.  So if you Google around a bit and look up how to get started in machine learning, several  of these tools that are shown on this slide are going to start popping up.  You can contact someone in your state or in your team and ask around about machine  learning and maybe they'll find some of these tools, or maybe you assign someone on your  technical team to learn more about machine learning and these tools will come in handy.  So I just want to briefly highlight a few of them.  Kaggle is a really great website that has run many competitions over the years, and  it's great for combining a lot of experts together who compete on trying to solve some  new problem with a new dataset.  And they'll come up with really amazing workflows that really pushes people to do cutting edge,  not just research, but cutting edge applied research to real problems.  So one of the cool things that I've seen from Kaggle over the years is people that are working  at all the big companies that are doing great work in machine learning and even people you've  never heard of that are all over the world will get on here and compete and come up with  amazing workflows that end up being applicable for various different problems.  So all of these resources, before I kind of dig into the weeds of all of them, they're  all free and you can get online and research and really go down the rabbit hole with them  as much as you want.  Scikit-learn is great for traditional statistics and machine learning and some new computer  vision.  And it's a really well maintained toolkit in the Python ecosystem, so I recommend looking  at that.  TensorFlow and PyTorch are really great for getting into the weeds of deep learning.  So if you have someone on your team that is really strong technically and can really  dig into this, then these are where you're kind of going to go to be able to do custom  models and optimizing models for a specific problem.  But I really recommend if you're getting started in this space to look at Fast.ai and Google  Colab because, as I mentioned, all these tools are free, but Fast.ai has a really great set  of reference material and courses and forums that you can get on and ask questions and  the community is really, really nice.  And I found them to be really helpful when trying to implement things.  In Google Colab, although there's this pro version now that has been a bit controversial  with some of the pricing, if you're doing the free work of just getting started, it's  a great resource for that.  And so you can kind of have your own virtual machine and your own notebook that you can  work on for free.  So maybe you've spent some time and you've learned a little bit about machine learning  from these different tools and you want to apply it to your problem.  So we'll talk about how you can do that.  So one of the things I want to kind of highlight here is that, as anyone knows, if you've applied  models to problems, when you train a model on some domain and try to apply it to another  domain, it can be very difficult to get it to work.  And so what I mean by that is, let's take an example of doing something like detecting  objects within Google Street View.  So maybe you have some imagery from Google Street View and you have some location data  from Google Street View of where these things are at, and you want to be able to detect  something like signs on the side of the road.  And you train your model off of Austin, Texas.  So you have all this data and your training set, you're training your model.  It's doing great.  And you're really excited and you're thinking, wow, I'm going to apply this somewhere else  and we're going to really make some changes here.  And then you try it somewhere in Vermont.  Well, now all of a sudden your model is not doing so great because one of the problems  that we run into is that the training data is not representative of the area that we're  looking at.  And so it can be really difficult to do things like transferring a model from one domain  to another.  And so that could be something like training on one city and applying that model to another  city.  And there's various different examples out there.  So what do you do if this doesn't work?  There are really two things that I want to highlight that you can do that can really  save you a lot of time and without getting too frustrated.  So instead of getting stuck in the rabbit hole of trying a lot of different solutions  and reworking solutions, the two strategies I really recommend are team up with someone  who's worked on something similar to this before.  Just like any other problem, we need domain expertise, and especially in the geospace  because it comes with its own problems outside of other domains.  I don't know the geography of all the different states or all the different cities or all  the different regions.  But if I know how the geography can affect your model, then that can be helpful.  And then knowing someone who knows that geography can be extremely helpful on the team.  So data expertise is also really important.  So who knows the data the best?  They need to be in the room.  And also, if you're thinking about doing something that's really difficult technically from the  machine learning side, then it might be that you need someone who's an expert in that space  and not just someone who's dabbled with it a little bit.  Especially if you want to do something experimental or if you want to try to carve out some new  model for your problem.  And it's really critical to spend time communicating and learning each other's languages because  as someone who's trained in the machine learning space, there's a lot of terms that we use  around other people that have been trained in machine learning that might mean something  different to you.  And I've run into this several times.  And it's also really important to investigate the data that you have on hand.  So think about how much data do you have?  What type of data do you have?  Is it raster data?  Is it vector data?  How precise of labels do you have?  Do you even have labels for your data?  Are you trying to just solve something without having any really good training examples?  And how should you augment the data?  And how often will you need to add data?  Or how many models do you need to be able to solve the problems that you're wanting  to solve?  So this can get really difficult of making these decisions and working through this.  But ultimately, we want to be able to extract value from the data that we have.  So let's hypothetically say you hired an expert machine learning and geospatial applications  to look at your data and problem.  So here are really the steps that we want to think about.  So start from the data.  There's been this huge craze in the past few years about all these new models that are  coming out and how AGI and all of these other buzzwords that are out there.  But really, we're thinking about applying machine learning for a specific problem and  trying to get value from that.  We really just need to think about the data that we have.  Because we want to think about data-centric machine learning.  So if you have some data in your cloud storage, hopefully, you can create some summary of  that data.  And that summary will contain things like what type of data is in there?  Is it raster data?  Is it vector data?  Is it cataloged well?  Is it something that I can easily look up samples from regions that I'm interested in?  What are some of the characteristics of that raw data?  So how many channels are in that raster data?  And what kind of schema is in my vector data?  Are there any redundant features which could be removed?  Other things you might want to think about are, are there areas where I'm missing data?  So what kind of geographical coverage do I have?  Do I have gaps where I need to go get samples?  Or if I have gaps there, is there any way I can augment the data I have to kind of fill  that gap?  So what unique characteristics are in my data that I can leverage?  And what other data sets might I need to pull in to be able to make this work?  So like any problem, the devil's in the details.  And if you have good knowledge of your data set, then you can really start looking at  using a model that lends itself to the problem and data at hand.  So think about the data, then worry about trying to apply the model.  So I want to hit on this term data-centric machine learning, because it's extremely important  now, especially as we've learned over the past few years, that you can train models  on data set one and apply it to data set two, and really get around having to worry about  mixing domains, as long as you pick a model that works well for your problem.  So what existing models can you leverage?  So you have your data on hand, you've got a good idea about what it is, what's available,  you pulled in other data sets, you've got it ready to use for your problem.  So we can think of machine learning really in two categories.  So there's classical machine learning.  Classical machine learning supposes a model from heuristics or statistics, it extracts  features that have been put together by experts.  It learns a mapping from a representation that's controlled directly.  So imagine changing the parameters of a model and really doing some tuning of knobs.  That's how you can kind of get value out of this model.  It's usually a low cost solution and a great first approach.  But really, where we've been moving in the past decade has been deep learning.  So that's really data driven learning.  So imagine these two steps of feature extraction and applying a model within this box being  done for you.  So this supposes a model based on learning directly from the data for a task or a set  of tasks.  And that's multitask learning, but we don't have to talk about that and get too deep there.  But these features are automatically computed from the data.  And these can lend to improving task performance.  So the examples I showed previously of change detection, and doing image registration, we're  really learning features from the data and applying that model all in one step.  We're not having to extract features, figure out what features are good or bad, throw them  out, go pick a model, figure out what model is good or bad, throw it out.  We're doing these two steps together, which can really help.  But there are some cons.  It can balloon in cost because having to use GPUs and cloud resources and kind of go down  the search of what model and how to train the model can be difficult.  But it's great when we have good representative data or a pre-trained model for a similar  problem that's available.  So the rules of thumb I kind of want to hit on here is no matter what problem, always  start with the simplest approach.  So look for some classical statistical approach that makes sense.  And if you need deep learning, then try to consult with someone who can kind of work  on this or look into applying deep learning for your problem.  The last resort is really to try to implement something custom and go into PyTorch or go  into TensorFlow and these toolkits and really try to dig through that, because that can  take years to do.  So since we know a little bit about our data and we want to use machine learning, where  can we find good models, especially for the geocase?  So I want to hit on a few different resources here, and I've shown a few of these previously.  Each of these have different models that are really useful for various different problems.  So the first one I want to talk about is Planetary Computer from Microsoft.  And I've just heard recently that AWS is also working on their own version of this, which  is cool.  That competition hopefully will add more to this space.  But Planetary Computer is a free resource, which leverages many different open source  libraries.  One of them is X-Ray Spatial, which I'll talk about briefly here in a moment.  And it performs functions for various different large scale cloud geoanalysis.  So Planetary Computer has many, many different examples on there.  All of them you can watch by signing up for the Planetary Computer, which is a free resource.  And it'll set up a VM for you and work you through different notebooks in Python that  can really show you some of these workflows.  And I also want to mention a little bit here about X-Ray Spatial.  This is a toolkit that we work on here at MakePath a little bit, and it performs functions  for rasters.  And it's backed with scalable libraries like Numba and Dask, which I'll talk about here  shortly.  So think about running really fast code on really large raster data.  That's kind of the benefits of using X-Ray Spatial.  And then Scikit-Learn here, you know, we talked about earlier how you can train classical  machine learning models and computer vision models with Scikit-Learn.  These kind of three tools are really good for getting started with some of the large-scale  raster analyses.  So if you're interested in deep learning, I really want to recommend my friend Robin  Cole's GitHub page, Satellite Image Deep Learning.  This is just a huge library full of various different applications of applying deep learning  for satellite image problems.  So think of things like fire detection, change detection, image registration, and, you know,  all these different things, road detection.  There's various different models that are shown in this GitHub page, and they all link  to different GitHub pages that have the models and have training data and workflows that  you can use.  So this is a really good place to get started.  TorchGeo is a library that has some rough edges, and it's got limited models in it right  now, but it's a work in progress, and it has some pre-trained models for things like tree  classification or for image segmentation, et cetera.  And there's some great notebooks as well on Kaggle and Fast.ai.  These are not necessarily geared specifically towards geo problems, but they do have several  of them within these two tools.  So it's important to note, at this point, I've really talked about the fun part of applying  machine learning to geo, which is really my passion and what I hope to continue working  on, but it's a really small percentage of the work that's done.  So I want to show this slide because this came out of a popular machine learning conference  a few years ago, and it really summarizes all the pieces for trying to apply machine  learning code to your problem.  There are all these different steps, and these are all kind of scaled by the amount of time  that they are.  And here in the middle is really my passion of working with a specific model to work on  your data and get you some kind of insight, but that's really a tiny portion of the work.  So I want to wrap up this portion of the talk just with a quick summary of some of the open  source tools that can be leveraged to scale up machine learning solutions to geo problems.  So for data preparation, Stack is a really great tool, as well as X-Ray.  These are really great for working with raster data.  Stack is a way to catalog your data into a JSON that can be easily searched and filtered.  So imagine having millions of files in your cloud storage, and you only want to grab files  from a specific date or specific region.  There are various different tools out there that can queue a Stack catalog and be able  to pull down that data that you're interested in.  X-Ray, as I mentioned earlier, it's really great for working with raster data and adding  labels to your data, so it makes sense with what channel you're applying what operations to.  Parquet is a really great format for large tabular data or large shapefiles, and that's  been the recent advancements with GeoParquet.  So adding on that geometry column and being able to store that efficiently.  For feature selection, I look at Scikit-Learn and X-Ray Spatial.  These two toolkits are really great for that, and we're adding more features with an X-Ray  Spatial all the time to improve feature selection.  For modeling, PyTorch and TensorFlow, these are for getting into the weeds.  And then Fast.ai is really for getting started.  And I recommend everyone just check out Fast.ai if you have any interest in coding in Python  for machine learning.  It's a great tool.  For scaling up tool, all of your tools look at Dask for scaling across machines or GPUs.  NVIDIA's CUDA is great for using code on the GPU.  Rapids is a toolkit out of NVIDIA that has various different open source libraries where  imagine you're doing work in pandas on tabular data, you can use CUDF from Rapids.  Or if you're using GeoPandas for your shape files and to do different vector analyses,  you can use Cuspatial.  And they're working diligently to try to have a one-to-one mapping between all of the features.  The last one I want to mention here in this portion of the talk is Numba, which is really  great for speeding up code on a single worker.  So imagine you have some code that's in NumPy or some other mathematical code that you want  to run.  Numba is really good at speeding that up.  So right now, I just want to take a brief pause if there are any questions before I  show some notebooks and really dig into more of the tools side of things.  Folks, feel free to either use the chat window or raise your hand.  There you go.  Eric Engstrom's got one.  Go ahead, Eric.  I have kind of a cheeky question.  I'm more or less curious whether or not customers are actually coming to you saying, hey, we  have a problem in Texas.  Are they really coming and saying, oh, we want to identify where there might be future  development?  Or is it more like, hey, we have these solutions.  Let's see if there's a problem.  Yeah, that's a good question.  So the proof of concept work I showed for the change detection came from conversations  that we had with Planet and Regrid about what would be a really cool thing we could all  work on together that seems like would be of interest to their customers.  And so putting together that proof of concept over the past few months has really been geared  towards a few specific customers that they have that are in the real estate business.  And that's kind of been the start of it.  After working on that proof of concept, we've been showing it around to other companies  and other people that we've met.  And it seems like there's been other areas that would be of interest to develop on.  And so that's why now we're kind of thinking about the different user stories to help develop  that product.  And the historic imagery work, that is something that Texas approached you to help solve, right?  Yes.  Yeah, because right now in Texas, they're paying analysts to hand register these images.  And their goal is to really have a tool that helps them.  And so we're kind of doing a little bit of R&D for them on this tool.  And the goal is to hopefully in the next year or so have all the technical side done.  So then they'll be ready to kind of pull the trigger on that.  I just thought I would actually chime in on the change detection.  I used to do a little work in it.  And we used it a lot for looking at tree canopy change and climate change, kind of like a  lot of municipalities want to know how they're doing on tree plantings and what their tree  cover was for the future, like temperatures in their cities and how they're doing on that.  So I just thought I'd put another use case out there.  Cool.  Yeah, thanks for that.  And it looks like Colin has a question.  How and where are you storing the imagery that feeds into the models?  What works best?  That's a good question.  So most of the, well, actually all the stuff I've shown today, we're using Google Cloud  to host the virtual machines that I'm using to actually use these resources.  And then we store the imagery that we're training on locally on whatever VM that we have the  GPU and CPU on that we're going to use to train.  The IO is really the biggest bottleneck I've seen, at least from my part.  Most of the time people can write code that runs pretty fast.  It's just about having the data where the code is executing.  So that way you're not slowed down by that.  But if someone is getting started, does it make sense to host, to just have stuff, a  few things locally or using something like Colab and having the data in sort of any kind  of blob storage?  Just imagining if someone doesn't have access to some VMs and they're just trying to load  and learn this on their own time, what do you recommend?  I would recommend if you get on Google Colab and you have a Gmail account, get on there  and have some of your data in Google Drive.  And that should be perfectly fine to get started.  There are several different Kaggle competitions that you can go download their data.  There's also fast AI tutorials where you can download their data onto Colab, onto your  Google Drive and then run the notebooks in a Colab environment.  And I think that's really the way to think about it is wherever you're going to execute  the code, try to put the data there because you don't want to be reading over the network  trying to train something or even apply something.  It's just going to slow you down a lot.  Do we have anyone from ADS in the cloud computing arena or the shared services on this call?  John or Tim?  I know we have Josiah, who's our new state head of AI and machine learning, couldn't  make it and did request this be recorded.  I see Chris, Bringa and James from Josiah's team here.  I don't mean to put – call anyone on the spot, but –  No.  I was just making sure that we had some support here so that they might be able to provide  some input as to which environments we would actually have access to and which are supported.  Yep.  Hopefully, Chris and James, you're ready for a slew of emails that will follow up with  this presentation.  Certainly.  Yeah.  I can help out with any questions that come up afterwards.  Cool.  Well, if there's no other questions right now, I can go ahead and dive into some of  the tools and feel free to ask questions at any time.  I know we've got about 20 minutes left here.  I want to get through these notebooks quick here and hopefully some of it will be of interest  and get some more conversation going.  The first tool I want to talk about is called Data Shader.  I don't know if anyone in this group has used Data Shader before or heard of it, but I'll  just give a brief introduction of what it is.  It is a general rasterization pipeline.  So if you use vector and raster data, and maybe you want to plot them together on a  single plot, you can take the vector data and the raster data and co-register them and  plot them together using Data Shader.  And so I want to walk through just a quick example of using Data Shader.  So this is the 2020 census data, and in this notebook, we're defining some areas of interest.  And feel free, if you're interested in these notebooks, let me know and I don't mind to  share them at all, but I just want to quickly go through what we're doing here.  We're using this data set called Synthetic People.  So Synthetic People takes the census blocks and takes that population field and extracts  a single point for each person within a census block and gives them the attributes that they  should have based on all the properties that we have within the census data.  So we've stored that data into a Parquet file, and Parquet, I mentioned before, is  really great for storing tabular data, and it helps with fast IO.  So in performance, you usually break that down into compute and IO, and Parquet handles  the IO portion of scaling.  And if you add on geometry to the Parquet file, you can actually use GeoParquet now,  which has been one of the advances in the last couple of years, which has been really  great for us.  So you want to think about choosing a data format that supports really fast IO, and Parquet  is binary.  It supports various different compression types.  It's a columnar store.  So if you want to load data that has 300 columns, but you only want to load two of those columns,  you can easily load just a couple of columns at a time, just whatever is of interest at  the moment.  And it's also partitionable.  So you can process partitions on various workers for the entire data set.  So to actually produce this data set, I'll show you here in a minute on the second notebook,  we took the census blocks, and we used DAS to horizontally scale some processing to be  able to get this synthetic people data set.  So within this data set, we just got X, Y, and race ethnicity information, and we can  use DataShader to be able to produce a canvas for the entire US.  And what we're doing right now is we're aggregating all of the points within each one of these  pixels to be able to come up with the count at every single pixel based on specific resolution.  And this is just showing that if we just use a linear scale, we can lose a lot of information  from the Midwest cities.  And in DataShader, you can specify how you want to aggregate the data.  So here is using a logarithmic scale, which looks much nicer, at least to my eyes.  And there's various different tools you can use within the DataShader pipeline to be able  to improve these plots.  So another cool thing you can do is use things like histogram equalization to try to balance  out the color values, and you can color like the 99th percentile red.  So this just shows all the high population areas throughout the US.  And we can also do things like visualizing different colors for different race ethnicity  information within the synthetic people data set.  So this is just showing across the entire US, and we're just plotting different race  ethnicity values for different colors.  And we can also zoom in on different cities.  So this notebook is not one that we put together originally.  This is from the 2010 census, but we updated the data for it.  So I just wanted to show you kind of what DataShader can be really useful for, because  this is aggregating over 300 million points within a single plot here.  And then if you zoom in on different specific cities, you can kind of see the different  spread of race ethnicity, and we're using the individual points and aggregating them  rather than just looking at the block.  So I just wanted to show that here briefly.  And the other thing I want to show you is how we can produce that data set.  So using various different tools, we'll start with Pandas here.  Pandas is really great for using tabular data.  And so imagine organizing NumPy arrays with labels.  And if you want to use geometry with that data, you can use GeoPandas.  So hopefully some of you in the Geo group here have seen GeoPandas before.  And it's really great in-memory data structure like Pandas.  And then we can scale up those operations on multiple workers using Dask.  So imagine you write some Pandas code or you write some GeoPandas code, you can scale that  with Dask.  And if you have GeoPandas code, you can actually use a recent toolkit called Dask GeoPandas.  And Dask GeoPandas scales those Pandas data frames with the geometry column.  And you can use these abstractions from Dask to be able to run code on many different workers  at the same time to be able to speed up your processing.  So it's a little rough around the edges as some open source toolkits are, but it's really  useful for doing large scale vector processing.  And so you can use Dask GeoPandas to be able to produce the census parquet data set, which  is something that we've done here at Makepath.  So since it's parquet, feel free to check this out on GitHub.  This takes all the census data from 2020 and exports it into parquet files.  There's also utility in here to generate the synthetic people that I'm showing here.  And these can all be run from the command line after you do the install.  So we'll jump back here to the notebook.  So in this notebook, we're using Dask GeoPandas to be able to horizontally scale up the census  blocks, which we've stored in parquet, to be able to get that synthetic people data.  So we load those blocks using read parquet.  So now we've got all these different attributes that we've kept up with.  This is just population and race and ethnicity information and the geometry for each of the blocks.  Then we write some code here to do polygons to points.  So for each one of those blocks, we're going to sample a bunch of points.  We're going to make sure that they fit within that block doing this checking here.  And then we're going to return a pandas data frame that has all the points with all the  race ethnicity information that we want.  One of the cool things about using Dask is it's scaling up really, really, really, really  simple code here, but makes it run really fast across many different workers.  And it's doing a lot of assumptions within its processing.  It's creating this task graph, and it needs to know what the output is expected.  So sometimes it doesn't know, and the code won't work very well.  But what you can do is just specify, hey, this is exactly the type of output I want.  I want it to be a pandas data frame with these specific columns, and these columns have these  data types.  And by specifying that, we're able to scale up this processing across many different workers.  So here we're saying run this polygons to points function on each partition of that  parquet file of this blocks.  And we want the output to be this output data frame.  And the cool thing about this is we don't want all of this to come back in memory on  our actual machine.  We want to just save this out directly into a parquet format in some other, you can imagine,  cloud storage or wherever you want to put it.  So we can actually just say map for each one of these partitions, run this function.  It's going to have this output, and don't give me back the data.  Go ahead and store out that data as a parquet, and write the metadata file so it's easier  for downstream processing.  So you can have really extremely large data sets and write all the code to process them  in a Jupyter notebook and never load that data on your machine so you're not trying  to overload your memory.  So let's shift gears now and talk just briefly about doing some image processing for deep  learning.  This notebook here, I will show you.  We also have this notebook and some more further steps on GitHub repo for doing the change  detection.  But I just want to talk about how within using X-Array, we can do things like loading really  large data sets and doing the pre-processing of those data sets without ever loading anything  into memory.  So the first thing we do is we're calling on these two different mosaics that we have,  one from 2017, one from 2022.  We're going to load those using Resterio, and so we're specifying here we want these  to be co-registered.  We want to have the same transform on the second raster, the same height, the same width.  And let's just make sure that when we load them up, we can easily stack them.  Then we're lazily loading.  So we're telling, like within the code, we're knowing where this data is and how we want  to chunk it up whenever we load it, but we're not actually calling it into memory.  So this line here is saying, okay, this raster A, I know that it's got four bands because  it's RGB and it has a near-infrared band, and I want to chunk it up into this size of  blocks, and I'm doing the same thing for the second raster.  So another cool thing about X-Ray is you can actually visualize what does this image cube  look like, what does this data cube look like, and you can make sure you can kind of check  and debug your code easily this way.  So we know we don't want the fourth channel because it's the near-infrared and our model  is not trained with that, so we just say give us the first three.  We want to scale it properly.  It's U and eight right now, but we want it to be zero to one before we normalize it,  so we just divide by 255.  We also want to do some preprocessing of subtracting the mean divided by the standard deviation.  So it's really easy to compute the means.  So we're saying compute the mean spatially.  So across all the XY coordinates, let's get the mean.  So that means let's get the mean for the RGB value and let's get the standard deviation  for the RGB value.  So we do that for the mean standard deviation for both rasters.  Then we can stack these rasters together, and we're going to stack them across a new  dimension called time.  So now we have this new data cube that's time.  So the first one's 2017, the second one's 2022, the RGB bands, then the spatial dimensions.  We can stack all of these means and standard deviations together.  So that way, when we do the normalization, we have this normalized image, which has two  time slices, three bands, and the pixels that we're interested in.  We can also do things like cropping, and I'm just going to speed up a little bit here so  we don't run out of too much time here, and also do things like re-chunking the data to  make sure it's the exact size that we want for our model.  And I just want to show this really quickly, and I'll share these links if you want them  as well.  This is our awesome machine learning change detection demo, which can get you the same  results that we showed in that previous slide.  And this repo, you can feel free to use it.  It'll help you download all the data.  It'll help you run the models and everything within this notebook.  And the only caveat I'll say is that you need to make sure you have a GPU that can run this.  So as long as you have a somewhat modern GPU that has a few gigabytes of virtual RAM,  you should be able to run this notebook.  So for the sake of time, I'll just briefly say I have another notebook that uses Numba  and X-Ray Spatial, and like I said, I'm more than willing to share these resources if you'd  like to see the notebooks.  All we show in this notebook is how we can take some really simple code of computing  slope and we can get some result of doing that using NumPy.  And then if we use, so that's around 400 milliseconds, and then if we use Numba, which is just really  good at doing vertical scaling, we can break it down into about five milliseconds running  on a laptop and then even do further improvements of getting it into the microsecond level of  speed.  So Numba is really a great tool for if you have code that you're running, how can I make  the code run a lot faster on a single worker?  And then you can pair up code that you've written with Numba with Dask to make it run  really, really fast across many different workers.  So I wanted to save some time here just for some questions, and if there's any more interest  in these specific notebooks, then feel free to bring that up now.  And I will share the code from here as well.  Thanks, Dylan.  I just mentioned to folks in the chat, there was a request for the notebooks, and I said  we'd get those from you and share them with people.  Sure.  Sounds good.  So, are there other questions for Dylan?  I think that was great.  A few things a little over my head as a simple Canadian here, but definitely learned a lot  and wish there were more hours in the day for me to dive into some of this stuff as  it is incredibly interesting.  Those data shader visuals were very slick.  Yeah, data shaders are a really, really cool tool.  I wanted to just quickly show, this is a great link.  This is on our blog.  I'll share this in the chat, and this will give you all the links that you need for all  the change detection stuff in case you want to check out that notebook and get it running.  There's also a video if you want to keep hearing me talk.  You can watch this YouTube video where I'll walk you through all of the steps of running  those notebooks, and here's a link to X-Ray Spatial.  If you're doing any kind of raster processing and you want to use Python, X-Ray Spatial  is for GIS professionals to be able to speed up their analyses.  I just want to show that.  Tim, it looks like your hand is up.  It is.  Dylan, I wanted to ask you if you can leave us in parting, knowing that this is a group  of largely government employees whose primary function is public service.  What have you seen or are you expecting to see in the years ahead as means of being a  functional translator of highly technical skills and tools that do really useful things  for public policy purposes, but being able to convincingly communicate them to perhaps  other persons in public service who may not understand an iota of what it is that you're  talking about or think it too complicated to apply to problems that are pressing?  What do you expect, and in particular, maybe even for MakePath's sake, how do you envision  yourself to help the public service sector, and how can we help communicate the value  of some of these things to certain leadership elsewhere?  Yeah, that's a really good question, and I'll try to answer it as best I can.  I think the value that I've seen so far has been from listening to what problems people  are working on and are concerned with and trying to meet them with some sort of similar  problem that we can show some imagery or show some results from and kind of meet in the  middle.  I think it's really key just for me to be able to communicate better with various different  audiences to kind of listen to what they care about, because what I care about outside of  just trying to apply really cool tools is having a group of people that know a lot of  things that I don't know that we can kind of work together and I can learn from their  expertise as well.  That's what really drives me.  So I think as far as the public policy and the specifics of working with different groups  that might have various different technical backgrounds or various different interests  in the tools part, I think showing them these visualizations and just talking about how  we can do this now so much faster, we can do this much more efficiently, and kind of  what broader impacts that has is really the key selling point from my view.  So I'm thinking about things like you want to get some kind of statistics off of data  sets that are really large, like from the technical side, we can do that.  But then what kind of outcome does that actually lead to and how can that help you?  So some of the things I heard from Nizjik that were really, really interesting to me  were some of the next gen 911 things that people are talking about and all of these  different applications that are going to affect hundreds of millions of people.  And just thinking about how my side of being the applier of the tools,  how can I put visualizations in front of people and convey a story to them that  is somewhat similar to what they're actually worried about to try to find that connection.  And I think with that, we're at 1029, and I know we've got a meeting at 1030.  We covered an amazing amount of ground in one hour.  Thank you very much, Dylan.  I think this was great.  And we will share those notebooks and one comment in the chat.  Sounds like something we can use, georeference our 62 imagery.  And Steve has a question around georeferencing of historic imagery in the public domain,  and I know Eric had his hand up afterwards.  So we'll follow up with Dylan's contact info with folks if that's cool and go from there.  Yeah, and feel free to reach out, and I'll get those notebooks.  I can send them to John or Tim, and I'll send just a zip file.  It'll be easy to just share around.  Perfect.  Okay.  All right.  Well, I will see some folks in our next meeting at 1030.  Thank you.  Thanks, Dylan.  Thank you all.  Take care. 