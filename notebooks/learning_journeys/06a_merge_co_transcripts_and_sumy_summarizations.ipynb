{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a helper notebook, to merge part results into overall results\n",
    "The part results will not be uploaded. This is only for transparency or if similar mistakes happen in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def pprint(long_string, width = 150):\n",
    "    # Use the textwrap module to pretty print the string\n",
    "    pretty_string = textwrap.fill(long_string, width=width)\n",
    "    print(pretty_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path): \n",
    "        merged_df = pd.read_csv(path)\n",
    "        strings = merged_df['String']\n",
    "        str_lst = strings.values\n",
    "\n",
    "        vocab = merged_df['Title'].values\n",
    "        identifier = merged_df['identifier']\n",
    "        identifier_vocab = pd.DataFrame({'ID': identifier, 'Vocab': vocab})\n",
    "        identifier_vocab = identifier_vocab.set_index('Vocab')['ID'].to_dict()\n",
    "        return merged_df, str_lst, vocab, identifier_vocab, identifier\n",
    "\n",
    "\n",
    "merged_df, str_lst, vocab, identifier_vocab, identifier = get_data('data/merged_data_for_AI.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge corrected transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all files\n",
    "\n",
    "with open(\"data/punctuation_correction/corrected_transcripts_0_to_3000.pkl\", \"rb\") as file:\n",
    "    co_transcripts_0_to_3000 = pickle.load(file)\n",
    "\n",
    "with open(\"data/punctuation_correction/corrected_transcripts_3000_to_5000.pkl\", \"rb\") as file:\n",
    "    co_transcripts_3000_to_5000 = pickle.load(file)\n",
    "\n",
    "with open(\"data/punctuation_correction/corrected_transcripts_5000_to_14000.pkl\", \"rb\") as file:\n",
    "    co_transcripts_5000_to_14000 = pickle.load(file)\n",
    "\n",
    "with open(\"data/punctuation_correction/corrected_transcripts_14000_to_21000.pkl\", \"rb\") as file:\n",
    "    co_transcripts_14000_to_21000 = pickle.load(file)\n",
    "\n",
    "with open(\"data/punctuation_correction/corrected_transcripts_21000_to_34000.pkl\", \"rb\") as file:\n",
    "    co_transcripts_21000_to_34000 = pickle.load(file)\n",
    "\n",
    "with open(\"data/punctuation_correction/corrected_transcripts_34000_to_36000.pkl\", \"rb\") as file:\n",
    "    co_transcripts_34000_to_36000 = pickle.load(file)        \n",
    "\n",
    "with open(\"data/punctuation_correction/corrected_transcripts_36000_to_40000.pkl\", \"rb\") as file:\n",
    "    co_transcripts_36000_to_40000 = pickle.load(file)    \n",
    "\n",
    "with open(\"data/punctuation_correction/corrected_transcripts_40000_to_57000.pkl\", \"rb\") as file:\n",
    "    co_transcripts_40000_to_57000 = pickle.load(file)    \n",
    "\n",
    "with open(\"data/punctuation_correction/corrected_transcripts_55000_to_71000.pkl\", \"rb\") as file:\n",
    "    co_transcripts_55000_to_71000 = pickle.load(file)    \n",
    "\n",
    "with open(\"data/punctuation_correction/corrected_transcripts_71000_to_71980.pkl\", \"rb\") as file:\n",
    "    co_transcripts_71000_to_71980 = pickle.load(file)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#co_transcripts = co_transcripts1 +co_transcripts2 +co_transcripts3 + co_transcripts4\n",
    "\n",
    "co_transcripts = co_transcripts_0_to_3000 + co_transcripts_3000_to_5000 \\\n",
    "                + co_transcripts_5000_to_14000 + co_transcripts_14000_to_21000 + co_transcripts_21000_to_34000 \\\n",
    "                + co_transcripts_34000_to_36000 + co_transcripts_36000_to_40000 + co_transcripts_40000_to_57000[:15000] \\\n",
    "                + co_transcripts_55000_to_71000 + co_transcripts_71000_to_71980 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/punctuation_correction/corrected_transcripts_all.pkl\", \"wb+\") as file:\n",
    "        pickle.dump(co_transcripts, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge summaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all corrected transcript files\n",
    "\n",
    "with open(\"data/punctuation_correction/summaries/sumy_summaries_0_to_2007.pkl\", \"rb\") as file:\n",
    "    sumy_summaries_0_to_2007 = pickle.load(file)\n",
    "\n",
    "\n",
    "with open(\"data/punctuation_correction/summaries/sumy_summaries_2007_to_14000.pkl\", \"rb\") as file:\n",
    "    sumy_summaries_2007_to_14000 = pickle.load(file)\n",
    "\n",
    "\n",
    "with open(\"data/punctuation_correction/summaries/sumy_summaries_12000_to_21000.pkl\", \"rb\") as file:\n",
    "    sumy_summaries_12000_to_21000 = pickle.load(file)\n",
    "\n",
    "with open(\"data/punctuation_correction/summaries/sumy_summaries_21000_to_34000.pkl\", \"rb\") as file:\n",
    "    sumy_summaries_21000_to_34000 = pickle.load(file)\n",
    "\n",
    "\n",
    "with open(\"data/punctuation_correction/summaries/sumy_summaries_34000_to_43000.pkl\", \"rb\") as file:\n",
    "    sumy_summaries_34000_to_43000 = pickle.load(file)\n",
    "\n",
    "\n",
    "with open(\"data/punctuation_correction/summaries/sumy_summaries_40000_to_55000.pkl\", \"rb\") as file:\n",
    "    sumy_summaries_40000_to_55000 = pickle.load(file)\n",
    "\n",
    "\n",
    "with open(\"data/punctuation_correction/summaries/sumy_summaries_55000_to_71000.pkl\", \"rb\") as file:\n",
    "    sumy_summaries_55000_to_71000 = pickle.load(file)\n",
    "\n",
    "\n",
    "with open(\"data/punctuation_correction/summaries/sumy_summaries_71000_to_71980.pkl\", \"rb\") as file:\n",
    "    sumy_summaries_71000_to_71980 = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumy_summaries_all = sumy_summaries_0_to_2007 + sumy_summaries_2007_to_14000[:-2000] + sumy_summaries_12000_to_21000 + sumy_summaries_21000_to_34000 \\\n",
    "                + sumy_summaries_34000_to_43000[:6000] + sumy_summaries_40000_to_55000 + sumy_summaries_55000_to_71000 + sumy_summaries_71000_to_71980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(co_transcripts_5000_to_14000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(sumy_summaries_all[5500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(sumy_summaries_all[54500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/punctuation_correction/summy_summaries_all\", \"wb+\") as file:\n",
    "    pickle.dump(sumy_summaries_all, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check order of summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(sumy_summaries_all[5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(co_transcripts[5000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unfortunally the summaries are in the wrong order, as the muliprocessing went wrong. (In the new function for creating summaries we fixxed the error.) The following cells are to fix the old results so we do not have to compute everything again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def hash_sentence(sentence):\n",
    "    return hashlib.sha256(sentence.encode()).hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "def sentence_separator(text, tokenizer):\n",
    "    sentences = tokenizer.to_sentences(text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(\"de\")\n",
    "\n",
    "\n",
    "co_transcripts_hashes = []\n",
    "for i, text in enumerate(co_transcripts):\n",
    "    cu_hash_list = []\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "    for j, sentence in enumerate(sentence_separator(text, tokenizer)):\n",
    "        sentence_hash = hash_sentence(sentence)\n",
    "        cu_hash_list.append(sentence_hash)\n",
    "    co_transcripts_hashes.append(cu_hash_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_hashes = []\n",
    "for i, summary in enumerate(sumy_summaries_all):\n",
    "    cu_hash_list = []\n",
    "    for j, sentence in enumerate(sentence_separator(summary, tokenizer)):\n",
    "        sentence_hash = hash_sentence(sentence)\n",
    "        cu_hash_list.append(sentence_hash)\n",
    "    summaries_hashes.append(cu_hash_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_matches(sum_list, transcript_list):\n",
    "    counter = 0\n",
    "    for sum_hash in sum_list:\n",
    "        if sum_hash in transcript_list:\n",
    "            counter +=1 \n",
    "    return counter\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_match(hashed_summary, co_transcripts_hashes, sindex, max_diff = 50):\n",
    "    num_sentences = len(hashed_summary)\n",
    "    #print(count_matches(hashed_summary, co_transcripts_hashes[sindex]))\n",
    "    if count_matches(hashed_summary, co_transcripts_hashes[sindex]) == num_sentences:\n",
    "         return sindex\n",
    "    for index_diff in range(max_diff):\n",
    "        if sindex - index_diff >= 0:\n",
    "            if count_matches(hashed_summary, co_transcripts_hashes[sindex-index_diff]) == num_sentences:\n",
    "                return sindex - index_diff\n",
    "        if sindex + index_diff < len(co_transcripts_hashes):\n",
    "            if count_matches(hashed_summary, co_transcripts_hashes[sindex+index_diff]) == num_sentences:\n",
    "                return sindex + index_diff\n",
    "    return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "matching_indices = {}\n",
    "for sindex, summary in enumerate(summaries_hashes):\n",
    "    if sindex % 10000 == 0:\n",
    "        print(sindex)\n",
    "    #check if already right position\n",
    "\n",
    "    match_index = find_match(summary, co_transcripts_hashes, sindex)\n",
    "    matching_indices[sindex] = match_index\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for summaries that either do not have any matching summary or match to more than one summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_to_key = {}\n",
    "\n",
    "for i in matching_indices.keys():\n",
    "    if matching_indices[i] not in value_to_key:\n",
    "        value_to_key[matching_indices[i]] = [i]\n",
    "    else:\n",
    "        value_to_key[matching_indices[i]].append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_matched = []\n",
    "matched_more_than_once = []\n",
    "already_matched_result_dict = {}\n",
    "for i in range(0, len(co_transcripts)):\n",
    "    if i not in value_to_key:\n",
    "        not_matched.append(i)\n",
    "    elif len(value_to_key[i]) > 1:\n",
    "        matched_more_than_once.append(i)\n",
    "    elif len(value_to_key[i]) == 1:\n",
    "        if i in already_matched_result_dict:\n",
    "            print(\"warning! \", i )\n",
    "        already_matched_result_dict[i] = value_to_key[i][0]\n",
    "    else:    \n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"correctly matched: \", len(already_matched_result_dict) , \"of \", len(co_transcripts))\n",
    "print(\"matched more than once: \", len(matched_more_than_once))\n",
    "print(\"not matched at all: \", len(not_matched))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for all transcripts we did not find a matching summary, we summarize them again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "def summarize_text(index, text):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"de\"))\n",
    "    summarizer = LexRankSummarizer()\n",
    "    summarizer.stop_words = get_stop_words(\"de\")\n",
    "\n",
    "    summary = summarizer(parser.document, 10)  # Summarize into 10 sentences\n",
    "    summary_text = \" \".join(str(sentence) for sentence in summary)\n",
    "\n",
    "    return index, summary_text\n",
    "    \n",
    "def summarize_texts_with_multiprocessing(texts, indices):\n",
    "    summaries = []\n",
    "    processed_count = 0\n",
    "\n",
    "    def update_progress(index):\n",
    "        nonlocal processed_count\n",
    "        processed_count += 1\n",
    "        if processed_count % 200 == 0:\n",
    "            print(\"Processed index:\", index)\n",
    "\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        results = []\n",
    "        for index in indices:\n",
    "            results.append(pool.apply_async(summarize_text, args=(index, texts[index]), callback=update_progress))\n",
    "\n",
    "        for result in results:\n",
    "            index, summary_text = result.get()\n",
    "            summaries.append({\"index\": index, \"summary\": summary_text})\n",
    "\n",
    "    return summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_more_than_once_results = summarize_texts_with_multiprocessing(co_transcripts, matched_more_than_once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this takes a few hours, as this seem to be the longest transcripts (about 4-6 times as long as the average)\n",
    "not_matched_results = summarize_texts_with_multiprocessing(co_transcripts, not_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to disk\n",
    "'''\n",
    "\n",
    "with open(\"data/punctuation_correction/summaries/not_matched_results.pkl\", \"wb+\") as file:\n",
    "    pickle.dump(not_matched_results, file)\n",
    "\n",
    "with open(\"data/punctuation_correction/summaries/matched_more_than_once_results.pkl\", \"wb+\") as file:\n",
    "    pickle.dump(matched_more_than_once_results, file)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load from disk\n",
    "\n",
    "with open(\"data/punctuation_correction/summaries/not_matched_results.pkl\", \"rb\") as file:\n",
    "  not_matched_results = pickle.load(file)\n",
    "\n",
    "with open(\"data/punctuation_correction/summaries/matched_more_than_once_results.pkl\", \"rb\") as file:\n",
    "  matched_more_than_once_results = pickle.load(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge all summaries together in the correct order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_matched_results_dict = {}\n",
    "\n",
    "for k in not_matched_results: \n",
    "    if k[\"index\"] in not_matched_results_dict:\n",
    "        print(\"warning! \", k)\n",
    "    else: \n",
    "        not_matched_results_dict[k[\"index\"]] = k[\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_more_than_once_results_dict = {}\n",
    "\n",
    "for k in matched_more_than_once_results: \n",
    "    if k[\"index\"] in matched_more_than_once_results_dict:\n",
    "        print(\"warning! \", k)\n",
    "    else: \n",
    "        matched_more_than_once_results_dict[k[\"index\"]] = k[\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_summaries = {}\n",
    "for k in range(len(co_transcripts)):\n",
    "    if np.isnan(k):\n",
    "        continue\n",
    "    if k in already_matched_result_dict:\n",
    "        cleaned_summaries[k] = sumy_summaries_all[value_to_key[k][0]]\n",
    "    else:\n",
    "        if k in not_matched_results_dict:\n",
    "            cleaned_summaries[k] = not_matched_results_dict[k]\n",
    "        else:\n",
    "            cleaned_summaries[k] = matched_more_than_once_results_dict[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sumy_summaries = []\n",
    "for k in range(len(co_transcripts)):\n",
    "    final_sumy_summaries.append(cleaned_summaries[k])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_sumy_summaries)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inspect a few example to validate, that the order is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_matched_result_dict.keys()\n",
    "#not_matched \n",
    "#matched_more_than_once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_index = 29722"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(final_sumy_summaries[ex_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(co_transcripts[ex_index])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save final summaries to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/punctuation_correction/summaries/sumy_summaries_cleaned.pkl\", \"wb+\") as file:\n",
    "    pickle.dump(final_sumy_summaries, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "gpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
