{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 13:24:07.750393: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-21 13:24:07.923892: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-21 13:24:09.579413: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deepmultilingualpunctuation import PunctuationModel\n",
    "import pickle\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path): \n",
    "        merged_df = pd.read_csv(path)\n",
    "        strings = merged_df['String']\n",
    "        str_lst = strings.values\n",
    "\n",
    "        vocab = merged_df['Title'].values\n",
    "        identifier = merged_df['identifier']\n",
    "        identifier_vocab = pd.DataFrame({'ID': identifier, 'Vocab': vocab})\n",
    "        identifier_vocab = identifier_vocab.set_index('Vocab')['ID'].to_dict()\n",
    "        return merged_df, str_lst, vocab, identifier_vocab, identifier\n",
    "\n",
    "\n",
    "merged_df, str_lst, vocab, identifier_vocab, identifier = get_data('data/merged_data_for_AI.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/gpt/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"none\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = PunctuationModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/57980 [00:00<?, ?it/s]/root/miniconda3/envs/gpt/lib/python3.10/site-packages/transformers/pipelines/base.py:1081: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "  0%|          | 79/57980 [04:42<88:15:25,  5.49s/it] "
     ]
    }
   ],
   "source": [
    "corrected = []\n",
    "start_index = 14000\n",
    "total_iterations = len(merged_df[\"transcript\"])- start_index\n",
    "with tqdm(total=total_iterations) as pbar:\n",
    "    for index, t in enumerate(merged_df[\"transcript\"][start_index:]):\n",
    "        if index % 1000 == 0:\n",
    "            # Save the corrected list to disk\n",
    "            with open(f'corrected_transcripts4.pkl', 'wb') as f:\n",
    "                pickle.dump(corrected, f)\n",
    "        try:\n",
    "            result = model.restore_punctuation(t)\n",
    "            corrected.append(result)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            corrected.append(\"ERROR: \" + repr(e))\n",
    "        pbar.update(1)\n",
    "\n",
    "# Save the final corrected list to disk\n",
    "with open('corrected_final.pkl', 'wb') as f:\n",
    "    pickle.dump(corrected, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corrected_transcripts.pkl', 'rb') as f:\n",
    "    ctl = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ctl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "20245\n",
      "37490\n",
      "54735\n"
     ]
    }
   ],
   "source": [
    "for i in range(3000, 71980, int((71980-3000)/4)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [\n",
    "    merged_df[\"transcript\"][3000:20245],\n",
    "    merged_df[\"transcript\"][20245:37490],\n",
    "    merged_df[\"transcript\"][37490:54735],\n",
    "    merged_df[\"transcript\"][54735:],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ellipsis' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/anaconda/envs/lj/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/anaconda/envs/lj/lib/python3.10/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"/tmp/ipykernel_5966/2550636438.py\", line 14, in restore_punctuation\n    model.to('cuda') # Move model to GPU if it's a PyTorch model\nAttributeError: 'ellipsis' object has no attribute 'to'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 35\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39mwith\u001b[39;00m Pool(processes\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m) \u001b[39mas\u001b[39;00m pool:\n\u001b[0;32m---> 35\u001b[0m     pool\u001b[39m.\u001b[39;49mmap(restore_punctuation, chunks)\n",
      "File \u001b[0;32m/anaconda/envs/lj/lib/python3.10/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[39m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[39m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, mapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m/anaconda/envs/lj/lib/python3.10/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.multiprocessing import Pool, Process, set_start_method\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.multiprocessing import Pool, Process, set_start_method\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Function for restoring punctuation\n",
    "def restore_punctuation(data):\n",
    "    model = ... # Initialize the model here, if it's a PyTorch model, it could be moved to a GPU.\n",
    "    model.to('cuda') # Move model to GPU if it's a PyTorch model\n",
    "    results = []\n",
    "    for i, (t, index) in enumerate(data):\n",
    "        result = model.restore_punctuation(t)\n",
    "        results.append((result, index))\n",
    "        if i % 1000 == 0:\n",
    "            print(f'Process {os.getpid()}: processed {i} out of {len(data)} data points')\n",
    "    # Save the result list to disk\n",
    "    with open(f'corrected_{index}.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "try:\n",
    "    set_start_method('spawn')   \n",
    "except RuntimeError:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with Pool(processes=4) as pool:\n",
    "    pool.map(restore_punctuation, chunks)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/lj/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"none\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PunctuationModel' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m         restore_punctuation(batch, model)\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 33\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[29], line 19\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[1;32m     18\u001b[0m     model \u001b[39m=\u001b[39m PunctuationModel()\n\u001b[0;32m---> 19\u001b[0m     model\u001b[39m.\u001b[39;49mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# Move model to GPU if it's a PyTorch model\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[39m# Preparing the data for processing\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(merged_df[\u001b[39m\"\u001b[39m\u001b[39mtranscript\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(merged_df[\u001b[39m\"\u001b[39m\u001b[39mtranscript\u001b[39m\u001b[39m\"\u001b[39m]))))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PunctuationModel' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Function for restoring punctuation\n",
    "def restore_punctuation(data, model):\n",
    "    results = []\n",
    "    for i, (t, index) in enumerate(data):\n",
    "        result = model.restore_punctuation(t)\n",
    "        results.append((result, index))\n",
    "        if i % 1000 == 0:\n",
    "            print(f'Processed {i} out of {len(data)} data points')\n",
    "    # Save the result list to disk\n",
    "    with open(f'corrected_{index}.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "def main():\n",
    "    model = PunctuationModel()\n",
    "    model.to('cuda') # Move model to GPU if it's a PyTorch model\n",
    "\n",
    "    # Preparing the data for processing\n",
    "    data = list(zip(merged_df[\"transcript\"], range(len(merged_df[\"transcript\"]))))\n",
    "\n",
    "    # Define your batch size here. Adjust this according to your GPU's memory.\n",
    "    batch_size = 1000\n",
    "\n",
    "    # Split data into batches and process each batch\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        restore_punctuation(batch, model)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71980, 768)\n"
     ]
    }
   ],
   "source": [
    "video_embeddings = pd.read_csv('data/sentence_transformer_embedding.csv')\n",
    "del video_embeddings['Unnamed: 0']\n",
    "video_embeddings_values = video_embeddings.values\n",
    "print(video_embeddings_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_cosine_similarity(A, b):\n",
    "    # Convert A and b to numpy arrays\n",
    "    A = np.array(A)\n",
    "    b = np.array(b)\n",
    "\n",
    "    # Calculate cosine similarity between A and b\n",
    "    similarity_scores = cosine_similarity(A, b.reshape(1, -1)).flatten()\n",
    "\n",
    "    # Sort the indices based on similarity scores\n",
    "    sorted_indices = np.argsort(similarity_scores)[::-1]\n",
    "\n",
    "    return sorted_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_videos(subject,lj_data, video_data_embeddings, video_titles, video_identifier, n = 3):\n",
    "    subject_dict = lj_data[subject]\n",
    "    print(\"Learning journey for \", subject, \" is:\")\n",
    "    for i in subject_dict.keys():\n",
    "        \n",
    "        print(i)\n",
    "        tmp_string = subject_dict[i][\"name\"][\"raw_content\"]\n",
    "        t_emb = subject_dict[i][\"name\"][\"embedding\"]\n",
    "        print(\"to learn \", tmp_string, \" watch the following videos:\")\n",
    "        cosine_similarity_sorted = calculate_cosine_similarity(video_data_embeddings,t_emb)\n",
    "        for index, v in enumerate(cosine_similarity_sorted[:3]):\n",
    "            title = video_titles[v]\n",
    "            #print(index)\n",
    "            #print(title)\n",
    "            url = \"https://www.youtube.com/watch?v=\" + video_identifier[v]\n",
    "            print(\"Video %i: %s\" % (index + 1, title))\n",
    "            print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Betriebswirtschaftslehre', 'Informatik', 'Medizin', 'Maschinenbau', 'Elektrotechnik', 'Psychologie', 'Jura/Rechtswissenschaften', 'Architektur', 'Chemie', 'Biologie', 'Geschichte', 'Soziologie', 'Volkswirtschaftslehre', 'Mathematik', 'Physik', 'Politikwissenschaft', 'Medienwissenschaft', 'Sprachwissenschaft/Linguistik', 'Pädagogik', 'Philosophie', 'Kunstgeschichte', 'Sportwissenschaft', 'Geografie', 'Musikwissenschaft', 'Ethnologie'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_data_we.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning journey for  Betriebswirtschaftslehre  is:\n",
      "Grundlagen der Betriebswirtschaftslehre\n",
      "to learn  Grundlagen der Betriebswirtschaftslehre  watch the following videos:\n",
      "Video 1: How The Economic Machine Works by Ray Dalio\n",
      "https://www.youtube.com/watch?v=PHe0bXAIuk0\n",
      "Video 2: The Theory of Production - Economics A2 Level Unit 3\n",
      "https://www.youtube.com/watch?v=jph_CTpjNAY\n",
      "Video 3: 3 Economic Rules of Thumb\n",
      "https://www.youtube.com/watch?v=X0KOV0o1P30\n",
      "Marketing\n",
      "to learn  Marketing  watch the following videos:\n",
      "Video 1: What is Marketing? Marketing In The Real and Business World - Marketing 101\n",
      "https://www.youtube.com/watch?v=VzsNqYARnd0\n",
      "Video 2: How to Develop a Marketing Strategy by Philip Kotler\n",
      "https://www.youtube.com/watch?v=ghFwpoH71NM\n",
      "Video 3: Marketing Mix: Promotion Strategy part 2\n",
      "https://www.youtube.com/watch?v=FT2mJBbU-wE\n",
      "Rechnungswesen\n",
      "to learn  Rechnungswesen  watch the following videos:\n",
      "Video 1: Introduction to Modern Internal Audit (Take Quiz in Description to get 1 CPE)\n",
      "https://www.youtube.com/watch?v=rSoDcwyoQRc\n",
      "Video 2: Accounting Lecture 01 - Basic Concepts\n",
      "https://www.youtube.com/watch?v=Wu6bUFWaNZo\n",
      "Video 3: Accounting Lecture 08 Part I - LIFO, FIFO & Average Cost\n",
      "https://www.youtube.com/watch?v=UGs1EXalrZo\n",
      "Finanzierung\n",
      "to learn  Finanzierung  watch the following videos:\n",
      "Video 1: Get Working Capital, Business Loans & Line of Credit Options with Rok Financial\n",
      "https://www.youtube.com/watch?v=d98vm1aZVrA\n",
      "Video 2: How to Find The Best Business Funding\n",
      "https://www.youtube.com/watch?v=NS2y4sJaU_w\n",
      "Video 3: Überziehungskredit, Dispositionskredit, Kontokorrentkredit | Grundbegriffe der Wirtschaftslehre\n",
      "https://www.youtube.com/watch?v=LfQZMxJWjeY\n",
      "Personalmanagement\n",
      "to learn  Personalmanagement  watch the following videos:\n",
      "Video 1: How to Manage Costs in ProjectManager\n",
      "https://www.youtube.com/watch?v=Vun_Cop6eMw\n",
      "Video 2: Introduction to Modern Internal Audit (Take Quiz in Description to get 1 CPE)\n",
      "https://www.youtube.com/watch?v=rSoDcwyoQRc\n",
      "Video 3: How to Manage Projects with MS Teams | Advisicon\n",
      "https://www.youtube.com/watch?v=tfW325pd3NI\n"
     ]
    }
   ],
   "source": [
    "get_top_n_videos(\"Betriebswirtschaftslehre\", study_data_we, video_embeddings_values,merged_df[\"Title\"],merged_df[\"identifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Barista', 'Gärtner', 'Stricken', 'Basteln', 'Malen', 'Kommunizieren', 'Meditation', 'Campingmanagement', 'Scrum', 'Azure DevOps', 'Projektmanagement', 'Immobilien', 'Teamleitung'])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll_data_we.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning journey for  Azure DevOps  is:\n",
      "Grundlagen der Azure Cloud\n",
      "to learn  Grundlagen der Azure Cloud  watch the following videos:\n",
      "Video 1: Cloud Computing Fundamentals\n",
      "https://www.youtube.com/watch?v=uroryFU78gM\n",
      "Video 2: Intro to Google Cloud, a song!\n",
      "https://www.youtube.com/watch?v=iVFmlgWvJcg\n",
      "Video 3: Continuous Integration, Continuous Deployment (CI-CD) with Azure DevOps\n",
      "https://www.youtube.com/watch?v=jRgLSMlp28U\n",
      "Verwalten von Source Control\n",
      "to learn  Verwalten von Source Control  watch the following videos:\n",
      "Video 1: HikCentral Access Control - Access Control Module Setup and Management\n",
      "https://www.youtube.com/watch?v=nezbYEC0aec\n",
      "Video 2: Track 2 01 From Workstation to Domain Admin Why Secure Administration Isnt Secure and How to Fix It\n",
      "https://www.youtube.com/watch?v=Wdbm2_1tn14\n",
      "Video 3: Control Techniques F600 Pump Drive Energy Features\n",
      "https://www.youtube.com/watch?v=4LicbciyAbc\n",
      "Azure Boards\n",
      "to learn  Azure Boards  watch the following videos:\n",
      "Video 1: Azure Arc enabled Kubernetes with GitOps | Azure Friday\n",
      "https://www.youtube.com/watch?v=4fT47TKprFQ\n",
      "Video 2: Loving Azure Boards with Delivery Plans 2.0\n",
      "https://www.youtube.com/watch?v=TEJTg5kdYE4\n",
      "Video 3: Best practices Using Azure Resource Manager (ARM) Templates\n",
      "https://www.youtube.com/watch?v=myYTGsONrn0\n",
      "Azure Pipelines\n",
      "to learn  Azure Pipelines  watch the following videos:\n",
      "Video 1: Azure Data Pipeline Design in 60 seconds\n",
      "https://www.youtube.com/watch?v=mvkpoRGvTFA\n",
      "Video 2: Continuous Integration & Continuous Delivery Automation With Azure Pipelines\n",
      "https://www.youtube.com/watch?v=tERsk63Ff1w\n",
      "Video 3: Azure DevOps Pipelines Tutorial - Continuous Integration\n",
      "https://www.youtube.com/watch?v=AtRa9Ezz9j4\n",
      "Azure Test Plans\n",
      "to learn  Azure Test Plans  watch the following videos:\n",
      "Video 1: AZ-900 | Microsoft Azure Fundamentals Full Course, Free Practice Tests, Website and Study Guides\n",
      "https://www.youtube.com/watch?v=NPEsD6n9A_I\n",
      "Video 2: MS-100 EP 05: Planning your on-premises infrastructure for Microsoft 365\n",
      "https://www.youtube.com/watch?v=geXlHHDxw1c\n",
      "Video 3: Terraform and Azure DevOps – Delivering a continuous and automated deployment | DevOps Lab\n",
      "https://www.youtube.com/watch?v=Llx4TeU2Hms\n",
      "Azure Artifacts\n",
      "to learn  Azure Artifacts  watch the following videos:\n",
      "Video 1: Terraform and Azure DevOps – Delivering a continuous and automated deployment | DevOps Lab\n",
      "https://www.youtube.com/watch?v=Llx4TeU2Hms\n",
      "Video 2: REPLACED WITH V2 - Microsoft Azure Master Class Part 8 - Application Services\n",
      "https://www.youtube.com/watch?v=_E73_SQN8ZU\n",
      "Video 3: Best practices Using Azure Resource Manager (ARM) Templates\n",
      "https://www.youtube.com/watch?v=myYTGsONrn0\n"
     ]
    }
   ],
   "source": [
    "get_top_n_videos(\"Azure DevOps\", ll_data_we, video_embeddings_values,merged_df[\"Title\"],merged_df[\"identifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "gpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
